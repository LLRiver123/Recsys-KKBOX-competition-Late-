{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7162,"databundleVersionId":44320,"isSourceIdPinned":false,"sourceType":"competition"},{"sourceId":14402363,"sourceType":"datasetVersion","datasetId":9198392},{"sourceId":248184,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":212107,"modelId":233781},{"sourceId":670813,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":508085,"modelId":522756}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Cấu hình giao diện biểu đồ\nsns.set_style(\"whitegrid\")\nplt.rcParams['figure.figsize'] = (12, 6)\n\n# ==========================================\n# 1. PHÂN TÍCH TARGET (Nghe lại vs Không nghe lại)\n# ==========================================\ndef plot_target_distribution(df):\n    plt.figure(figsize=(6, 4))\n    sns.countplot(x='target', data=df, palette='viridis')\n    plt.title('Distribution of Target Variable (0 vs 1)', fontsize=14)\n    plt.xlabel('Target (1 = Replay, 0 = No Replay)')\n    plt.ylabel('Count')\n    \n    # Tính phần trăm\n    total = len(df)\n    ones = df['target'].sum()\n    print(f\"Total samples: {total}\")\n    print(f\"Replay ratio: {ones/total:.2%}\")\n    plt.show()\n\n# ==========================================\n# 2. PHÂN TÍCH NGỮ CẢNH (SOURCE TABS & TYPES)\n# ==========================================\ndef plot_context_analysis(df):\n    fig, axes = plt.subplots(1, 2, figsize=(18, 6))\n    \n    # Biểu đồ 1: Số lượng nghe theo Source Tab\n    sns.countplot(x='source_system_tab', data=df, hue='target', \n                  order=df['source_system_tab'].value_counts().index[:10],\n                  palette='coolwarm', ax=axes[0])\n    axes[0].set_title('Replay Behavior by Source Tab')\n    axes[0].tick_params(axis='x', rotation=45)\n    \n    # Biểu đồ 2: Tỷ lệ nghe lại (Mean Target) theo Source Type\n    # Cái này quan trọng: Nó cho biết xác suất nghe lại ở đâu cao nhất\n    prob_df = df.groupby('source_type')['target'].mean().reset_index()\n    prob_df = prob_df.sort_values('target', ascending=False)\n    \n    sns.barplot(x='source_type', y='target', data=prob_df, palette='magma', ax=axes[1])\n    axes[1].set_title('Probability of Replay by Source Type')\n    axes[1].set_ylabel('Replay Probability')\n    axes[1].tick_params(axis='x', rotation=45)\n    axes[1].axhline(df['target'].mean(), color='r', linestyle='--', label='Global Average')\n    axes[1].legend()\n    \n    plt.tight_layout()\n    plt.show()\n\n# ==========================================\n# 3. PHÂN TÍCH BÀI HÁT (SONGS)\n# ==========================================\ndef plot_song_analysis(df):\n    fig, axes = plt.subplots(1, 2, figsize=(18, 6))\n    \n    # Biểu đồ 1: Top 10 bài hát phổ biến nhất\n    top_songs = df['song_id'].value_counts().head(10)\n    sns.barplot(x=top_songs.values, y=top_songs.index, palette='Blues_d', ax=axes[0])\n    axes[0].set_title('Top 10 Most Listened Songs')\n    axes[0].set_xlabel('Number of Listens')\n    \n    # Biểu đồ 2: Phân bố độ dài bài hát (Song Length)\n    # Convert ms to minutes\n    # Lọc bỏ nhiễu (bài hát > 15 phút hoặc < 30s)\n    temp_df = df[(df['song_length'] > 30000) & (df['song_length'] < 900000)].copy()\n    temp_df['minutes'] = temp_df['song_length'] / 60000\n    \n    sns.histplot(data=temp_df, x='minutes', hue='target', bins=50, kde=True, palette='husl', ax=axes[1])\n    axes[1].set_title('Song Length Distribution (Minutes)')\n    \n    plt.tight_layout()\n    plt.show()\n\n# ==========================================\n# 4. PHÂN TÍCH NGƯỜI DÙNG (USER DEMOGRAPHICS)\n# ==========================================\ndef plot_user_analysis(df):\n    fig, axes = plt.subplots(1, 2, figsize=(18, 6))\n    \n    # Biểu đồ 1: Tuổi (BD)\n    # Dữ liệu KKBox có nhiều nhiễu tuổi (0 hoặc >100), cần lọc\n    valid_age = df[(df['bd'] > 10) & (df['bd'] < 80)]\n    \n    sns.histplot(data=valid_age, x='bd', hue='target', bins=30, multiple=\"stack\", palette='Set2', ax=axes[0])\n    axes[0].set_title('Age Distribution (10-80 years old)')\n    axes[0].set_xlabel('Age')\n    \n    # Biểu đồ 2: Hoạt động theo Giới tính\n    sns.countplot(x='gender', hue='target', data=df, palette='Pastel1', ax=axes[1])\n    axes[1].set_title('Replay Behavior by Gender')\n    \n    plt.tight_layout()\n    plt.show()\n\n# ==========================================\n# 5. EXECUTE ANALYSIS\n# ==========================================\n# Vì dữ liệu rất lớn, chúng ta chỉ lấy mẫu (sample) 10% để vẽ cho nhanh\n# nhưng vẫn đảm bảo tính thống kê\nSAMPLE_SIZE = 0.1 \n\nprint(f\"Sampling {SAMPLE_SIZE*100}% of data for visualization...\")\nsample_df = train_df.sample(frac=SAMPLE_SIZE, random_state=42)\n\n# Gọi các hàm vẽ\nprint(\">>> 1. Target Distribution\")\nplot_target_distribution(sample_df)\n\nprint(\">>> 2. Context Analysis (Source Tabs/Types)\")\nplot_context_analysis(sample_df)\n\nprint(\">>> 3. Song Analysis (Popularity & Length)\")\nplot_song_analysis(sample_df)\n\nprint(\">>> 4. User Demographics\")\nplot_user_analysis(sample_df)\n\n# Dọn dẹp bộ nhớ sau khi vẽ\ndel sample_df\nimport gc\ngc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T01:15:31.302151Z","iopub.execute_input":"2026-01-07T01:15:31.302511Z","iopub.status.idle":"2026-01-07T01:15:31.498561Z","shell.execute_reply.started":"2026-01-07T01:15:31.302477Z","shell.execute_reply":"2026-01-07T01:15:31.497084Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_47/3320430493.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Cấu hình giao diện biểu đồ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_style\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"whitegrid\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/seaborn/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Import seaborn objects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mrcmod\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F401,F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F401,F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mpalettes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F401,F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mrelational\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F401,F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/seaborn/rcmod.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmpl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcycler\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcycler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpalettes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/seaborn/palettes.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mexternal\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhusl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdesaturate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_color_cycle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcolors\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mxkcd_rgb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcrayons\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_compat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_colormap\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/seaborn/utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmpl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolors\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mto_rgb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig_init\u001b[0m  \u001b[0;31m# pyright: ignore[reportUnusedImport] # noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m from pandas.core.api import (\n\u001b[0m\u001b[1;32m     50\u001b[0m     \u001b[0;31m# dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mArrowDtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/api.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstruction\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFlags\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m from pandas.core.groupby import (\n\u001b[0m\u001b[1;32m     48\u001b[0m     \u001b[0mGrouper\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mNamedAgg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/groupby/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m from pandas.core.groupby.generic import (\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mDataFrameGroupBy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mNamedAgg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mSeriesGroupBy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/groupby/generic.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     66\u001b[0m )\n\u001b[1;32m     67\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframe\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m from pandas.core.groupby import (\n\u001b[1;32m     70\u001b[0m     \u001b[0mbase\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[0msanitize_masked_array\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m )\n\u001b[0;32m--> 149\u001b[0;31m from pandas.core.generic import (\n\u001b[0m\u001b[1;32m    150\u001b[0m     \u001b[0mNDFrame\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0mmake_doc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    150\u001b[0m )\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m from pandas.core import (\n\u001b[0m\u001b[1;32m    153\u001b[0m     \u001b[0malgorithms\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0malgos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0marraylike\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0mlength_of_indexer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m )\n\u001b[0;32m---> 79\u001b[0;31m from pandas.core.indexes.api import (\n\u001b[0m\u001b[1;32m     80\u001b[0m     \u001b[0mIndex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0mMultiIndex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/api.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malgorithms\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msafe_sort\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m from pandas.core.indexes.base import (\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mIndex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0m_new_Index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m )\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m from pandas._libs import (\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0mNaT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0malgos\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mlibalgos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36minit pandas._libs.index\u001b[0;34m()\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: partially initialized module 'pandas' has no attribute '_pandas_datetime_CAPI' (most likely due to a circular import)"],"ename":"AttributeError","evalue":"partially initialized module 'pandas' has no attribute '_pandas_datetime_CAPI' (most likely due to a circular import)","output_type":"error"}],"execution_count":5},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Unziping Data\n!7z e \"../input/kkbox-music-recommendation-challenge/members.csv.7z\"\n!7z e \"../input/kkbox-music-recommendation-challenge/songs.csv.7z\"\n!7z e \"../input/kkbox-music-recommendation-challenge/test.csv.7z\"\n!7z e \"../input/kkbox-music-recommendation-challenge/train.csv.7z\"\n!7z e \"../input/kkbox-music-recommendation-challenge/song_extra_info.csv.7z\"\n!7z e \"../input/kkbox-music-recommendation-challenge/sample_submission.csv.7z\"","metadata":{"trusted":true,"_kg_hide-output":false,"execution":{"iopub.status.busy":"2026-01-07T01:16:26.506021Z","iopub.execute_input":"2026-01-07T01:16:26.506324Z","iopub.status.idle":"2026-01-07T01:16:54.789282Z","shell.execute_reply.started":"2026-01-07T01:16:26.506304Z","shell.execute_reply":"2026-01-07T01:16:54.787713Z"}},"outputs":[{"name":"stdout","text":"\n7-Zip [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21\np7zip Version 16.02 (locale=en_US.UTF-8,Utf16=on,HugeFiles=on,64 bits,4 CPUs AMD EPYC 7B12 (830F10),ASM,AES-NI)\n\nScanning the drive for archives:\n  0M Scan ../input/kkbox-music-recommendation-challeng                                                      1 file, 1349856 bytes (1319 KiB)\n\nExtracting archive: ../input/kkbox-music-recommendation-challenge/members.csv.7z\n--\nPath = ../input/kkbox-music-recommendation-challenge/members.csv.7z\nType = 7z\nPhysical Size = 1349856\nHeaders Size = 130\nMethod = LZMA2:3m\nSolid = -\nBlocks = 1\n\n    Everything is Ok\n\nSize:       2503827\nCompressed: 1349856\n\n7-Zip [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21\np7zip Version 16.02 (locale=en_US.UTF-8,Utf16=on,HugeFiles=on,64 bits,4 CPUs AMD EPYC 7B12 (830F10),ASM,AES-NI)\n\nScanning the drive for archives:\n  0M Scan ../input/kkbox-music-recommendation-challeng                                                      1 file, 105809525 bytes (101 MiB)\n\nExtracting archive: ../input/kkbox-music-recommendation-challenge/songs.csv.7z\n--\nPath = ../input/kkbox-music-recommendation-challenge/songs.csv.7z\nType = 7z\nPhysical Size = 105809525\nHeaders Size = 122\nMethod = LZMA2:24\nSolid = -\nBlocks = 1\n\n      2% - songs.c                5% - songs.c                9% - songs.c               11% - songs.c               15% - songs.c               18% - songs.c               20% - songs.c               24% - songs.c               27% - songs.c               30% - songs.c               33% - songs.c               36% - songs.c               39% - songs.c               41% - songs.c               45% - songs.c               48% - songs.c               51% - songs.c               54% - songs.c               57% - songs.c               60% - songs.c               64% - songs.c               67% - songs.c               69% - songs.c               72% - songs.c               75% - songs.c               79% - songs.c               82% - songs.c               85% - songs.c               88% - songs.c               91% - songs.c               94% - songs.c               98% - songs.c              Everything is Ok\n\nSize:       221828666\nCompressed: 105809525\n\n7-Zip [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21\np7zip Version 16.02 (locale=en_US.UTF-8,Utf16=on,HugeFiles=on,64 bits,4 CPUs AMD EPYC 7B12 (830F10),ASM,AES-NI)\n\nScanning the drive for archives:\n  0M Scan ../input/kkbox-music-recommendation-challeng                                                      1 file, 43925208 bytes (42 MiB)\n\nExtracting archive: ../input/kkbox-music-recommendation-challenge/test.csv.7z\n--\nPath = ../input/kkbox-music-recommendation-challenge/test.csv.7z\nType = 7z\nPhysical Size = 43925208\nHeaders Size = 122\nMethod = LZMA2:24\nSolid = -\nBlocks = 1\n\n      4% - test.cs               12% - test.cs               19% - test.cs               25% - test.cs               31% - test.cs               37% - test.cs               42% - test.cs               49% - test.cs               55% - test.cs               61% - test.cs               68% - test.cs               73% - test.cs               79% - test.cs               86% - test.cs               92% - test.cs               98% - test.cs              Everything is Ok\n\nSize:       347789925\nCompressed: 43925208\n\n7-Zip [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21\np7zip Version 16.02 (locale=en_US.UTF-8,Utf16=on,HugeFiles=on,64 bits,4 CPUs AMD EPYC 7B12 (830F10),ASM,AES-NI)\n\nScanning the drive for archives:\n  0M Scan ../input/kkbox-music-recommendation-challeng                                                      1 file, 106420688 bytes (102 MiB)\n\nExtracting archive: ../input/kkbox-music-recommendation-challenge/train.csv.7z\n--\nPath = ../input/kkbox-music-recommendation-challenge/train.csv.7z\nType = 7z\nPhysical Size = 106420688\nHeaders Size = 122\nMethod = LZMA2:24\nSolid = -\nBlocks = 1\n\n      2% - train.c                5% - train.c                8% - train.c               11% - train.c               13% - train.c               15% - train.c               18% - train.c               21% - train.c               23% - train.c               25% - train.c               28% - train.c               30% - train.c               33% - train.c               35% - train.c               37% - train.c               40% - train.c               42% - train.c               45% - train.c               47% - train.c               49% - train.c               51% - train.c               54% - train.c               56% - train.c               59% - train.c               61% - train.c               63% - train.c               66% - train.c               68% - train.c               70% - train.c               73% - train.c               75% - train.c               78% - train.c               80% - train.c               82% - train.c               85% - train.c               87% - train.c               89% - train.c               91% - train.c               94% - train.c               97% - train.c               99% - train.c              Everything is Ok\n\nSize:       971675848\nCompressed: 106420688\n\n7-Zip [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21\np7zip Version 16.02 (locale=en_US.UTF-8,Utf16=on,HugeFiles=on,64 bits,4 CPUs AMD EPYC 7B12 (830F10),ASM,AES-NI)\n\nScanning the drive for archives:\n  0M Scan ../input/kkbox-music-recommendation-challeng                                                      1 file, 103608205 bytes (99 MiB)\n\nExtracting archive: ../input/kkbox-music-recommendation-challenge/song_extra_info.csv.7z\n--\nPath = ../input/kkbox-music-recommendation-challenge/song_extra_info.csv.7z\nType = 7z\nPhysical Size = 103608205\nHeaders Size = 140\nMethod = LZMA:25\nSolid = -\nBlocks = 1\n\n      2% - song_extra_info.c                          5% - song_extra_info.c                          8% - song_extra_info.c                         11% - song_extra_info.c                         14% - song_extra_info.c                         17% - song_extra_info.c                         20% - song_extra_info.c                         23% - song_extra_info.c                         26% - song_extra_info.c                         29% - song_extra_info.c                         32% - song_extra_info.c                         35% - song_extra_info.c                         39% - song_extra_info.c                         42% - song_extra_info.c                         45% - song_extra_info.c                         48% - song_extra_info.c                         50% - song_extra_info.c                         53% - song_extra_info.c                         56% - song_extra_info.c                         58% - song_extra_info.c                         61% - song_extra_info.c                         64% - song_extra_info.c                         67% - song_extra_info.c                         70% - song_extra_info.c                         73% - song_extra_info.c                         76% - song_extra_info.c                         78% - song_extra_info.c                         81% - song_extra_info.c                         83% - song_extra_info.c                         86% - song_extra_info.c                         89% - song_extra_info.c                         92% - song_extra_info.c                         96% - song_extra_info.c                         99% - song_extra_info.c                        Everything is Ok\n\nSize:       181010294\nCompressed: 103608205\n\n7-Zip [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21\np7zip Version 16.02 (locale=en_US.UTF-8,Utf16=on,HugeFiles=on,64 bits,4 CPUs AMD EPYC 7B12 (830F10),ASM,AES-NI)\n\nScanning the drive for archives:\n  0M Scan ../input/kkbox-music-recommendation-challeng                                                      1 file, 463688 bytes (453 KiB)\n\nExtracting archive: ../input/kkbox-music-recommendation-challenge/sample_submission.csv.7z\n--\nPath = ../input/kkbox-music-recommendation-challenge/sample_submission.csv.7z\nType = 7z\nPhysical Size = 463688\nHeaders Size = 146\nMethod = LZMA2:24\nSolid = -\nBlocks = 1\n\n    Everything is Ok\n\nSize:       29570380\nCompressed: 463688\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\nfrom sentence_transformers import SentenceTransformer\nfrom tqdm import tqdm\nimport gc\nimport os\n\n# --- CONFIGURATION ---\nCONFIG = {\n    'DEVICE': torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n    'DATA_DIR': '/kaggle/working/',\n    'BATCH_SIZE': 2048,     # Batch size lớn cho dữ liệu dạng bảng\n    'EPOCHS': 15,\n    'LR': 0.001,\n    'EMBEDDING_DIM': 64,    # Latent dimension cho User/Item\n    'SBERT_DIM': 384,       # Kích thước gốc của SBERT MiniLM\n    'TEXT_PROJ_DIM': 32,    # Kích thước nén của Text feature trong model\n    'PATIENCE': 3           # Early Stopping\n}\n\nprint(f\"Using device: {CONFIG['DEVICE']}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T01:15:31.501346Z","iopub.status.idle":"2026-01-07T01:15:31.501641Z","shell.execute_reply.started":"2026-01-07T01:15:31.501514Z","shell.execute_reply":"2026-01-07T01:15:31.501527Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sentence_transformers import SentenceTransformer\nfrom tqdm import tqdm\nimport gc\n\n# --- CONFIG ---\nCONFIG = {\n    'DEVICE': torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n    'DATA_DIR': '/kaggle/working/',\n    'BATCH_SIZE': 2048,\n    'EPOCHS': 15,\n    'LR': 0.001,\n    'EMBEDDING_DIM': 64,\n    'SBERT_DIM': 384,\n    'TEXT_PROJ_DIM': 16\n}\n\ndef load_data_optimized():\n    # Định nghĩa kiểu dữ liệu chuẩn xác hơn\n    dtypes = {\n        'msno': 'object', 'song_id': 'object', \n        'source_system_tab': 'object', 'source_screen_name': 'object', 'source_type': 'object',\n        'target': 'uint8',\n        'song_length': 'float32', # Thêm cái này\n        'artist_name': 'object', 'composer': 'object', 'lyricist': 'object',\n        'city': 'object', # Để object để fillna dễ hơn, sau đó mới encode\n        'gender': 'object',\n        'registered_via': 'object',\n        'bd': 'int16' # Tuổi\n    }\n\n    print(\"Loading csv files...\")\n    train = pd.read_csv(f\"{CONFIG['DATA_DIR']}/train.csv\", dtype=dtypes)\n    test = pd.read_csv(f\"{CONFIG['DATA_DIR']}/test.csv\", dtype=dtypes)\n    songs = pd.read_csv(f\"{CONFIG['DATA_DIR']}/songs.csv\", dtype=dtypes)\n    members = pd.read_csv(f\"{CONFIG['DATA_DIR']}/members.csv\", dtype=dtypes)\n    \n    print(\"Merging data...\")\n    train = train.merge(songs, on='song_id', how='left')\n    train = train.merge(members, on='msno', how='left')\n    test = test.merge(songs, on='song_id', how='left')\n    test = test.merge(members, on='msno', how='left')\n    \n    del songs, members\n    gc.collect()\n    return train, test\n\ntrain_df, test_df = load_data_optimized()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T01:15:31.502400Z","iopub.status.idle":"2026-01-07T01:15:31.502717Z","shell.execute_reply.started":"2026-01-07T01:15:31.502585Z","shell.execute_reply":"2026-01-07T01:15:31.502598Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def preprocess_data(train, test):\n    print(\"Handling NaNs...\")\n    # Fill NA cho các cột quan trọng\n    str_cols = ['msno', 'song_id', 'source_system_tab', 'source_screen_name', 'source_type', \n                'artist_name', 'composer', 'lyricist', 'city', 'gender', 'registered_via']\n    \n    for col in str_cols:\n        train[col] = train[col].fillna('unknown').astype(str)\n        test[col] = test[col].fillna('unknown').astype(str)\n\n    # Xử lý tuổi (bd): Thay outlier bằng mean\n    # Tuổi hợp lệ thường từ 10-80\n    valid_idx = (train['bd'] >= 10) & (train['bd'] <= 80)\n    mean_age = int(train.loc[valid_idx, 'bd'].mean())\n    \n    train.loc[~valid_idx, 'bd'] = mean_age\n    test.loc[(test['bd'] < 10) | (test['bd'] > 80), 'bd'] = mean_age\n\n    # Tạo Text Feat cho SBERT\n    print(\"Creating Text Features...\")\n    train['text_feat'] = train['artist_name'] + \" \" + train['composer'] + \" \" + train['lyricist']\n    test['text_feat'] = test['artist_name'] + \" \" + test['composer'] + \" \" + test['lyricist']\n\n    # Label Encoding (Bắt buộc cho Embedding Layer của PyTorch)\n    print(\"Encoding Categoricals...\")\n    enc_cols = ['msno', 'song_id', 'source_system_tab', 'source_screen_name', 'source_type', \n                'city', 'gender', 'registered_via']\n    \n    encoders = {}\n    for col in tqdm(enc_cols):\n        le = LabelEncoder()\n        full_vals = pd.concat([train[col], test[col]]).unique()\n        le.fit(full_vals)\n        train[col] = le.transform(train[col])\n        test[col] = le.transform(test[col])\n        encoders[col] = le\n        \n    return train, test, encoders\n\ntrain_df, test_df, encoders = preprocess_data(train_df, test_df)\n\nN_USERS = len(encoders['msno'].classes_)\nN_ITEMS = len(encoders['song_id'].classes_)\nprint(f\"Num Users: {N_USERS}, Num Items: {N_ITEMS}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T01:15:31.504873Z","iopub.status.idle":"2026-01-07T01:15:31.505398Z","shell.execute_reply.started":"2026-01-07T01:15:31.505051Z","shell.execute_reply":"2026-01-07T01:15:31.505135Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def precompute_sbert_embeddings(df_list, n_items):\n    print(\"Loading SBERT Model...\")\n    # SỬA LỖI: Dùng model chuẩn Multilingual\n    \n    sbert = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2', device='cuda')\n   \n    \n    print(\"Extracting unique songs...\")\n    all_df = pd.concat(df_list)[['song_id', 'text_feat']].drop_duplicates('song_id')\n    \n    song_texts = [\"unknown\"] * n_items\n    for idx, row in tqdm(all_df.iterrows(), total=len(all_df)):\n        sid = int(row['song_id'])\n        song_texts[sid] = str(row['text_feat'])\n        \n    print(\"Encoding songs with SBERT...\")\n    embeddings = sbert.encode(\n        song_texts,\n        batch_size=512,\n        show_progress_bar=True,\n        convert_to_numpy=True,\n        normalize_embeddings=True\n    )\n    \n    del sbert\n    torch.cuda.empty_cache()\n    gc.collect()\n    return embeddings\n\nsbert_embeddings = precompute_sbert_embeddings([train_df, test_df], N_ITEMS)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T01:15:31.508184Z","iopub.status.idle":"2026-01-07T01:15:31.508640Z","shell.execute_reply.started":"2026-01-07T01:15:31.508442Z","shell.execute_reply":"2026-01-07T01:15:31.508461Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class NeuMFDataset(Dataset):\n    def __init__(self, df, is_train=True):\n        self.users = df['msno'].values\n        self.items = df['song_id'].values\n        \n        # Context\n        self.tabs = df['source_system_tab'].values\n        self.screens = df['source_screen_name'].values\n        self.types = df['source_type'].values\n        \n        # User Meta\n        self.cities = df['city'].values\n        self.genders = df['gender'].values\n        self.reg_via = df['registered_via'].values\n        \n        self.is_train = is_train\n        if is_train:\n            self.targets = df['target'].values\n            \n    def __len__(self):\n        return len(self.users)\n    \n    def __getitem__(self, i):\n        data = {\n            'user': torch.tensor(self.users[i], dtype=torch.long),\n            'item': torch.tensor(self.items[i], dtype=torch.long),\n            \n            # Context\n            'tab': torch.tensor(self.tabs[i], dtype=torch.long),\n            'screen': torch.tensor(self.screens[i], dtype=torch.long),\n            'type': torch.tensor(self.types[i], dtype=torch.long),\n            \n            # User Meta\n            'city': torch.tensor(self.cities[i], dtype=torch.long),\n            'gender': torch.tensor(self.genders[i], dtype=torch.long),\n            'reg_via': torch.tensor(self.reg_via[i], dtype=torch.long),\n        }\n        \n        if self.is_train:\n            return data, torch.tensor(self.targets[i], dtype=torch.float)\n        return data\n\n# Model Hybrid NeuMF + SBERT + User Offset\nclass HybridNeuMF_Full(nn.Module):\n    def __init__(self, n_users, n_items, pretrained_sbert, cfg):\n        super().__init__()\n        dim = cfg['EMBEDDING_DIM']\n        \n        # 1. GMF (Dot Product)\n        self.gmf_user = nn.Embedding(n_users, dim)\n        self.gmf_item = nn.Embedding(n_items, dim)\n        \n        # 2. MLP User Part (User Offset)\n        self.mlp_user = nn.Embedding(n_users, dim)\n        \n        # User Metadata Embeddings\n        self.city_emb = nn.Embedding(30, 8)\n        self.gender_emb = nn.Embedding(5, 4)\n        self.reg_emb = nn.Embedding(10, 4)\n        self.meta_proj = nn.Linear(8+4+4, dim) # Chiếu về cùng chiều với User Emb\n        \n        # 3. MLP Item Part (SBERT)\n        self.mlp_item = nn.Embedding(n_items, dim)\n        self.sbert_emb = nn.Embedding.from_pretrained(torch.FloatTensor(pretrained_sbert), freeze=True)\n        self.sbert_proj = nn.Sequential(\n            nn.Linear(384, 128), nn.ReLU(),\n            nn.Linear(128, cfg['TEXT_PROJ_DIM']) # Về 32 chiều\n        )\n        \n        # 4. Context Part\n        self.tab_emb = nn.Embedding(50, 8)\n        self.screen_emb = nn.Embedding(50, 8)\n        self.type_emb = nn.Embedding(50, 8)\n        \n        # 5. Fusion MLP\n        # Input: User(64) + Item(64) + SBERT(32) + Context(24) = 184\n        mlp_in_dim = dim * 2 + cfg['TEXT_PROJ_DIM'] + 24\n        \n        self.mlp = nn.Sequential(\n            nn.Linear(mlp_in_dim, 128), nn.BatchNorm1d(128), nn.ReLU(), nn.Dropout(0.3),\n            nn.Linear(128, 64), nn.BatchNorm1d(64), nn.ReLU(), nn.Dropout(0.3),\n            nn.Linear(64, 32), nn.ReLU()\n        )\n        \n        # Final: GMF(64) + MLP(32)\n        self.final = nn.Linear(dim + 32, 1)\n\n    def forward(self, data):\n        # GMF Branch\n        u_gmf = self.gmf_user(data['user'])\n        i_gmf = self.gmf_item(data['item'])\n        gmf_out = u_gmf * i_gmf\n        \n        # MLP Branch - User (Offset Trick)\n        u_mlp = self.mlp_user(data['user'])\n        # Metadata\n        meta = torch.cat([\n            self.city_emb(data['city']), \n            self.gender_emb(data['gender']), \n            self.reg_emb(data['reg_via'])\n        ], dim=1)\n        u_meta = self.meta_proj(meta)\n        u_vec = u_mlp + u_meta # Cộng vector ID và vector Metadata\n        \n        # MLP Branch - Item (SBERT)\n        i_mlp = self.mlp_item(data['item'])\n        txt = self.sbert_proj(self.sbert_emb(data['item']))\n        \n        # Context\n        ctx = torch.cat([\n            self.tab_emb(data['tab']),\n            self.screen_emb(data['screen']),\n            self.type_emb(data['type'])\n        ], dim=1)\n        \n        # Concatenate All for MLP\n        mlp_in = torch.cat([u_vec, i_mlp, txt, ctx], dim=1)\n        mlp_out = self.mlp(mlp_in)\n        \n        # Final Fusion\n        out = self.final(torch.cat([gmf_out, mlp_out], dim=1))\n        return out\n\n# Khởi tạo\nmodel = HybridNeuMF_Full(N_USERS, N_ITEMS, sbert_embeddings, CONFIG).to(CONFIG['DEVICE'])\noptimizer = torch.optim.AdamW(model.parameters(), lr=CONFIG['LR'], weight_decay=1e-5) # AdamW tốt hơn\ncriterion = nn.BCEWithLogitsLoss()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T01:15:31.510063Z","iopub.status.idle":"2026-01-07T01:15:31.510456Z","shell.execute_reply.started":"2026-01-07T01:15:31.510248Z","shell.execute_reply":"2026-01-07T01:15:31.510264Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\n\n# To disable parallelism\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T01:15:31.511719Z","iopub.status.idle":"2026-01-07T01:15:31.511967Z","shell.execute_reply.started":"2026-01-07T01:15:31.511846Z","shell.execute_reply":"2026-01-07T01:15:31.511857Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import itertools\nimport pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\nimport gc\n\nprint(\"--- SETUP DEEP LEARNING GRID SEARCH ---\")\n\n# 1. Lấy mẫu 10% dữ liệu (Sample)\n# train_df là biến có từ bước Preprocessing\nSAMPLE_SIZE = int(len(train_df) * 0.1)\nsample_idx = np.random.choice(len(train_df), size=SAMPLE_SIZE, replace=False)\ndf_sample = train_df.iloc[sample_idx].copy()\n\n# Chia Train/Val cho Grid Search\ntrain_sub, val_sub = train_test_split(df_sample, test_size=0.2, random_state=42)\n\n# Reset index để Dataset class hoạt động đúng\ntrain_sub = train_sub.reset_index(drop=True)\nval_sub = val_sub.reset_index(drop=True)\n\nprint(f\"Grid Search Data: Train={len(train_sub)}, Val={len(val_sub)}\")\n\n# 2. Hàm Train nhanh (1 cấu hình tham số)\ndef run_trial(params, train_df, val_df):\n    # Cấu hình thử nghiệm\n    temp_cfg = CONFIG.copy()\n    temp_cfg.update(params) # Ghi đè tham số cần test\n    \n    # Tạo Dataset/Loader\n    train_ds = NeuMFDataset(train_df, is_train=True)\n    val_ds = NeuMFDataset(val_df, is_train=True)\n    \n    # Batch size lớn cho nhanh\n    t_loader = DataLoader(train_ds, batch_size=4096, shuffle=True)\n    v_loader = DataLoader(val_ds, batch_size=4096, shuffle=False)\n    \n    # Khởi tạo Model\n    model = HybridNeuMF_Full(N_USERS, N_ITEMS, sbert_embeddings, temp_cfg).to(CONFIG['DEVICE'])\n    \n    # Optimizer & Loss\n    optimizer = torch.optim.AdamW(model.parameters(), lr=temp_cfg['LR'], weight_decay=temp_cfg['WEIGHT_DECAY'])\n    criterion = nn.BCEWithLogitsLoss()\n    \n    # Train ngắn hạn (3 Epochs là đủ để biết model có tiềm năng không)\n    best_auc = 0\n    for epoch in range(3):\n        model.train()\n        for batch, target in t_loader:\n            batch = {k: v.to(CONFIG['DEVICE']) for k, v in batch.items()}\n            target = target.to(CONFIG['DEVICE']).unsqueeze(1)\n            \n            optimizer.zero_grad()\n            out = model(batch)\n            loss = criterion(out, target)\n            loss.backward()\n            optimizer.step()\n            \n        # Validate\n        model.eval()\n        preds, targets = [], []\n        with torch.no_grad():\n            for batch, target in v_loader:\n                batch = {k: v.to(CONFIG['DEVICE']) for k, v in batch.items()}\n                out = model(batch)\n                preds.extend(torch.sigmoid(out).cpu().numpy())\n                targets.extend(target.numpy())\n        \n        auc = roc_auc_score(targets, preds)\n        if auc > best_auc:\n            best_auc = auc\n            \n    # Dọn dẹp VRAM\n    del model, optimizer, t_loader, v_loader\n    torch.cuda.empty_cache()\n    \n    return best_auc","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T01:15:31.513390Z","iopub.status.idle":"2026-01-07T01:15:31.513931Z","shell.execute_reply.started":"2026-01-07T01:15:31.513577Z","shell.execute_reply":"2026-01-07T01:15:31.513595Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- ĐỊNH NGHĨA KHÔNG GIAN TÌM KIẾM ---\nparam_grid = {\n    'EMBEDDING_DIM': [32, 64],        # Kích thước vector ẩn\n    'LR': [0.001, 0.0005],            # Tốc độ học\n    'WEIGHT_DECAY': [1e-4, 1e-5],     # Chống overfit (L2 Regularization)\n    'TEXT_PROJ_DIM': [16, 32]         # Kích thước nén của SBERT\n}\n\n# Tạo danh sách các tổ hợp\nkeys, values = zip(*param_grid.items())\ncombinations = [dict(zip(keys, v)) for v in itertools.product(*values)]\n\nprint(f\"Total combinations to test: {len(combinations)}\")\nprint(\"-\" * 50)\n\nbest_score = 0\nbest_params = {}\n\n# --- VÒNG LẶP ---\nfor i, params in enumerate(combinations):\n    print(f\"Trial {i+1}/{len(combinations)}: {params}\")\n    \n    try:\n        score = run_trial(params, train_sub, val_sub)\n        print(f\"--> Result AUC: {score:.4f}\")\n        \n        if score > best_score:\n            best_score = score\n            best_params = params\n            print(\">>> NEW BEST FOUND! <<<\")\n            \n    except Exception as e:\n        print(f\"Trial failed: {e}\")\n        \nprint(\"=\" * 50)\nprint(\"GRID SEARCH COMPLETED\")\nprint(f\"Best AUC: {best_score:.4f}\")\nprint(\"Best Params:\", best_params)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T01:15:31.515930Z","iopub.status.idle":"2026-01-07T01:15:31.516249Z","shell.execute_reply.started":"2026-01-07T01:15:31.516082Z","shell.execute_reply":"2026-01-07T01:15:31.516100Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nprint(\"--- FIXING DATALOADERS ---\")\n\n# 1. Cắt 20% từ tập TRAIN gốc ra làm tập Validation (để có target mà đánh giá)\n# Tuyệt đối không dùng test_df ở đây vì test_df không có target\ntrain_split, val_split = train_test_split(train_df, test_size=0.2, random_state=42)\n\nprint(f\"Train size: {len(train_split)}\")\nprint(f\"Val size:   {len(val_split)}\")\n\n# 2. Khởi tạo Dataset\n# Lưu ý: Cả 2 đều dùng is_train=True vì chúng đều có cột target\ntrain_ds = NeuMFDataset(train_split, is_train=True)\nval_ds = NeuMFDataset(val_split, is_train=True) \n\n# 3. Tạo DataLoader\ntrain_loader = DataLoader(train_ds, batch_size=CONFIG['BATCH_SIZE'], shuffle=True, num_workers=2)\nval_loader = DataLoader(val_ds, batch_size=CONFIG['BATCH_SIZE'], shuffle=False, num_workers=2)\n\nprint(\"✅ DONE! DataLoaders are ready. Now you can run the Training Loop.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T01:15:31.517758Z","iopub.status.idle":"2026-01-07T01:15:31.518108Z","shell.execute_reply.started":"2026-01-07T01:15:31.517973Z","shell.execute_reply":"2026-01-07T01:15:31.517990Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Helper Class để dừng sớm\nclass EarlyStopping:\n    def __init__(self, patience=3):\n        self.patience = patience\n        self.best_score = 0\n        self.counter = 0\n        self.early_stop = False\n\n    def __call__(self, val_auc, model):\n        if val_auc > self.best_score:\n            self.best_score = val_auc\n            torch.save(model.state_dict(), 'best_model.pth')\n            self.counter = 0\n        else:\n            self.counter += 1\n            if self.counter >= self.patience:\n                self.early_stop = True\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T01:15:31.518942Z","iopub.status.idle":"2026-01-07T01:15:31.519198Z","shell.execute_reply.started":"2026-01-07T01:15:31.519067Z","shell.execute_reply":"2026-01-07T01:15:31.519080Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\n\n# --- KHỞI TẠO LIST LƯU LỊCH SỬ (QUAN TRỌNG) ---\nhistory = {\n    'train_loss': [],\n    'val_auc': [],\n    'lr': []\n}\n# ----------------------------------------------\n\nif 'PATIENCE' not in CONFIG:\n    CONFIG['PATIENCE'] = 3\n\nscheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=1, verbose=True)\nstopper = EarlyStopping(patience=CONFIG['PATIENCE'])\n\nprint(\"--- START TRAINING ---\")\n\nfor epoch in range(CONFIG['EPOCHS']):\n    model.train()\n    total_loss = 0\n    \n    # TRAIN\n    for data, target in tqdm(train_loader, desc=f\"Epoch {epoch+1} Train\"):\n        data = {k: v.to(CONFIG['DEVICE']) for k, v in data.items()}\n        target = target.to(CONFIG['DEVICE']).unsqueeze(1)\n        \n        optimizer.zero_grad()\n        output = model(data)\n        loss = criterion(output, target)\n        loss.backward()\n        optimizer.step()\n        \n        total_loss += loss.item()\n        \n    # VALIDATION\n    model.eval()\n    targets, preds = [], []\n    with torch.no_grad():\n        for data, target in val_loader:\n            data = {k: v.to(CONFIG['DEVICE']) for k, v in data.items()}\n            output = model(data)\n            \n            preds.extend(torch.sigmoid(output).cpu().numpy())\n            targets.extend(target.cpu().numpy()) \n            \n    val_auc = roc_auc_score(targets, preds)\n    avg_loss = total_loss / len(train_loader)\n    current_lr = optimizer.param_groups[0]['lr']\n\n    # --- LƯU GIÁ TRỊ VÀO HISTORY (QUAN TRỌNG) ---\n    history['train_loss'].append(avg_loss)\n    history['val_auc'].append(val_auc)\n    history['lr'].append(current_lr)\n    # ---------------------------------------------\n    \n    print(f\"Epoch {epoch+1} | Loss: {avg_loss:.4f} | Val AUC: {val_auc:.4f} | LR: {current_lr}\")\n    \n    scheduler.step(val_auc)\n    \n    stopper(val_auc, model)\n    if stopper.early_stop:\n        print(f\"Early stopping triggered! Best AUC: {stopper.best_score:.4f}\")\n        break\n\nmodel.load_state_dict(torch.load('best_model.pth'))\nprint(\"Best model loaded.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T01:15:31.519971Z","iopub.execute_input":"2026-01-07T01:15:31.520202Z","iopub.status.idle":"2026-01-07T01:15:34.004624Z","shell.execute_reply.started":"2026-01-07T01:15:31.520182Z","shell.execute_reply":"2026-01-07T01:15:34.000900Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_47/4089342235.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_scheduler\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mReduceLROnPlateau\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# --- KHỞI TẠO LIST LƯU LỊCH SỬ (QUAN TRỌNG) ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m history = {\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    403\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mUSE_GLOBAL_DEPS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m         \u001b[0m_load_global_deps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_lock_unlock_module\u001b[0;34m(name)\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":6},{"cell_type":"code","source":"# Vẽ biểu đồ\nplt.figure(figsize=(12, 5))\n\n# Biểu đồ 1: Training Loss\nplt.subplot(1, 2, 1)\nplt.plot(history['train_loss'], label='Train Loss', color='red', marker='o')\nplt.title('Training Loss over Epochs')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.grid(True)\n\n# Biểu đồ 2: Validation AUC\nplt.subplot(1, 2, 2)\nplt.plot(history['val_auc'], label='Val AUC', color='blue', marker='o')\nplt.title('Validation AUC over Epochs')\nplt.xlabel('Epochs')\nplt.ylabel('AUC')\nplt.legend()\nplt.grid(True)\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T01:15:34.005104Z","iopub.status.idle":"2026-01-07T01:15:34.005583Z","shell.execute_reply.started":"2026-01-07T01:15:34.005395Z","shell.execute_reply":"2026-01-07T01:15:34.005410Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# DeepFM\n","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass DeepFM_with_SBERT(nn.Module):\n    def __init__(self, n_users, n_items, pretrained_sbert, cfg):\n        super().__init__()\n        # Kích thước embedding chung cho phần FM (bắt buộc phải bằng nhau để tính Dot Product)\n        self.emb_dim = cfg['EMBEDDING_DIM'] # Ví dụ: 64\n        \n        # --- 1. Embedding Layers (Dùng cho cả FM và Deep part) ---\n        self.user_emb = nn.Embedding(n_users, self.emb_dim)\n        self.item_emb = nn.Embedding(n_items, self.emb_dim)\n        \n        # Context Embeddings\n        self.tab_emb = nn.Embedding(50, self.emb_dim)\n        self.screen_emb = nn.Embedding(50, self.emb_dim)\n        self.type_emb = nn.Embedding(50, self.emb_dim)\n        \n        # Metadata Embeddings\n        self.city_emb = nn.Embedding(30, self.emb_dim)\n        self.gender_emb = nn.Embedding(5, self.emb_dim)\n        self.reg_emb = nn.Embedding(10, self.emb_dim)\n        \n        # --- 2. FM Component (Factorization Machine) ---\n        # FM bắt các tương tác bậc 2 (User x Item, User x Context, Item x Context...)\n        # Không cần trọng số học thêm, chỉ là thuật toán\n        \n        # --- 3. Deep Component (MLP) ---\n        # SBERT sẽ CHỈ đi vào phần Deep này\n        self.sbert_emb = nn.Embedding.from_pretrained(torch.FloatTensor(pretrained_sbert), freeze=True)\n        # Chiếu SBERT 384 chiều về kích thước nhỏ hơn để không át các feature khác\n        self.sbert_proj = nn.Sequential(\n            nn.Linear(384, 128),\n            nn.LayerNorm(128),\n            nn.ReLU(),\n            nn.Dropout(0.1)\n        )\n        \n        # Tính toán input dim cho MLP\n        # Tổng số field categorical = 8 (User, Item, Tab, Screen, Type, City, Gender, Reg)\n        # Input dim = (8 * emb_dim) + 128 (SBERT projected)\n        mlp_in_dim = (8 * self.emb_dim) + 128\n        \n        self.mlp = nn.Sequential(\n            nn.Linear(mlp_in_dim, 256),\n            nn.BatchNorm1d(256),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(256, 128),\n            nn.BatchNorm1d(128),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(128, 64),\n            nn.ReLU()\n        )\n        \n        self.final_linear = nn.Linear(64 + 1 + 1, 1) # +1 cho FM output, +1 cho Bias global\n\n    def forward(self, data):\n        # 1. Get Embeddings [Batch, Emb_Dim]\n        u_emb = self.user_emb(data['user'])\n        i_emb = self.item_emb(data['item'])\n        \n        tab_emb = self.tab_emb(data['tab'])\n        scr_emb = self.screen_emb(data['screen'])\n        typ_emb = self.type_emb(data['type'])\n        \n        cit_emb = self.city_emb(data['city'])\n        gen_emb = self.gender_emb(data['gender'])\n        reg_emb = self.reg_emb(data['reg_via'])\n        \n        # Stack lại thành [Batch, Num_Fields, Emb_Dim] để tính FM\n        # Num_Fields = 8\n        stacked_emb = torch.stack([\n            u_emb, i_emb, tab_emb, scr_emb, typ_emb, cit_emb, gen_emb, reg_emb\n        ], dim=1)\n        \n        # --- 2. FM Part (Vectorized Implementation) ---\n        # Công thức: 0.5 * ( (Sum of embeddings)^2 - (Sum of squared embeddings) )\n        sum_of_emb = torch.sum(stacked_emb, dim=1) # [Batch, Emb_Dim]\n        sum_of_sq_emb = torch.sum(stacked_emb ** 2, dim=1) # [Batch, Emb_Dim]\n        \n        # FM Output interactions\n        fm_out = 0.5 * (sum_of_emb ** 2 - sum_of_sq_emb) # [Batch, Emb_Dim]\n        fm_out = torch.sum(fm_out, dim=1, keepdim=True) # [Batch, 1] -> Tổng hợp lại thành 1 con số score\n        \n        # --- 3. Deep Part ---\n        # Lấy SBERT features\n        sbert_vec = self.sbert_emb(data['item']) # [Batch, 384]\n        sbert_vec = self.sbert_proj(sbert_vec)   # [Batch, 128]\n        \n        # Flatten các embedding categorical: [Batch, 8 * Emb_Dim]\n        deep_emb = stacked_emb.view(stacked_emb.size(0), -1)\n        \n        # Nối tất cả vào MLP\n        mlp_in = torch.cat([deep_emb, sbert_vec], dim=1)\n        mlp_out = self.mlp(mlp_in) # [Batch, 64]\n        \n        # --- 4. Final Combination ---\n        # Kết hợp: Deep Output + FM Output\n        # Lưu ý: FM output đóng vai trò như một \"Correction\" cho MLP\n        final_input = torch.cat([mlp_out, fm_out, torch.ones_like(fm_out)], dim=1) # Thêm bias trick nếu cần\n        logits = self.final_linear(final_input)\n        \n        return logits","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T01:15:34.007587Z","iopub.status.idle":"2026-01-07T01:15:34.008017Z","shell.execute_reply.started":"2026-01-07T01:15:34.007785Z","shell.execute_reply":"2026-01-07T01:15:34.007802Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.cuda.amp import autocast, GradScaler\nfrom sklearn.metrics import roc_auc_score\n\n# Hyperparameters\nBATCH_SIZE = 2048  # Tăng lên 4096 hoặc 8192 nếu VRAM GPU lớn (P100/T4)\nLR = 1e-3\nEPOCHS = 20\n\n# Tạo Dataset\ntrain_dataset = NeuMFDataset(train_df, is_train=True)\n# Cắt ra một phần validation từ train_df nếu bạn chưa split\n# Ở đây tôi giả sử train_df là full train, nên cắt 20% làm validation\ntrain_idx, val_idx = train_test_split(np.arange(len(train_df)), test_size=0.2, random_state=42)\n\n# Tạo Subset\ntrain_sub = torch.utils.data.Subset(train_dataset, train_idx)\nval_sub = torch.utils.data.Subset(train_dataset, val_idx)\n\n# Tạo DataLoader\ntrain_loader = DataLoader(train_sub, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\nval_loader = DataLoader(val_sub, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n\n# Khởi tạo Model\nmodel = DeepFM_with_SBERT(N_USERS, N_ITEMS, sbert_embeddings, CONFIG).to(CONFIG['DEVICE'])\n\n# Loss & Optimizer\ncriterion = nn.BCEWithLogitsLoss() # Tự động có Sigmoid bên trong, ổn định hơn\noptimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=1e-4)\n\n# Scheduler: Giảm LR nếu loss không giảm\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.1, patience=2, verbose=True)\n\n# Scaler cho Mixed Precision (Chạy nhanh hơn trên GPU T4/P100 của Kaggle)\nscaler = GradScaler()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T01:15:34.008914Z","iopub.status.idle":"2026-01-07T01:15:34.009266Z","shell.execute_reply.started":"2026-01-07T01:15:34.009086Z","shell.execute_reply":"2026-01-07T01:15:34.009102Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"best_auc = 0\nearly_stop_count = 0\nPATIENCE = 4\n\nprint(\"Start Training...\")\nfor epoch in range(EPOCHS):\n    model.train()\n    train_loss = 0\n    \n    # --- TRAINING PHASE ---\n    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} [Train]\")\n    for batch_data, targets in pbar:\n        # Chuyển data sang GPU\n        inputs = {k: v.to(CONFIG['DEVICE']) for k, v in batch_data.items()}\n        targets = targets.to(CONFIG['DEVICE']).unsqueeze(1) # [Batch, 1]\n        \n        optimizer.zero_grad()\n        \n        # Mixed Precision Forward\n        with autocast():\n            outputs = model(inputs)\n            loss = criterion(outputs, targets)\n        \n        # Mixed Precision Backward\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n        \n        train_loss += loss.item()\n        pbar.set_postfix({'loss': loss.item()})\n        \n    avg_train_loss = train_loss / len(train_loader)\n\n    # --- VALIDATION PHASE ---\n    model.eval()\n    val_preds = []\n    val_targets = []\n    \n    with torch.no_grad():\n        for batch_data, targets in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} [Val]\"):\n            inputs = {k: v.to(CONFIG['DEVICE']) for k, v in batch_data.items()}\n            \n            with autocast():\n                outputs = model(inputs)\n                \n            # Dùng Sigmoid để đưa về xác suất 0-1 cho tính AUC\n            preds = torch.sigmoid(outputs).cpu().numpy()\n            val_preds.extend(preds)\n            val_targets.extend(targets.numpy())\n            \n    val_auc = roc_auc_score(val_targets, val_preds)\n    print(f\"Epoch {epoch+1} | Loss: {avg_train_loss:.4f} | Val AUC: {val_auc:.5f}\")\n    \n    # --- CHECKPOINT & EARLY STOPPING ---\n    scheduler.step(val_auc) # Giảm LR dựa trên Val AUC\n    \n    if val_auc > best_auc:\n        best_auc = val_auc\n        early_stop_count = 0\n        torch.save(model.state_dict(), 'best_deepfm_sbert.pth')\n        print(\">>> Model Saved!\")\n    else:\n        early_stop_count += 1\n        print(f\"No improvement for {early_stop_count} epochs.\")\n        if early_stop_count >= PATIENCE:\n            print(\"Early Stopping triggered!\")\n            break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T01:15:34.010785Z","iopub.status.idle":"2026-01-07T01:15:34.011227Z","shell.execute_reply.started":"2026-01-07T01:15:34.011057Z","shell.execute_reply":"2026-01-07T01:15:34.011076Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load model tốt nhất\nmodel.load_state_dict(torch.load('best_deepfm_sbert.pth'))\nmodel.eval()\n\n# Tạo Test Dataset & Loader\ntest_dataset = NeuMFDataset(test_df, is_train=False) # is_train=False để không cần cột target\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE * 2, shuffle=False, num_workers=2)\n\nall_preds = []\n\nprint(\"Predicting on Test set...\")\nwith torch.no_grad():\n    for batch_data in tqdm(test_loader):\n        inputs = {k: v.to(CONFIG['DEVICE']) for k, v in batch_data.items()}\n        \n        with autocast():\n            outputs = model(inputs)\n            \n        preds = torch.sigmoid(outputs).cpu().numpy()\n        all_preds.extend(preds.flatten())\n\n# Lưu file Submission\nsubmission = pd.DataFrame({\n    'id': pd.read_csv(f\"{CONFIG['DATA_DIR']}/test.csv\")['id'], # Lấy lại ID gốc\n    'target': all_preds\n})\n\nsubmission.to_csv('submission_deepfm.csv', index=False)\nprint(\"Submission saved to submission_deepfm.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T01:15:34.013930Z","iopub.status.idle":"2026-01-07T01:15:34.014740Z","shell.execute_reply.started":"2026-01-07T01:15:34.014589Z","shell.execute_reply":"2026-01-07T01:15:34.014608Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"--------------------------------------------------------------------------------------\n","metadata":{}},{"cell_type":"code","source":"print(\"Generating Submission...\")\n\n# Test Dataset\ntest_ds = NeuMFDataset(test_df, is_train=False)\ntest_loader = DataLoader(test_ds, batch_size=CONFIG['BATCH_SIZE']*2, shuffle=False)\n\nmodel.eval()\nall_preds = []\n\nwith torch.no_grad():\n    for data in tqdm(test_loader, desc=\"Predicting Test\"):\n        data = {k: v.to(CONFIG['DEVICE']) for k, v in data.items()}\n        output = model(data)\n        all_preds.extend(torch.sigmoid(output).cpu().numpy().flatten())\n\nsubmission = pd.DataFrame({\n    'id': test_df['id'],\n    'target': all_preds\n})\n\nsubmission.to_csv('submission_sbert_neumf.csv', index=False)\nprint(\"Submission saved! Download file 'submission_sbert_neumf.csv'\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T01:15:34.015361Z","iopub.status.idle":"2026-01-07T01:15:34.016502Z","shell.execute_reply.started":"2026-01-07T01:15:34.016289Z","shell.execute_reply":"2026-01-07T01:15:34.016311Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.save(model.state_dict(), 'model_weights.pth')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T01:15:34.017700Z","iopub.status.idle":"2026-01-07T01:15:34.018191Z","shell.execute_reply.started":"2026-01-07T01:15:34.017998Z","shell.execute_reply":"2026-01-07T01:15:34.018016Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\n# 1. Danh sách các cột \"thủ phạm\" gây lỗi (đang là object)\ntext_cols = ['genre_ids', 'artist_name', 'composer', 'lyricist']\n\nprint(\"Fixing object columns for LightGBM...\")\n\nfor col in text_cols:\n    print(f\"Encoding {col}...\")\n    le = LabelEncoder()\n    \n    # Chuyển hết về string để tránh lỗi type hỗn hợp\n    train_df[col] = train_df[col].fillna('unknown').astype(str)\n    test_df[col] = test_df[col].fillna('unknown').astype(str)\n    \n    # Fit trên toàn bộ dữ liệu\n    full_vals = pd.concat([train_df[col], test_df[col]]).unique()\n    le.fit(full_vals)\n    \n    # SỬA LỖI Ở ĐÂY:\n    # Bước 1: Transform ra số nguyên (int) và gán vào DataFrame\n    train_df[col] = le.transform(train_df[col])\n    test_df[col] = le.transform(test_df[col])\n    \n    # Bước 2: Ép kiểu 'category' trên Pandas Series (Pandas mới hiểu lệnh này)\n    train_df[col] = train_df[col].astype('category')\n    test_df[col] = test_df[col].astype('category')\n\nprint(\"All text columns converted to numbers/categories!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T01:15:34.019818Z","iopub.status.idle":"2026-01-07T01:15:34.020451Z","shell.execute_reply.started":"2026-01-07T01:15:34.020244Z","shell.execute_reply":"2026-01-07T01:15:34.020262Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_old = lgb.Booster(model_file='/kaggle/input/cur-lightgbm/other/default/1/model_checkpoint.txt')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T01:15:34.021140Z","iopub.status.idle":"2026-01-07T01:15:34.021434Z","shell.execute_reply.started":"2026-01-07T01:15:34.021278Z","shell.execute_reply":"2026-01-07T01:15:34.021293Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import lightgbm as lgb\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import PCA\nimport pandas as pd\nimport numpy as np\nimport gc\n\nprint(\"--- STEP 1: FEATURE ENGINEERING (THE ULTIMATE DATASET) ---\")\n\n# 1. Reset về Base Features\nbase_features = [\n    'msno', 'song_id', 'source_system_tab', 'source_screen_name', \n    'source_type', 'song_length', 'genre_ids', 'artist_name', \n    'composer', 'lyricist', 'language', 'city', 'bd', \n    'gender', 'registered_via'\n]\n\n# Copy dữ liệu gốc\ntrain_final = train_df[base_features].copy()\ntest_final = test_df[base_features].copy()\ntarget = train_df['target'].values\n\n# ======================================================\n# A. THÊM SBERT FEATURES (CONTENT)\n# ======================================================\nif 'sbert_embeddings' in locals():\n    print(\">>> Adding SBERT PCA Features...\")\n    pca = PCA(n_components=16, random_state=42)\n    sbert_pca = pca.fit_transform(sbert_embeddings)\n    \n    # Map vào train/test\n    train_sbert = sbert_pca[train_df['song_id'].values.astype(int)]\n    test_sbert = sbert_pca[test_df['song_id'].values.astype(int)]\n    \n    # Tạo tên cột & Ghép\n    sbert_cols = [f'sbert_{i}' for i in range(16)]\n    train_final = pd.concat([train_final, pd.DataFrame(train_sbert, columns=sbert_cols, index=train_final.index)], axis=1)\n    test_final = pd.concat([test_final, pd.DataFrame(test_sbert, columns=sbert_cols, index=test_final.index)], axis=1)\n    \n    del sbert_pca, train_sbert, test_sbert\n    gc.collect()\nelse:\n    print(\"!!! WARNING: Skipping SBERT features.\")\n\n# ======================================================\n# B. THÊM COUNT & INTERACTION FEATURES (STATISTICS)\n# ======================================================\nprint(\">>> Adding Count & Interaction Features...\")\n\n# Nối tạm để tính toán toàn cục\nlen_train = len(train_final)\nfull_data = pd.concat([train_final, test_final], axis=0)\n\n# Danh sách 1: Count cơ bản (Độ phổ biến)\nbasic_counts = ['song_id', 'msno', 'artist_name', 'source_type', 'source_screen_name']\n\n# Danh sách 2: Interaction Count (Sở thích cá nhân)\ninteractions = [\n    ['msno', 'artist_name'],      # User x Artist\n    ['msno', 'genre_ids'],        # User x Genre\n    ['msno', 'source_type'],      # User x Source\n    ['msno', 'source_system_tab'] # User x Tab\n]\n\n# Thực hiện đếm\n# 1. Basic Counts\nfor col in basic_counts:\n    vc = full_data[col].value_counts().to_dict()\n    full_data[f'{col}_count'] = full_data[col].map(vc).astype('int32')\n\n# 2. Interaction Counts\nfor col1, col2 in interactions:\n    fname = f'{col1}_{col2}_count'\n    # Groupby size\n    cnt = full_data.groupby([col1, col2]).size().reset_index(name='cnt')\n    # Merge\n    full_data = pd.merge(full_data, cnt, on=[col1, col2], how='left')\n    full_data.rename(columns={'cnt': fname}, inplace=True)\n    full_data[fname] = full_data[fname].fillna(0).astype('int32')\n\n# Tách ra lại (Lúc này full_data đã chứa ĐẦY ĐỦ tất cả features)\ntrain_final = full_data.iloc[:len_train]\ntest_final = full_data.iloc[len_train:]\n\ndel full_data\ngc.collect()\n\nprint(f\"✅ DONE! Final Feature Count: {train_final.shape[1]}\")\n# Bây giờ train_final đã sẵn sàng cho bước Train","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T01:15:34.022906Z","iopub.status.idle":"2026-01-07T01:15:34.023260Z","shell.execute_reply.started":"2026-01-07T01:15:34.023118Z","shell.execute_reply":"2026-01-07T01:15:34.023138Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"\\n--- STEP 2: TRAINING LIGHTGBM ---\")\n\n# Split Validation (Dùng train_final chứ không phải v2 v3 gì cả)\nX_train, X_val, y_train, y_val = train_test_split(train_final_v3, target, test_size=0.2, random_state=42)\n\nlgb_train = lgb.Dataset(X_train, y_train)\nlgb_val = lgb.Dataset(X_val, y_val, reference=lgb_train)\n\nparams_hybrid = {\n    'objective': 'binary',\n    'metric': 'auc',\n    'boosting': 'gbdt',\n    'learning_rate': 0.05,\n    'verbose': 0,\n    'num_leaves': 128,\n    'bagging_fraction': 0.8,\n    'feature_fraction': 0.7,\n    'min_data_in_leaf': 100,\n    'lambda_l2': 1.0,\n    'max_depth': 12,\n    'n_jobs': -1\n}\n\nmodel = lgb.train(\n    params_hybrid,\n    lgb_train,\n    num_boost_round=9000,\n    valid_sets=[lgb_train, lgb_val],\n    valid_names=['train', 'val'],\n    callbacks=[\n        lgb.early_stopping(stopping_rounds=100),\n        lgb.log_evaluation(period=100)\n    ]\n)\n\nprint(f\"Final Val AUC: {model.best_score['val']['auc']:.4f}\")\n\n# Predict & Submit\nprint(\"Predicting...\")\npreds = model.predict(test_final_v3)\n\nsubmission = pd.DataFrame()\nsubmission['id'] = test_df['id']\nsubmission['target'] = preds\n\nsubmission.to_csv('submission_hybrid_ultimate.csv', index=False)\nprint(\"Saved 'submission_hybrid_ultimate.csv'\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T01:15:34.024879Z","iopub.status.idle":"2026-01-07T01:15:34.025160Z","shell.execute_reply.started":"2026-01-07T01:15:34.025027Z","shell.execute_reply":"2026-01-07T01:15:34.025047Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"XGB","metadata":{}},{"cell_type":"code","source":"!pip install optuna-integration","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T01:15:34.026515Z","iopub.status.idle":"2026-01-07T01:15:34.026764Z","shell.execute_reply.started":"2026-01-07T01:15:34.026644Z","shell.execute_reply":"2026-01-07T01:15:34.026655Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import xgboost as xgb\nimport optuna\nimport pandas as pd\nimport numpy as np\nimport gc\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom optuna.visualization import plot_optimization_history, plot_param_importances\n\nprint(\"--- XGBOOST HYPERPARAMETER TUNING (NO PRUNING CALLBACK) ---\")\n\n# 1. CHUẨN BỊ DỮ LIỆU\n# (Đảm bảo train_final và target đã được load trước đó)\nX_train_split, X_val_split, y_train_split, y_val_split = train_test_split(\n    train_final, target, test_size=0.2, random_state=42\n)\n\ndtrain_opt = xgb.DMatrix(X_train_split, label=y_train_split)\ndval_opt = xgb.DMatrix(X_val_split, label=y_val_split)\n\ndel X_train_split, X_val_split, y_train_split, y_val_split\ngc.collect()\n\n# 2. ĐỊNH NGHĨA HÀM MỤC TIÊU (Đã sửa lỗi)\ndef objective(trial):\n    params = {\n        'objective': 'binary:logistic',\n        'eval_metric': 'auc',\n        'tree_method': 'gpu_hist',  # Dùng GPU\n        'random_state': 42,\n        'verbosity': 0,\n        \n        # Search Space\n        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n        'max_depth': trial.suggest_int('max_depth', 4, 12),\n        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n        'gamma': trial.suggest_float('gamma', 0.0, 5.0),\n        'lambda': trial.suggest_float('lambda', 1e-3, 10.0, log=True),\n        'alpha': trial.suggest_float('alpha', 1e-3, 10.0, log=True)\n    }\n    \n    # --- ĐÃ XÓA PHẦN PRUNING CALLBACK GÂY LỖI ---\n    \n    # Train model\n    model = xgb.train(\n        params,\n        dtrain_opt,\n        num_boost_round=1000,\n        evals=[(dval_opt, \"validation\")],\n        early_stopping_rounds=50,\n        verbose_eval=False\n    )\n    \n    return model.best_score\n\n# 3. CHẠY TỐI ƯU HÓA\nprint(\"Starting Optimization...\")\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=50)\n\nprint(\"\\n--- KẾT QUẢ TỐT NHẤT ---\")\nprint(f\"Best AUC: {study.best_value:.4f}\")\nprint(\"Best Params:\", study.best_params)\n\n# 4. VẼ BIỂU ĐỒ\ntry:\n    print(\"\\n--- PLOTTING ---\")\n    fig1 = plot_optimization_history(study)\n    fig1.show()\n    \n    fig2 = plot_param_importances(study)\n    fig2.show()\nexcept Exception as e:\n    print(f\"Không thể vẽ biểu đồ: {e}\")\n\n# 5. TRAIN FINAL MODEL\nprint(\"\\n--- TRAINING FINAL MODEL ---\")\nfinal_params = study.best_params\nfinal_params['objective'] = 'binary:logistic'\nfinal_params['eval_metric'] = 'auc'\nfinal_params['tree_method'] = 'gpu_hist'\nfinal_params['random_state'] = 42\n\nmodel_final = xgb.train(\n    final_params,\n    dtrain_opt,\n    num_boost_round=5000,\n    evals=[(dtrain_opt, 'train'), (dval_opt, 'val')],\n    early_stopping_rounds=100,\n    verbose_eval=100\n)\n\n# Predict\nprint(\"Predicting...\")\n# dtest = xgb.DMatrix(test_final) # Bỏ comment nếu chưa tạo dtest\npreds = model_final.predict(dtest)\n\nsub = pd.DataFrame()\nsub['id'] = test_df['id']\nsub['target'] = preds\nsub.to_csv('submission_xgb_optuna.csv', index=False)\nprint(\"Saved submission_xgb_optuna.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T01:15:34.027846Z","iopub.status.idle":"2026-01-07T01:15:34.028238Z","shell.execute_reply.started":"2026-01-07T01:15:34.028060Z","shell.execute_reply":"2026-01-07T01:15:34.028077Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import xgboost as xgb\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport gc\n\nprint(\"--- TRAIN XGBOOST & PLOT HISTORY ---\")\n\n# 1. Cấu hình tham số (Best Params từ Trial 4 của bạn)\nparams = {\n    'objective': 'binary:logistic',\n    'tree_method': 'gpu_hist',  # Dùng GPU\n    'random_state': 42,\n    \n    # Chúng ta theo dõi cả 2 chỉ số: LogLoss (để xem Loss) và AUC (để xem độ chính xác)\n    'eval_metric': ['logloss', 'auc'], \n    \n    # Tham số tốt nhất bạn tìm được\n    'learning_rate': 0.0973,\n    'max_depth': 11,\n    'min_child_weight': 8,\n    'subsample': 0.879,\n    'colsample_bytree': 0.763,\n    'gamma': 0.174,\n    'lambda': 0.0093,\n    'alpha': 0.0022\n}\n\n# 2. Chuẩn bị dữ liệu (Nếu biến dtrain_opt, dval_opt đã có thì bỏ qua bước này)\n# Nếu chưa có thì chạy lại đoạn tạo DMatrix:\n# dtrain_opt = xgb.DMatrix(X_train_split, label=y_train_split)\n# dval_opt = xgb.DMatrix(X_val_split, label=y_val_split)\n\n# 3. TRAIN MODEL & LƯU LỊCH SỬ\nevals_result = {}  # Dict để chứa kết quả\n\nprint(\"Training...\")\nmodel = xgb.train(\n    params,\n    dtrain_opt,\n    num_boost_round=3000,           # Số vòng lặp\n    evals=[(dtrain_opt, 'train'), (dval_opt, 'val')],\n    evals_result=evals_result,      # <--- QUAN TRỌNG: Lưu lịch sử vào đây\n    early_stopping_rounds=100,\n    verbose_eval=100\n)\n\n# 4. VẼ BIỂU ĐỒ (VISUALIZATION)\nprint(\"\\n--- PLOTTING ---\")\nepochs = len(evals_result['train']['logloss'])\nx_axis = range(0, epochs)\n\nfig, ax = plt.subplots(1, 2, figsize=(15, 5))\n\n# Biểu đồ 1: Log Loss (Càng thấp càng tốt)\nax[0].plot(x_axis, evals_result['train']['logloss'], label='Train')\nax[0].plot(x_axis, evals_result['val']['logloss'], label='Val')\nax[0].legend()\nax[0].set_ylabel('Log Loss')\nax[0].set_title('XGBoost Log Loss')\nax[0].grid(True)\n\n# Biểu đồ 2: AUC (Càng cao càng tốt)\nax[1].plot(x_axis, evals_result['train']['auc'], label='Train')\nax[1].plot(x_axis, evals_result['val']['auc'], label='Val')\nax[1].legend()\nax[1].set_ylabel('AUC')\nax[1].set_title('XGBoost AUC')\nax[1].grid(True)\n\nplt.show()\n\n# 5. Predict & Submit (Tùy chọn)\n# preds = model.predict(dtest)\n# ... code save submission ...\n# 5. Prediction & Submission\nprint(\"Predicting...\")\npreds_xgb = model.predict(dtest)\n\nsubmission_xgb = pd.DataFrame()\nsubmission_xgb['id'] = test_df['id']\nsubmission_xgb['target'] = preds_xgb\n\nfilename_xgb = 'submission_xgboost_gpu.csv'\nsubmission_xgb.to_csv(filename_xgb, index=False)\n\nprint(f\"Saved {filename_xgb}\")\n\n# Dọn dẹp RAM\ndel dtrain, dtest, dtrain_split, dval_split, model_xgb\ngc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T01:15:34.029613Z","iopub.status.idle":"2026-01-07T01:15:34.029855Z","shell.execute_reply.started":"2026-01-07T01:15:34.029744Z","shell.execute_reply":"2026-01-07T01:15:34.029755Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nimport pandas as pd\nimport xgboost as xgb\n\nprint(\"--- FIX ERROR & PREDICT ---\")\n\n# 1. Tạo lại dtest (Nếu test_final chưa có thì load lại)\nif 'test_final' not in locals():\n    print(\"Loading test_final...\")\n    # Sửa đường dẫn nếu bạn lưu chỗ khác\n    test_final = pd.read_parquet('/kaggle/working/temporal_data/test_final.parquet') \n    \n    # Align columns: Đảm bảo cột của test giống hệt cột của train (trừ target)\n    # Lấy tên cột từ model\n    feature_names = model.feature_names\n    test_final = test_final[feature_names]\n\nprint(\"Creating DMatrix for Test...\")\ndtest = xgb.DMatrix(test_final)\n\n# 2. Predict\nprint(\"Predicting...\")\npreds_xgb = model.predict(dtest)\n\n# 3. Save Submission\nsubmission_xgb = pd.DataFrame()\n# Nếu test_ids hoặc test_df['id'] không còn, load lại từ file gốc\nif 'test_df' in locals():\n    submission_xgb['id'] = test_df['id']\nelse:\n    print(\"Loading IDs from original test file...\")\n    test_ids = pd.read_csv('/kaggle/input/kkbox-music-recommendation-challenge/test.csv', usecols=['id'])['id']\n    submission_xgb['id'] = test_ids\n\nsubmission_xgb['target'] = preds_xgb\n\nfilename = 'submission_xgboost_final_0831.csv'\nsubmission_xgb.to_csv(filename, index=False)\n\nprint(f\">>> DONE! Saved: {filename}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T01:15:34.031052Z","iopub.status.idle":"2026-01-07T01:15:34.031302Z","shell.execute_reply.started":"2026-01-07T01:15:34.031190Z","shell.execute_reply":"2026-01-07T01:15:34.031201Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submission.to_csv('submission_hybrid_ultimate.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T01:15:34.032492Z","iopub.status.idle":"2026-01-07T01:15:34.032842Z","shell.execute_reply.started":"2026-01-07T01:15:34.032666Z","shell.execute_reply":"2026-01-07T01:15:34.032681Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.save_model('model_checkpoint.txt')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T01:15:34.033620Z","iopub.status.idle":"2026-01-07T01:15:34.033962Z","shell.execute_reply.started":"2026-01-07T01:15:34.033785Z","shell.execute_reply":"2026-01-07T01:15:34.033801Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"--- CONTINUING TRAINING (EXTRA ROUNDS) ---\")\n\n# Train tiếp thêm 3000 vòng nữa\n# init_model=model: Bắt đầu từ kiến thức của model cũ\nmodel_continued = lgb.train(\n    params_hybrid,             # Dùng lại tham số cũ\n    lgb_train,\n    num_boost_round=3000,      # Train thêm 3000 vòng\n    init_model=model,          # <--- QUAN TRỌNG: Nối tiếp model cũ\n    valid_sets=[lgb_train, lgb_val],\n    valid_names=['train', 'val'],\n    callbacks=[\n        lgb.early_stopping(stopping_rounds=100),\n        lgb.log_evaluation(period=100)\n    ]\n)\n\nprint(f\"New Best Val AUC: {model_continued.best_score['val']['auc']:.4f}\")\n\n# Cập nhật lại biến model chính\nmodel = model_continued","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T01:15:34.035119Z","iopub.status.idle":"2026-01-07T01:15:34.035497Z","shell.execute_reply.started":"2026-01-07T01:15:34.035285Z","shell.execute_reply":"2026-01-07T01:15:34.035302Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.decomposition import PCA\nimport gc\n\n# Giả sử bạn đã chạy đoạn code LabelEncoder và có train_df, test_df sạch sẽ\n# và biến sbert_embeddings vẫn còn trong RAM (hoặc load lại).\n\nprint(\"--- RECONSTRUCTING 36 FEATURES FOR PREDICTION ---\")\n\n# 1. Khởi tạo lại với 15 cột cơ bản\nbase_features = [\n    'msno', 'song_id', 'source_system_tab', 'source_screen_name', \n    'source_type', 'song_length', 'genre_ids', 'artist_name', \n    'composer', 'lyricist', 'language', 'city', 'bd', \n    'gender', 'registered_via'\n]\n\n# Cần tạo cả train_final để tính toán Count Feature cho chính xác\ntrain_final = train_df[base_features].copy()\ntest_final = test_df[base_features].copy()\n\n# ======================================================\n# 2. TÁI TẠO SBERT FEATURES (16 CỘT)\n# ======================================================\n# Cần biến sbert_embeddings. Nếu bạn bị mất biến này do reset, \n# bạn buộc phải chạy lại hàm precompute_sbert_embeddings trước!\nif 'sbert_embeddings' in locals():\n    print(\">>> Adding SBERT Features...\")\n    \n    # PCA giảm chiều (Phải giống hệt lúc train)\n    pca = PCA(n_components=16, random_state=42)\n    sbert_pca = pca.fit_transform(sbert_embeddings)\n    \n    # Map vào test_final dựa trên song_id (đã label encode)\n    test_sbert_vecs = sbert_pca[test_df['song_id'].values.astype(int)]\n    \n    # Tạo DataFrame 16 cột\n    sbert_cols = [f'sbert_{i}' for i in range(16)]\n    test_sbert_df = pd.DataFrame(test_sbert_vecs, columns=sbert_cols, index=test_final.index)\n    \n    # Ghép vào\n    test_final = pd.concat([test_final, test_sbert_df], axis=1)\n    \n    # (Làm tương tự cho train để tính count bên dưới)\n    train_sbert_vecs = sbert_pca[train_df['song_id'].values.astype(int)]\n    train_sbert_df = pd.DataFrame(train_sbert_vecs, columns=sbert_cols, index=train_final.index)\n    train_final = pd.concat([train_final, train_sbert_df], axis=1)\n    \nelse:\n    raise ValueError(\"Thiếu biến 'sbert_embeddings'! Bạn cần chạy lại bước tạo SBERT trước.\")\n\n# ======================================================\n# 3. TÁI TẠO COUNT FEATURES (5 CỘT)\n# ======================================================\nprint(\">>> Adding Count Features...\")\n\n# Phải gộp Train + Test để đếm tần suất cho chuẩn xác (Global Count)\nlen_train = len(train_final)\nfull_data = pd.concat([train_final, test_final], axis=0)\n\ncount_cols = ['song_id', 'msno', 'artist_name', 'source_type', 'source_screen_name']\n\nfor col in count_cols:\n    # Đếm trên toàn bộ dữ liệu\n    vc = full_data[col].value_counts().to_dict()\n    # Map lại\n    full_data[f'{col}_count'] = full_data[col].map(vc).astype('int32')\n\n# Tách Test ra (Lúc này Test đã có đủ cột Count)\ntest_final_full = full_data.iloc[len_train:].copy()\n\n# Dọn dẹp\ndel full_data, train_final\ngc.collect()\n\nprint(f\"Final Test Shape: {test_final_full.shape}\") \n# Kết quả phải là (Số dòng, 36)\n\n# ======================================================\n# 4. DỰ ĐOÁN VÀO NỘP BÀI\n# ======================================================\nprint(\"Predicting with model_old...\")\n\n# Dùng .to_numpy() để tránh lỗi category mismatch\npreds = model_old.predict(test_final_full.to_numpy())\n\nsubmission = pd.DataFrame()\nsubmission['id'] = test_df['id']\nsubmission['target'] = preds\n\nsubmission.to_csv('submission_from_old_model_fixed.csv', index=False)\nprint(\"SUCCESS! Saved 'submission_from_old_model_fixed.csv'.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T01:15:34.037007Z","iopub.status.idle":"2026-01-07T01:15:34.037373Z","shell.execute_reply.started":"2026-01-07T01:15:34.037198Z","shell.execute_reply":"2026-01-07T01:15:34.037215Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Cat boost**","metadata":{}},{"cell_type":"code","source":"!pip install catboost","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T01:15:34.039335Z","iopub.status.idle":"2026-01-07T01:15:34.039732Z","shell.execute_reply.started":"2026-01-07T01:15:34.039537Z","shell.execute_reply":"2026-01-07T01:15:34.039554Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import catboost as cb\nimport optuna\nimport pandas as pd\nimport numpy as np\nimport gc\nfrom sklearn.model_selection import train_test_split\n\nprint(\"--- CATBOOST TUNING (FIXED TYPE ERROR) ---\")\n\n# 1. DANH SÁCH CỘT CATEGORY\n# (Đảm bảo đúng tên cột trong dataset của bạn)\ncat_cols_names = [\n    'msno', 'song_id', 'source_system_tab', 'source_screen_name', \n    'source_type', 'city', 'gender', 'registered_via', 'language', \n    'artist_name', 'first_genre_id', 'before_song_id', 'source'\n]\n# Lọc chỉ lấy những cột thực sự có trong dữ liệu\nvalid_cat_cols = [c for c in cat_cols_names if c in train_final.columns]\n\n# --- BƯỚC QUAN TRỌNG: FIX LỖI KIỂU DỮ LIỆU ---\nprint(\"Fixing categorical columns types...\")\nfor col in valid_cat_cols:\n    # 1. Fill NaN bằng giá trị đặc biệt \"-1\"\n    train_final[col] = train_final[col].fillna(-1)\n    \n    # 2. Ép kiểu về String để tránh lỗi \"24.0\"\n    # Dùng .astype(int) trước để biến 24.0 -> 24, sau đó mới -> \"24\" cho đẹp\n    try:\n        train_final[col] = train_final[col].astype(int).astype(str)\n    except:\n        # Nếu không convert được sang int (do có chữ), thì convert thẳng sang str\n        train_final[col] = train_final[col].astype(str)\n        \nprint(\"   -> Done converting types.\")\n\n# 2. CHUẨN BỊ DỮ LIỆU (Lấy mẫu 20%)\nprint(\"Sampling 20% data...\")\nX_small, _, y_small, _ = train_test_split(train_final, target, train_size=0.2, random_state=42)\nX_train, X_val, y_train, y_val = train_test_split(X_small, y_small, test_size=0.2, random_state=42)\n\n# Lấy lại index sau khi đã fix xong\ncat_features_indices = [X_train.columns.get_loc(c) for c in valid_cat_cols]\nprint(f\"Categorical indices: {cat_features_indices}\")\n\n# Tạo Pool\ntrain_pool = cb.Pool(X_train, y_train, cat_features=cat_features_indices)\nval_pool = cb.Pool(X_val, y_val, cat_features=cat_features_indices)\n\n# Xóa biến thừa\ndel X_small, y_small, X_train, X_val, y_train, y_val\ngc.collect()\n\n# 3. HÀM MỤC TIÊU\ndef objective(trial):\n    params = {\n        'iterations': 500, # Chạy ít để test nhanh\n        'learning_rate': trial.suggest_float('learning_rate', 0.05, 0.3),\n        'depth': trial.suggest_int('depth', 4, 10),\n        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1, 10),\n        'random_strength': trial.suggest_float('random_strength', 0.1, 10),\n        'bagging_temperature': trial.suggest_float('bagging_temperature', 0.0, 1.0),\n        \n        # Cấu hình GPU\n        'task_type': 'GPU',\n        'devices': '0',\n        'border_count': 128, # Tối ưu tốc độ GPU\n        \n        'loss_function': 'Logloss',\n        'eval_metric': 'AUC',\n        'verbose': 0,\n        'early_stopping_rounds': 30\n    }\n    \n    model = cb.CatBoostClassifier(**params)\n    model.fit(train_pool, eval_set=val_pool)\n    \n    return model.get_best_score()['validation']['AUC']\n\n# 4. CHẠY TUNING\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=20)\n\nprint(\"\\n--- KẾT QUẢ TỐT NHẤT ---\")\nprint(f\"Best AUC: {study.best_value:.4f}\")\nprint(\"Best Params:\", study.best_params)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T01:15:34.040956Z","iopub.status.idle":"2026-01-07T01:15:34.041232Z","shell.execute_reply.started":"2026-01-07T01:15:34.041106Z","shell.execute_reply":"2026-01-07T01:15:34.041118Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import catboost as cb\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport gc\nfrom sklearn.model_selection import train_test_split\n\nprint(\"--- TRAIN FINAL CATBOOST (BEST PARAMS) ---\")\n\n# 1. CẤU HÌNH THAM SỐ (Lấy từ Trial 13 của bạn)\nfinal_params = {\n    'learning_rate': 0.2181,\n    'depth': 8,\n    'l2_leaf_reg': 7.84,\n    'random_strength': 0.612,\n    'bagging_temperature': 0.359,\n    \n    # Cấu hình chung\n    'iterations': 3000,          # Tăng lên 3000 để học sâu hơn\n    'loss_function': 'Logloss',\n    'eval_metric': 'AUC',\n    'task_type': 'GPU',          # Dùng GPU\n    'devices': '0',\n    'verbose': 100,\n    'early_stopping_rounds': 100,\n    'border_count': 128,\n    'train_dir': 'catboost_info'\n}\n\n# 2. XỬ LÝ DỮ LIỆU (QUAN TRỌNG: FIX TEST SET)\n# Danh sách cột category\ncat_cols_names = [\n    'msno', 'song_id', 'source_system_tab', 'source_screen_name', \n    'source_type', 'city', 'gender', 'registered_via', 'language', \n    'artist_name', 'first_genre_id', 'before_song_id', 'source'\n]\nvalid_cat_cols = [c for c in cat_cols_names if c in train_final.columns]\n\n# Lấy index cột category\ncat_features_indices = [train_final.columns.get_loc(c) for c in valid_cat_cols]\n\n# --- Fix lỗi kiểu dữ liệu cho TEST SET (Train đã fix ở bước trước rồi) ---\nprint(\"Fixing categorical types for Test set...\")\nif 'test_final' not in locals():\n    # Load lại nếu chưa có\n    test_final = pd.read_parquet('/kaggle/working/temporal_data/test_final.parquet')\n    test_final = test_final[model.feature_names] # Đảm bảo đúng cột\n\nfor col in valid_cat_cols:\n    # Fill NaN và ép kiểu String giống hệt Train\n    test_final[col] = test_final[col].fillna(-1)\n    try:\n        test_final[col] = test_final[col].astype(int).astype(str)\n    except:\n        test_final[col] = test_final[col].astype(str)\n\n# 3. CHUẨN BỊ FULL DATASET\nprint(\"Splitting Train/Val...\")\nX_train_full, X_val_full, y_train_full, y_val_full = train_test_split(\n    train_final, target, test_size=0.2, random_state=42\n)\n\ntrain_pool = cb.Pool(X_train_full, y_train_full, cat_features=cat_features_indices)\nval_pool = cb.Pool(X_val_full, y_val_full, cat_features=cat_features_indices)\n\n# Xóa biến tạm để nhẹ RAM\ndel X_train_full, X_val_full, y_train_full, y_val_full\ngc.collect()\n\n# 4. TRAIN MODEL\nprint(\"Training CatBoost...\")\nmodel_cb = cb.CatBoostClassifier(**final_params)\nmodel_cb.fit(train_pool, eval_set=val_pool)\n\n# 5. VẼ BIỂU ĐỒ\nprint(\"\\n--- PLOTTING ---\")\nhistory = model_cb.get_evals_result()\nepochs = range(len(history['learn']['AUC']))\n\nfig, ax = plt.subplots(1, 2, figsize=(15, 5))\n\n# Biểu đồ AUC\nax[0].plot(epochs, history['learn']['AUC'], label='Train AUC')\nax[0].plot(epochs, history['validation']['AUC'], label='Val AUC')\nax[0].set_title('CatBoost AUC')\nax[0].legend(); ax[0].grid(True)\n\n# Biểu đồ Loss\nax[1].plot(epochs, history['learn']['Logloss'], label='Train Loss')\nax[1].plot(epochs, history['validation']['Logloss'], label='Val Loss')\nax[1].set_title('CatBoost LogLoss')\nax[1].legend(); ax[1].grid(True)\n\nplt.show()\n\n# 6. PREDICT & SUBMIT\nprint(\"Predicting...\")\ntest_pool = cb.Pool(test_final, cat_features=cat_features_indices)\npreds = model_cb.predict_proba(test_pool)[:, 1]\n\nsub = pd.DataFrame()\n# Load ID từ file gốc nếu cần\nif 'test_df' in locals():\n    sub['id'] = test_df['id']\nelse:\n    ids = pd.read_csv('/kaggle/input/kkbox-music-recommendation-challenge/test.csv', usecols=['id'])['id']\n    sub['id'] = ids\n\nsub['target'] = preds\nfilename = 'submission_catboost_best_0824.csv'\nsub.to_csv(filename, index=False)\nprint(f\"Saved: {filename}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T01:15:34.042646Z","iopub.status.idle":"2026-01-07T01:15:34.042962Z","shell.execute_reply.started":"2026-01-07T01:15:34.042809Z","shell.execute_reply":"2026-01-07T01:15:34.042821Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport os\n\nprint(\"--- ĐỌC DỮ LIỆU TỪ CATBOOST_INFO ---\")\n\n# Đường dẫn folder (thường nằm ngay thư mục hiện tại)\ninfo_dir = '/kaggle/working/catboost_info'\n\n# Kiểm tra xem có file log không\nif os.path.exists(info_dir):\n    try:\n        # CatBoost lưu log dưới dạng file TSV (Tab Separated Values)\n        # Tên file có thể thay đổi tùy phiên bản, thường là 'learn_error.tsv' và 'test_error.tsv'\n        # Hoặc đôi khi nó gộp chung. Hãy kiểm tra file nào có đuôi .tsv hoặc .txt\n        \n        # Load lịch sử Train\n        train_log = pd.read_csv(f'{info_dir}/learn_error.tsv', sep='\\t')\n        # Load lịch sử Val\n        val_log = pd.read_csv(f'{info_dir}/test_error.tsv', sep='\\t')\n        \n        print(\"Đã tìm thấy log! Đang vẽ biểu đồ...\")\n        \n        # Vẽ hình\n        fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n\n        # 1. Biểu đồ LogLoss\n        # Cột đầu tiên là iter, cột thứ 2 là Logloss (thường là vậy)\n        ax[0].plot(train_log.iloc[:,0], train_log.iloc[:,1], label='Train Loss')\n        ax[0].plot(val_log.iloc[:,0], val_log.iloc[:,1], label='Val Loss')\n        ax[0].set_title('Restored LogLoss Curve')\n        ax[0].set_xlabel('Iterations')\n        ax[0].legend()\n        ax[0].grid(True)\n\n        # 2. Biểu đồ AUC\n        # Cột thứ 3 thường là AUC (nếu bạn set eval_metric='AUC')\n        if train_log.shape[1] > 2:\n            ax[1].plot(train_log.iloc[:,0], train_log.iloc[:,2], label='Train AUC')\n            ax[1].plot(val_log.iloc[:,0], val_log.iloc[:,2], label='Val AUC')\n            ax[1].set_title(f\"Restored AUC Curve (Max Val: {val_log.iloc[:,2].max():.4f})\")\n            ax[1].set_xlabel('Iterations')\n            ax[1].legend()\n            ax[1].grid(True)\n        \n        plt.show()\n        \n    except Exception as e:\n        print(f\"Có lỗi khi đọc file log: {e}\")\n        print(\"Danh sách file trong folder:\", os.listdir(info_dir))\nelse:\n    print(\"Không tìm thấy thư mục catboost_info. Bạn có chắc là nó còn đó không?\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T01:15:34.045703Z","iopub.status.idle":"2026-01-07T01:15:34.046051Z","shell.execute_reply.started":"2026-01-07T01:15:34.045889Z","shell.execute_reply":"2026-01-07T01:15:34.045909Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# FinalPart\n","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport time\nimport gc\nfrom sklearn.preprocessing import LabelEncoder\nfrom scipy import sparse\nfrom scipy.sparse.linalg import svds\nfrom collections import defaultdict\n\n# --- CẤU HÌNH ĐƯỜNG DẪN ---\n# Hãy thay đổi đường dẫn này trỏ đúng vào nơi chứa dataset của bạn trên Kaggle\nINPUT_DIR = '/kaggle/working' \n# Hoặc nếu tên dataset khác, ví dụ: '/kaggle/input/kkbox-music-recommendation-challenge'\n\n# Tạo thư mục làm việc (mô phỏng cấu trúc folder cũ của code)\nWORK_DIR = '/kaggle/working'\nTEMP_DIR = os.path.join(WORK_DIR, 'temporal_data')\n\nif not os.path.exists(TEMP_DIR):\n    os.makedirs(TEMP_DIR)\n\nprint(\"Đã cấu hình xong đường dẫn.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T01:15:34.048047Z","iopub.status.idle":"2026-01-07T01:15:34.048403Z","shell.execute_reply.started":"2026-01-07T01:15:34.048229Z","shell.execute_reply":"2026-01-07T01:15:34.048247Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- BLOCK XỬ LÝ DỮ LIỆU TOÀN DIỆN ---\nprint(\"Bắt đầu quy trình xử lý dữ liệu...\")\n\n# ==================================================\n# PHẦN 1: ID PROCESSING (id_process.py)\n# Mục tiêu: Encode ID sang số, xử lý ngày tháng, tách Genre\n# ==================================================\nprint(\"\\n--- [1/2] Đang chạy: id_process (Làm sạch & Encode ID) ---\")\n\n# 1. Load Data Gốc\n# Dùng f-string với biến INPUT_DIR bạn đã định nghĩa\nmembers = pd.read_csv(f'{INPUT_DIR}/members.csv')\nsongs = pd.read_csv(f'{INPUT_DIR}/songs.csv')\nsongs_extra = pd.read_csv(f'{INPUT_DIR}/song_extra_info.csv')\ntrain = pd.read_csv(f'{INPUT_DIR}/train.csv')\ntest = pd.read_csv(f'{INPUT_DIR}/test.csv')\n\n# 2. Lọc dữ liệu (Chỉ giữ lại Songs/Members có trong Train/Test)\nprint(\"   -> Đang lọc dữ liệu thừa...\")\nsong_id_set = set(pd.concat([train['song_id'], test['song_id']]))\nsongs = songs[songs['song_id'].isin(song_id_set)].copy()\nsongs_extra = songs_extra[songs_extra['song_id'].isin(song_id_set)].copy()\n\nmsno_set = set(pd.concat([train['msno'], test['msno']]))\nmembers = members[members['msno'].isin(msno_set)].copy()\n\n# 3. Label Encoding (User & Item IDs)\nprint(\"   -> Đang mã hóa ID User & Song...\")\n# Encode User (msno)\nmsno_encoder = LabelEncoder()\nall_msno = pd.concat([members['msno'], train['msno'], test['msno']]).unique()\nmsno_encoder.fit(all_msno.astype(str))\n\nmembers['msno'] = msno_encoder.transform(members['msno'].astype(str))\ntrain['msno'] = msno_encoder.transform(train['msno'].astype(str))\ntest['msno'] = msno_encoder.transform(test['msno'].astype(str))\n\n# Encode Song (song_id)\nsong_encoder = LabelEncoder()\nall_songs = pd.concat([songs['song_id'], songs_extra['song_id'], train['song_id'], test['song_id']]).unique()\nsong_encoder.fit(all_songs.astype(str))\n\nsongs['song_id'] = song_encoder.transform(songs['song_id'].astype(str))\nsongs_extra['song_id'] = song_encoder.transform(songs_extra['song_id'].astype(str))\ntrain['song_id'] = song_encoder.transform(train['song_id'].astype(str))\ntest['song_id'] = song_encoder.transform(test['song_id'].astype(str))\n\n# 4. Context Features (Source Tab/Type...)\nprint(\"   -> Đang mã hóa Context Features...\")\ncols = ['source_system_tab', 'source_screen_name', 'source_type']\nfor col in cols:\n    le = LabelEncoder()\n    full_data = pd.concat([train[col], test[col]]).astype(str)\n    le.fit(full_data)\n    train[col] = le.transform(train[col].astype(str))\n    test[col] = le.transform(test[col].astype(str))\n\n# 5. Process Members Info\nprint(\"   -> Đang xử lý Members Info...\")\nfor col in ['city', 'gender', 'registered_via']:\n    members[col] = members[col].fillna('Unknown').astype(str)\n    members[col] = LabelEncoder().fit_transform(members[col])\n\n# Hàm xử lý ngày tháng an toàn\ndef safe_date_convert(x):\n    try:\n        return time.mktime(time.strptime(str(int(float(x))), '%Y%m%d'))\n    except:\n        return np.nan\n\nmembers['registration_init_time'] = members['registration_init_time'].apply(safe_date_convert)\nmembers['expiration_date'] = members['expiration_date'].apply(safe_date_convert)\n\n# 6. Process Songs Info (Genres splitting)\nprint(\"   -> Đang xử lý Songs Info (Genre splitting)...\")\n# Logic tách genre của tác giả cũ\ngenre_id = np.zeros((len(songs), 4))\nsong_genre_ids = songs['genre_ids'].fillna('0').astype(str).values\n\nfor i in range(len(songs)):\n    ids = song_genre_ids[i].split('|')\n    l = len(ids)\n    if l > 0: genre_id[i, 0] = int(ids[0])\n    if l > 1: genre_id[i, 1] = int(ids[1])\n    if l > 2: genre_id[i, 2] = int(ids[2])\n    genre_id[i, 3] = l\n\nsongs['first_genre_id'] = genre_id[:, 0]\nsongs['second_genre_id'] = genre_id[:, 1]\nsongs['third_genre_id'] = genre_id[:, 2]\nsongs['genre_id_cnt'] = genre_id[:, 3]\n\n# Encode Genres\nall_genres = pd.concat([songs['first_genre_id'], songs['second_genre_id'], songs['third_genre_id']]).unique()\ngenre_le = LabelEncoder().fit(all_genres.astype(int))\nsongs['first_genre_id'] = genre_le.transform(songs['first_genre_id'].astype(int))\nsongs['second_genre_id'] = genre_le.transform(songs['second_genre_id'].astype(int))\nsongs['third_genre_id'] = genre_le.transform(songs['third_genre_id'].astype(int))\n\n# Encode Text Columns\nfor col in ['artist_name', 'lyricist', 'composer', 'language']:\n    songs[col] = songs[col].fillna('Unknown').astype(str)\n    songs[col] = LabelEncoder().fit_transform(songs[col])\n\n# --- LƯU FILE ID ĐÃ XỬ LÝ VÀO TEMP_DIR ---\nprint(\"   -> Đang lưu các file ID tạm thời...\")\nmembers.to_csv(os.path.join(TEMP_DIR, 'members_id.csv'), index=False)\nsongs.to_csv(os.path.join(TEMP_DIR, 'songs_id.csv'), index=False)\nsongs_extra.to_csv(os.path.join(TEMP_DIR, 'songs_extra_id.csv'), index=False)\ntrain.to_csv(os.path.join(TEMP_DIR, 'train_id.csv'), index=False)\ntest.to_csv(os.path.join(TEMP_DIR, 'test_id.csv'), index=False)\n\n# Giải phóng bộ nhớ\ndel members, songs, songs_extra, train, test\ngc.collect()\n\n\n# ==================================================\n# PHẦN 2: COUNT LOGIC (cnt_log_process.py)\n# Mục tiêu: Tạo các feature đếm (User nghe bao nhiêu bài, Bài hát hot thế nào...)\n# ==================================================\nprint(\"\\n--- [2/2] Đang chạy: cnt_log_process (Tạo Count Features) ---\")\n\n# 1. Load lại data vừa lưu\nprint(\"   -> Loading processed ID files...\")\ntrain = pd.read_csv(os.path.join(TEMP_DIR, 'train_id.csv'))\ntest = pd.read_csv(os.path.join(TEMP_DIR, 'test_id.csv'))\nmember = pd.read_csv(os.path.join(TEMP_DIR, 'members_id.csv'))\nsong = pd.read_csv(os.path.join(TEMP_DIR, 'songs_id.csv'))\n\n# 2. Tạo Feature Counts\nprint(\"   -> Generating Count Features...\")\ndata = pd.concat([train[['msno', 'song_id']], test[['msno', 'song_id']]])\n\n# Member Play Count\nmem_rec_cnt = data['msno'].value_counts().to_dict()\nmember['msno_rec_cnt'] = member['msno'].map(mem_rec_cnt).fillna(0)\nmember['bd'] = member['bd'].apply(lambda x: np.nan if x <= 0 or x >= 75 else x)\n\n# Song Popularity Counts\ndata = data.merge(song[['song_id', 'artist_name', 'composer', 'lyricist', 'first_genre_id']], on='song_id', how='left')\n\n# Đếm số bài hát của mỗi nghệ sĩ/tác giả (Artist có bao nhiêu bài)\nfor col in ['artist_name', 'composer', 'lyricist', 'first_genre_id']:\n    cnt_map = song[col].value_counts().to_dict()\n    song[f'{col.replace(\"_name\",\"\")}_song_cnt'] = song[col].map(cnt_map).fillna(0)\n\n# Đếm số lượt nghe (Rec Count) của mỗi bài hát/nghệ sĩ (Bài này được nghe bao nhiêu lần)\nfor col in ['song_id', 'artist_name', 'composer', 'lyricist', 'first_genre_id']:\n    rec_cnt_map = data[col].value_counts().to_dict()\n    song_col_name = col if col != 'first_genre_id' else 'genre'\n    song_col_name = song_col_name.replace('_name', '')\n    song[f'{song_col_name}_rec_cnt'] = song[col].map(rec_cnt_map).fillna(0)\n\n# 3. Context Features (Dummies mean encoding)\nprint(\"   -> Generating Context Features...\")\nconcat_df = pd.concat([train.drop('target', axis=1), test.drop('id', axis=1)])\ndummy_feat = ['source_system_tab', 'source_screen_name', 'source_type']\n\nfor feat in dummy_feat:\n    # Get dummies và tính mean cho từng user\n    dummies = pd.get_dummies(concat_df[feat].astype(str), prefix=f'msno_{feat}')\n    dummies['msno'] = concat_df['msno'].values\n    # Group by MSNO và tính trung bình (tỷ lệ user dùng tab/screen đó)\n    user_context_profile = dummies.groupby('msno').mean().reset_index()\n    member = member.merge(user_context_profile, on='msno', how='left')\n\n# 4. Log Transform & Save\nprint(\"   -> Applying Log Transform & Saving Final Files...\")\n# Log transform cho member features\nfor feat in ['msno_rec_cnt']:\n    member[feat] = np.log1p(member[feat])\n\n# Log transform cho song features\nsong_log_cols = [c for c in song.columns if '_cnt' in c or c == 'song_length']\nfor feat in song_log_cols:\n    if feat in song.columns:\n        song[feat] = np.log1p(song[feat])\n\n# Save final files\nmember.to_csv(os.path.join(TEMP_DIR, 'members_id_cnt.csv'), index=False)\nsong.to_csv(os.path.join(TEMP_DIR, 'songs_id_cnt.csv'), index=False)\ntrain.to_csv(os.path.join(TEMP_DIR, 'train_id_cnt.csv'), index=False)\ntest.to_csv(os.path.join(TEMP_DIR, 'test_id_cnt.csv'), index=False)\n\nprint(\">>> HOÀN TẤT XỬ LÝ DỮ LIỆU CƠ BẢN.\")\nprint(f\"File đã được lưu tại: {TEMP_DIR}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T01:15:34.049466Z","iopub.status.idle":"2026-01-07T01:15:34.049714Z","shell.execute_reply.started":"2026-01-07T01:15:34.049604Z","shell.execute_reply":"2026-01-07T01:15:34.049615Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport time\nimport gc\nfrom sklearn.preprocessing import LabelEncoder\n\n# --- CẤU HÌNH ---\nINPUT_DIR = '/kaggle/working' # Sửa lại đúng đường dẫn dataset của bạn\nWORK_DIR = '/kaggle/working'\nTEMP_DIR = os.path.join(WORK_DIR, 'temporal_data')\n\nif not os.path.exists(TEMP_DIR):\n    os.makedirs(TEMP_DIR)\n\n# ==============================================================================\n# PHẦN 1: ID PROCESS (Xử lý ID, encode và lưu file tạm)\n# ==============================================================================\nprint(\"--- Đang chạy: id_process.py ---\")\n\n# 1. Load Data\nprint(\"Loading raw data...\")\nmembers = pd.read_csv(f'{INPUT_DIR}/members.csv')\nsongs = pd.read_csv(f'{INPUT_DIR}/songs.csv')\nsongs_extra = pd.read_csv(f'{INPUT_DIR}/song_extra_info.csv')\ntrain = pd.read_csv(f'{INPUT_DIR}/train.csv')\ntest = pd.read_csv(f'{INPUT_DIR}/test.csv')\n\n# 2. Filter (Giữ lại bài hát/user xuất hiện trong train/test)\nsong_id_set = set(pd.concat([train['song_id'], test['song_id']]))\nsongs = songs[songs['song_id'].isin(song_id_set)].copy()\nsongs_extra = songs_extra[songs_extra['song_id'].isin(song_id_set)].copy()\n\nmsno_set = set(pd.concat([train['msno'], test['msno']]))\nmembers = members[members['msno'].isin(msno_set)].copy()\n\nprint('Data loaded and filtered.')\n\n# 3. Preprocess MSNO (User ID)\nprint('Encoding MSNO...')\nmsno_encoder = LabelEncoder()\nall_msno = pd.concat([members['msno'], train['msno'], test['msno']]).unique()\nmsno_encoder.fit(all_msno.astype(str))\n\nmembers['msno'] = msno_encoder.transform(members['msno'].astype(str))\ntrain['msno'] = msno_encoder.transform(train['msno'].astype(str))\ntest['msno'] = msno_encoder.transform(test['msno'].astype(str))\n\n# 4. Preprocess Song ID\nprint('Encoding Song ID...')\nsong_id_encoder = LabelEncoder()\nall_songs = pd.concat([songs['song_id'], songs_extra['song_id'], train['song_id'], test['song_id']]).unique()\nsong_id_encoder.fit(all_songs.astype(str))\n\nsongs['song_id'] = song_id_encoder.transform(songs['song_id'].astype(str))\nsongs_extra['song_id'] = song_id_encoder.transform(songs_extra['song_id'].astype(str))\ntrain['song_id'] = song_id_encoder.transform(train['song_id'].astype(str))\ntest['song_id'] = song_id_encoder.transform(test['song_id'].astype(str))\n\n# 5. Preprocess Train/Test Features (Source info)\nprint('Processing Source Info...')\ncolumns = ['source_system_tab', 'source_screen_name', 'source_type']\nfor column in columns:\n    column_encoder = LabelEncoder()\n    combined = pd.concat([train[column], test[column]]).astype(str)\n    column_encoder.fit(combined)\n    train[column] = column_encoder.transform(train[column].astype(str))\n    test[column] = column_encoder.transform(test[column].astype(str))\n\n# 6. Preprocess Members (City, Gender, Date)\nprint('Processing Members Info...')\ncolumns = ['city', 'gender', 'registered_via']\nfor column in columns:\n    column_encoder = LabelEncoder()\n    members[column] = members[column].fillna('Unknown').astype(str)\n    column_encoder.fit(members[column])\n    members[column] = column_encoder.transform(members[column])\n\n# Xử lý ngày tháng (Fix lỗi mktime với NaN)\ndef safe_date_convert(x):\n    try:\n        return time.mktime(time.strptime(str(int(float(x))), '%Y%m%d'))\n    except:\n        return np.nan\n\nmembers['registration_init_time'] = members['registration_init_time'].apply(safe_date_convert)\nmembers['expiration_date'] = members['expiration_date'].apply(safe_date_convert)\n\n# 7. Preprocess Songs (Genre, Artist, Composer...)\nprint('Processing Songs Info...')\n# Logic tách Genre cũ\ngenre_id = np.zeros((len(songs), 4))\nsong_genre_ids = songs['genre_ids'].fillna('0').astype(str).values\n\nfor i in range(len(songs)):\n    ids = song_genre_ids[i].split('|')\n    l = len(ids)\n    if l > 0: genre_id[i, 0] = int(ids[0])\n    if l > 1: genre_id[i, 1] = int(ids[1])\n    if l > 2: genre_id[i, 2] = int(ids[2])\n    genre_id[i, 3] = l\n\nsongs['first_genre_id'] = genre_id[:, 0]\nsongs['second_genre_id'] = genre_id[:, 1]\nsongs['third_genre_id'] = genre_id[:, 2]\nsongs['genre_id_cnt'] = genre_id[:, 3]\n\n# Encode Genres\ngenre_encoder = LabelEncoder()\nall_genres = pd.concat([songs['first_genre_id'], songs['second_genre_id'], songs['third_genre_id']]).unique()\ngenre_encoder.fit(all_genres.astype(int))\nsongs['first_genre_id'] = genre_encoder.transform(songs['first_genre_id'].astype(int))\nsongs['second_genre_id'] = genre_encoder.transform(songs['second_genre_id'].astype(int))\nsongs['third_genre_id'] = genre_encoder.transform(songs['third_genre_id'].astype(int))\nsongs.drop('genre_ids', axis=1, inplace=True)\n\n# Helper functions for text counting\ndef artist_count(x):\n    x = str(x)\n    return x.count('and') + x.count(',') + x.count(' feat') + x.count('&') + 1\n\ndef get_count(x):\n    try:\n        x = str(x)\n        return sum(map(x.count, ['|', '/', '\\\\', ';'])) + 1\n    except:\n        return 0\n\ndef get_first_term(x):\n    try:\n        x = str(x)\n        if x.count('|') > 0: x = x.split('|')[0]\n        if x.count('/') > 0: x = x.split('/')[0]\n        if x.count('\\\\') > 0: x = x.split('\\\\')[0]\n        if x.count(';') > 0: x = x.split(';')[0]\n        return x.strip()\n    except:\n        return x\n\nsongs['artist_cnt'] = songs['artist_name'].apply(artist_count).astype(np.int8)\nsongs['lyricist_cnt'] = songs['lyricist'].apply(get_count).astype(np.int8)\nsongs['composer_cnt'] = songs['composer'].apply(get_count).astype(np.int8)\nsongs['is_featured'] = songs['artist_name'].apply(lambda x: 1 if ' feat' in str(x) else 0).astype(np.int8)\n\n# Encode Text Columns\nsongs['artist_name'] = songs['artist_name'].astype(str).apply(get_first_term)\nsongs['lyricist'] = songs['lyricist'].astype(str).apply(get_first_term)\nsongs['composer'] = songs['composer'].astype(str).apply(get_first_term)\nsongs['language'] = songs['language'].fillna(-1).astype(str)\n\nfor col in ['artist_name', 'lyricist', 'composer', 'language']:\n    le = LabelEncoder()\n    songs[col] = le.fit_transform(songs[col])\n\n# --- LƯU FILE ID (Bắt buộc phải lưu trước khi sang bước 2) ---\nprint('Saving ID Processed files...')\nmembers.to_csv(f'{TEMP_DIR}/members_id.csv', index=False)\nsongs.to_csv(f'{TEMP_DIR}/songs_id.csv', index=False)\nsongs_extra.to_csv(f'{TEMP_DIR}/songs_extra_id.csv', index=False)\ntrain.to_csv(f'{TEMP_DIR}/train_id.csv', index=False)\ntest.to_csv(f'{TEMP_DIR}/test_id.csv', index=False)\n\n# Clean up\ndel members, songs, songs_extra, train, test\ngc.collect()\n\n\n# ==============================================================================\n# PHẦN 2: COUNT LOG PROCESS (Tính toán thống kê từ file ID đã lưu)\n# ==============================================================================\nprint(\"\\n--- Đang chạy: cnt_log_process.py ---\")\n\n# 1. Load Data (Lúc này file đã tồn tại)\ntrain = pd.read_csv(f'{TEMP_DIR}/train_id.csv')\ntest = pd.read_csv(f'{TEMP_DIR}/test_id.csv')\nmember = pd.read_csv(f'{TEMP_DIR}/members_id.csv')\nsong = pd.read_csv(f'{TEMP_DIR}/songs_id.csv')\n# song_extra = pd.read_csv(f'{TEMP_DIR}/songs_extra_id.csv') # File này chưa dùng trong logic dưới\n\nsong_origin = song.copy() # Giữ bản gốc nếu cần merge\n# Song DataFrame chuẩn hóa\nsong_base = pd.DataFrame({'song_id': range(max(train.song_id.max(), test.song_id.max())+1)})\nsong = song_base.merge(song, on='song_id', how='left')\n\ndata = pd.concat([train[['msno', 'song_id']], test[['msno', 'song_id']]])\n\n# 2. Member Count\nprint(\"Generating Member Counts...\")\nmem_rec_cnt = data['msno'].value_counts().to_dict()\nmember['msno_rec_cnt'] = member['msno'].map(mem_rec_cnt).fillna(0)\nmember['bd'] = member['bd'].apply(lambda x: np.nan if x <= 0 or x >= 75 else x)\n\n# 3. Song Counts (Artist, Composer, Genre...)\nprint(\"Generating Song Counts...\")\nfor col in ['artist_name', 'composer', 'lyricist', 'first_genre_id']:\n    cnt_map = song[col].value_counts().to_dict()\n    song[f'{col.replace(\"_name\",\"\")}_song_cnt'] = song[col].map(cnt_map).fillna(0)\n\n# 4. Rec Counts (Số lần được nghe)\n# Merge thông tin bài hát vào bảng tương tác (data) để đếm\ndata_merged = data.merge(song[['song_id', 'artist_name', 'composer', 'lyricist', 'first_genre_id']], on='song_id', how='left')\n\nfor col in ['song_id', 'artist_name', 'composer', 'lyricist', 'first_genre_id']:\n    rec_cnt_map = data_merged[col].value_counts().to_dict()\n    \n    # Đặt tên cột output\n    prefix = col.replace('_name', '')\n    if col == 'first_genre_id': prefix = 'genre'\n    \n    song[f'{prefix}_rec_cnt'] = song[col].map(rec_cnt_map).fillna(0)\n\n# 5. MSNO Context Features (Mean Encoding cho Context)\nprint(\"Generating Context Features...\")\ndummy_feat = ['source_system_tab', 'source_screen_name', 'source_type']\nconcat_df = pd.concat([train.drop('target', axis=1), test.drop('id', axis=1)])\n\nfor feat in dummy_feat:\n    # Get dummies\n    dummies = pd.get_dummies(concat_df[feat].astype(str), prefix=f'msno_{feat}')\n    dummies['msno'] = concat_df['msno'].values\n    \n    # Group mean\n    feat_profile = dummies.groupby('msno').mean().reset_index()\n    member = member.merge(feat_profile, on='msno', how='left')\n\n# Map prob back to train/test\n# Logic này hơi nặng, dùng map sẽ nhanh hơn apply từng dòng\ntrain_temp = train.merge(member, on='msno', how='left')\ntest_temp = test.merge(member, on='msno', how='left')\n\nfor feat in dummy_feat:\n    col_name = f'msno_{feat}_prob'\n    # Tính xác suất user dùng context này (lấy đúng cột dummies tương ứng với giá trị context hiện tại)\n    # Cách nhanh nhất là lookup ma trận, nhưng giữ logic cũ dùng apply (đã tối ưu 1 chút)\n    \n    # Tạo map dict cho nhanh: msno -> {tab_val: prob, tab_val2: prob...}\n    # Tuy nhiên để giữ code đơn giản theo ý bạn, ta dùng logic gán cột trực tiếp nếu có thể\n    pass \n    # Phần này trong code gốc bạn dùng apply lambda x: x['msno_source_..._%d']. \n    # Để code chạy được mà không crash RAM, ta tạm bỏ qua 6 dòng apply phức tạp đó nếu RAM yếu.\n    # Nếu RAM > 16GB, bạn có thể giữ nguyên logic apply cũ. \n    # Ở đây tôi đã tính sẵn `msno_source...` trong bảng `member`, việc merge đã đưa features đó vào rồi.\n\n# 6. Log Transform & Save\nprint(\"Log Transforming & Saving...\")\nfeatures = ['msno_rec_cnt']\nfor feat in features:\n    member[feat] = np.log1p(member[feat])\nmember.to_csv(f'{TEMP_DIR}/members_id_cnt.csv', index=False)\n\nsong_log_feats = ['song_length', 'song_rec_cnt', 'artist_song_cnt', 'composer_song_cnt', \n                  'lyricist_song_cnt', 'genre_song_cnt', 'artist_rec_cnt', \n                  'composer_rec_cnt', 'lyricist_rec_cnt', 'genre_rec_cnt']\n\nfor feat in song_log_feats:\n    if feat in song.columns:\n        song[feat] = song[feat].fillna(0).clip(lower=0)\n        song[feat] = np.log1p(song[feat])\n\nsong.to_csv(f'{TEMP_DIR}/songs_id_cnt.csv', index=False)\ntrain.to_csv(f'{TEMP_DIR}/train_id_cnt.csv', index=False)\ntest.to_csv(f'{TEMP_DIR}/test_id_cnt.csv', index=False)\n\nprint(\"Hoàn thành! File đã lưu tại:\", TEMP_DIR)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T01:15:34.051204Z","iopub.status.idle":"2026-01-07T01:15:34.051475Z","shell.execute_reply.started":"2026-01-07T01:15:34.051330Z","shell.execute_reply":"2026-01-07T01:15:34.051340Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport gc\nfrom scipy.sparse import coo_matrix\nfrom scipy.sparse.linalg import svds\nfrom sklearn.preprocessing import LabelEncoder\n\n# --- CẤU HÌNH ---\n# Đảm bảo biến này đúng với đường dẫn của bạn\nif 'TEMP_DIR' not in globals():\n    TEMP_DIR = '/kaggle/working/temporal_data' \nif 'INPUT_DIR' not in globals():\n    INPUT_DIR = '/kaggle/input/kkbox-music-recommendation-challenge'\n\nprint(\"==================================================\")\nprint(\"PHẦN 3: ISRC PROCESSING (FIXED)\")\nprint(\"==================================================\")\n\n# 1. Load data\nprint(\"Loading data for ISRC step...\")\ntrain = pd.read_csv(f'{TEMP_DIR}/train_id_cnt.csv')\ntest = pd.read_csv(f'{TEMP_DIR}/test_id_cnt.csv')\nsong = pd.read_csv(f'{TEMP_DIR}/songs_id_cnt.csv')\n\n# 2. Xử lý ISRC\n# Vì cột isrc hay bị lỗi, ta dùng cơ chế Try-Except an toàn\nprint(\"Processing ISRC features...\")\n\n# Tạo cột isrc mặc định (Dummy) để tránh crash nếu file thiếu\nif 'isrc' not in song.columns:\n    print(\"   -> Cột 'isrc' thiếu. Đang tạo Dummy ISRC...\")\n    song['isrc'] = np.nan\n\n# Fill NaN\nisrc = song['isrc'].fillna('000000000000').astype(str)\n\n# Cắt chuỗi\nsong['cc'] = isrc.str.slice(0, 2)\nsong['xxx'] = isrc.str.slice(2, 5)\n\n# Xử lý năm (YY)\ndef parse_year(x):\n    try:\n        val = int(x)\n        return 2000 + val if val < 18 else 1900 + val\n    except:\n        return 2017 # Default year\n\nsong['yy'] = isrc.str.slice(5, 7).apply(parse_year)\n\n# Encode\nprint(\"Encoding ISRC parts...\")\nsong['cc'] = LabelEncoder().fit_transform(song['cc'])\nsong['xxx'] = LabelEncoder().fit_transform(song['xxx'])\nsong['isrc_missing'] = (song['cc'] == 0).astype(int)\n\n# 3. Tạo Count Features cho ISRC\nprint(\"Generating ISRC Counts...\")\nfor col in ['cc', 'xxx', 'yy']:\n    cnt_map = song[col].value_counts().to_dict()\n    song[f'{col}_song_cnt'] = song[col].map(cnt_map).fillna(0)\n\n# Merge với data tương tác để đếm Rec Count\n# Lưu ý: Chỉ lấy các cột cần thiết để tiết kiệm RAM\ndata = pd.concat([train[['msno', 'song_id']], test[['msno', 'song_id']]])\ndata = data.merge(song[['song_id', 'cc', 'xxx', 'yy']], on='song_id', how='left')\n\nfor col in ['cc', 'xxx', 'yy']:\n    rec_map = data[col].value_counts().to_dict()\n    song[f'{col}_rec_cnt'] = song[col].map(rec_map).fillna(0)\n\n# Log transform & Clean up\nnew_feats = ['cc_song_cnt', 'xxx_song_cnt', 'yy_song_cnt', 'cc_rec_cnt', 'xxx_rec_cnt', 'yy_rec_cnt']\nfor feat in new_feats:\n    song[feat] = np.log1p(np.maximum(song[feat], 0))\n\nif 'isrc' in song.columns:\n    song.drop('isrc', axis=1, inplace=True)\n\n# Lưu kết quả\nsong.to_csv(f'{TEMP_DIR}/songs_id_cnt_isrc.csv', index=False)\nprint(\"Saved: songs_id_cnt_isrc.csv\")\n\n\nprint(\"\\n==================================================\")\nprint(\"PHẦN 4: SVD PROCESSING (FIXED TYPE ERROR)\")\nprint(\"==================================================\")\n\n# 1. Chuẩn bị dữ liệu\nprint(\"Preparing SVD Matrix...\")\nmembers = pd.read_csv(f'{TEMP_DIR}/members_id_cnt.csv')\nsong = pd.read_csv(f'{TEMP_DIR}/songs_id_cnt_isrc.csv')\n\n# Gộp toàn bộ tương tác\ndf_all = pd.concat([train, test], axis=0).reset_index(drop=True)\n# Merge Artist ID vào df_all\ndf_all = df_all.merge(song[['song_id', 'artist_name']], on='song_id', how='left')\n\n# 2. SVD User-Song\nprint(\"Running SVD on User-Song...\")\nn_components = 48 \n\n# [FIX]: Ép kiểu int cho kích thước ma trận\nn_users = int(members['msno'].max() + 1)\nn_items = int(song['song_id'].max() + 1)\n\nrow = df_all['msno'].fillna(0).astype(int).values\ncol = df_all['song_id'].fillna(0).astype(int).values\ndata_ones = np.ones(len(df_all))\n\n# Tạo ma trận thưa\nR_song = coo_matrix((data_ones, (row, col)), shape=(n_users, n_items))\n\n# SVD Calculation\nu_song, s_song, vt_song = svds(R_song.astype(float), k=n_components)\n\n# Save Latent Factors (Member)\nu_cols = [f'member_component_{i}' for i in range(n_components)]\nmembers_svd = pd.DataFrame(u_song, columns=u_cols)\nmembers_svd['msno'] = range(n_users)\nmembers = members.merge(members_svd, on='msno', how='left')\n\n# Save Latent Factors (Song)\nv_cols = [f'song_component_{i}' for i in range(n_components)]\nsongs_svd = pd.DataFrame(vt_song.T, columns=v_cols)\nsongs_svd['song_id'] = range(n_items)\nsong = song.merge(songs_svd, on='song_id', how='left')\n\n# 3. SVD User-Artist\nprint(\"Running SVD on User-Artist...\")\nn_components_art = 16\n\n# [FIX]: Ép kiểu int cho Artist ID và kích thước ma trận\ndf_all['artist_name'] = df_all['artist_name'].fillna(0).astype(int)\nn_artists = int(song['artist_name'].max() + 1)\n\nrow_art = df_all['msno'].fillna(0).astype(int).values\ncol_art = df_all['artist_name'].values\n\n# [FIX]: Đảm bảo shape là tuple of ints\nR_art = coo_matrix((data_ones, (row_art, col_art)), shape=(n_users, n_artists))\n\nu_art, s_art, vt_art = svds(R_art.astype(float), k=n_components_art)\n\n# Save Latent Factors Artist\nu_art_cols = [f'member_artist_component_{i}' for i in range(n_components_art)]\nmem_art_svd = pd.DataFrame(u_art, columns=u_art_cols)\nmem_art_svd['msno'] = range(n_users)\nmembers = members.merge(mem_art_svd, on='msno', how='left')\n\n# Merge Artist vectors vào Song\nart_cols = [f'artist_component_{i}' for i in range(n_components_art)]\nart_svd = pd.DataFrame(vt_art.T, columns=art_cols)\nart_svd['artist_name'] = range(n_artists)\nsong = song.merge(art_svd, on='artist_name', how='left')\n\n# 4. Dot Products\nprint(\"Calculating Dot Products...\")\n\ndef fast_dot(df, u_mat, s_val, v_mat, u_col, i_col):\n    u_idx = df[u_col].fillna(0).astype(int).values\n    i_idx = df[i_col].fillna(0).astype(int).values\n    \n    # Clip index an toàn\n    i_idx = np.clip(i_idx, 0, v_mat.shape[0]-1)\n    \n    return np.sum((u_mat[u_idx] * v_mat[i_idx]) * s_val, axis=1)\n\ndf_all['song_embeddings_dot'] = fast_dot(df_all, u_song, s_song, vt_song.T, 'msno', 'song_id')\ndf_all['artist_embeddings_dot'] = fast_dot(df_all, u_art, s_art, vt_art.T, 'msno', 'artist_name')\n\n# Gán ngược lại Train/Test\ntrain_len = len(train)\ntrain['song_embeddings_dot'] = df_all['song_embeddings_dot'].values[:train_len]\ntrain['artist_embeddings_dot'] = df_all['artist_embeddings_dot'].values[:train_len]\n\ntest['song_embeddings_dot'] = df_all['song_embeddings_dot'].values[train_len:]\ntest['artist_embeddings_dot'] = df_all['artist_embeddings_dot'].values[train_len:]\n\n# 5. Save Final Files\nprint(\"Saving final SVD processed files...\")\ntrain.to_csv(f'{TEMP_DIR}/train_id_cnt_svd.csv', index=False)\ntest.to_csv(f'{TEMP_DIR}/test_id_cnt_svd.csv', index=False)\nmembers.to_csv(f'{TEMP_DIR}/members_id_cnt_svd.csv', index=False)\nsong.to_csv(f'{TEMP_DIR}/songs_id_cnt_isrc_svd.csv', index=False)\n\nprint(\">>> HOÀN THÀNH TOÀN BỘ LOGIC XỬ LÝ DỮ LIỆU.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T01:15:34.053449Z","iopub.status.idle":"2026-01-07T01:15:34.053938Z","shell.execute_reply.started":"2026-01-07T01:15:34.053651Z","shell.execute_reply":"2026-01-07T01:15:34.053670Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"--- Đang chạy: timestamp_process.py ---\")\n\ntr = pd.read_csv(f'{TEMP_DIR}/train_id_cnt_svd.csv')\nte = pd.read_csv(f'{TEMP_DIR}/test_id_cnt_svd.csv')\nmem = pd.read_csv(f'{TEMP_DIR}/members_id_cnt_svd.csv')\nsong = pd.read_csv(f'{TEMP_DIR}/songs_id_cnt_isrc_svd.csv')\n\n## continous index\n# FIX: append -> concat\nconcat = pd.concat([tr[['msno', 'song_id']], te[['msno', 'song_id']]])\nconcat['timestamp'] = range(len(concat))\n\n## windows_based count\n# Giảm số lượng window hoặc kích thước nếu chạy quá lâu\nwindow_sizes = [10, 25, 500, 5000, 10000, 50000]\n\nmsno_list = concat['msno'].values\nsong_list = concat['song_id'].values\n\n# Logic này cực kỳ chậm (O(N*Windows)), chạy trên Kaggle có thể bị Timeout\n# Bạn có thể cân nhắc bỏ bớt window_sizes lớn nếu cần\ndef get_window_cnt(values, idx, window_size):\n    lower = max(0, idx-window_size)\n    upper = min(len(values), idx+window_size)\n    # Đoạn này dùng numpy slicing tương đối ổn\n    return (values[lower:idx] == values[idx]).sum(), (values[idx:upper] == values[idx]).sum()\n\nprint(\"Bắt đầu chạy window loop... (Có thể rất lâu)\")\nfor window_size in window_sizes:\n    msno_before_cnt = np.zeros(len(concat))\n    song_before_cnt = np.zeros(len(concat))\n    msno_after_cnt = np.zeros(len(concat))\n    song_after_cnt = np.zeros(len(concat))\n    \n    # Để tối ưu hơn, nên dùng pandas rolling window, nhưng để giữ logic code:\n    for i in range(len(concat)):\n        if i % 100000 == 0: print(f\"Window {window_size}: processing row {i}\")\n        msno_before_cnt[i], msno_after_cnt[i] = get_window_cnt(msno_list, i, window_size)\n        song_before_cnt[i], song_after_cnt[i] = get_window_cnt(song_list, i, window_size)\n        \n    concat['msno_%d_before_cnt'%window_size] = msno_before_cnt\n    concat['song_%d_before_cnt'%window_size] = song_before_cnt\n    concat['msno_%d_after_cnt'%window_size] = msno_after_cnt\n    concat['song_%d_after_cnt'%window_size] = song_after_cnt\n    \n    print('Window size for %d done.'%window_size)\n\n## till_now count\nmsno_dict = defaultdict(lambda: 0)\nsong_dict = defaultdict(lambda: 0)\n\nmsno_till_now_cnt = np.zeros(len(concat))\nsong_till_now_cnt = np.zeros(len(concat))\nfor i in range(len(concat)):\n    msno_till_now_cnt[i] = msno_dict[msno_list[i]]\n    msno_dict[msno_list[i]] += 1\n    \n    song_till_now_cnt[i] = song_dict[song_list[i]]\n    song_dict[song_list[i]] += 1\n\nconcat['msno_till_now_cnt'] = msno_till_now_cnt\nconcat['song_till_now_cnt'] = song_till_now_cnt\n\nprint('Till-now count done.')\n\n## varience\ndef timestamp_map(x):\n    if x < 7377418:\n        x = (x - 0.0) / (7377417.0 - 0.0) * (1484236800.0 - 1471190400.0) + 1471190400.0\n    else:\n        x = (x - 7377417.0) / (9934207.0 - 7377417.0) * (1488211200.0 - 1484236800.0) + 1484236800.0\n\n    return x\n    \nconcat['timestamp'] = concat['timestamp'].apply(timestamp_map)\n\nmsno_mean = concat.groupby(by='msno').mean()['timestamp'].to_dict()\nmem['msno_timestamp_mean'] = mem['msno'].map(msno_mean)\n\nmsno_std = concat.groupby(by='msno').std()['timestamp'].to_dict()\nmem['msno_timestamp_std'] = mem['msno'].map(msno_std)\n\nsong_mean = concat.groupby(by='song_id').mean()['timestamp'].to_dict()\nsong['song_timestamp_mean'] = song['song_id'].map(song_mean)\n\nsong_std = concat.groupby(by='song_id').std()['timestamp'].to_dict()\nsong['song_timestamp_std'] = song['song_id'].map(song_std)\n\nprint('Varience done.')\n\n## save to files\nfeatures = ['msno_till_now_cnt', 'song_till_now_cnt']\nfor window_size in window_sizes:\n    features += ['msno_%d_before_cnt'%window_size, 'song_%d_before_cnt'%window_size, \\\n            'msno_%d_after_cnt'%window_size, 'song_%d_after_cnt'%window_size]\nfor feat in features:\n    concat[feat] = np.log1p(concat[feat])\n\nfeatures = ['timestamp'] + features\n\ndata = concat[features].values\nfor i in range(len(features)):\n    tr[features[i]] = data[:len(tr), i]\n    te[features[i]] = data[len(tr):, i]\n\ntr.to_csv(f'{TEMP_DIR}/train_id_cnt_svd_stamp.csv', index=False)\nte.to_csv(f'{TEMP_DIR}/test_id_cnt_svd_stamp.csv', index=False)\nmem.to_csv(f'{TEMP_DIR}/members_id_cnt_svd_stamp.csv', index=False)\nsong.to_csv(f'{TEMP_DIR}/songs_id_cnt_isrc_svd_stamp.csv', index=False)\n\nprint(\"Hoàn thành timestamp_process.py\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T01:15:34.055125Z","iopub.status.idle":"2026-01-07T01:15:34.055541Z","shell.execute_reply.started":"2026-01-07T01:15:34.055322Z","shell.execute_reply":"2026-01-07T01:15:34.055341Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"--- Đang chạy: before_after_process.py (FIXED) ---\")\n\ntr = pd.read_csv(f'{TEMP_DIR}/train_id_cnt_svd_stamp.csv')\nte = pd.read_csv(f'{TEMP_DIR}/test_id_cnt_svd_stamp.csv')\n\nprint('data loaded.')\n\n## continous index\n# FIX: append -> concat\nconcat = pd.concat([tr[['msno', 'song_id', 'source_type', 'source_screen_name', 'timestamp']], \n                    te[['msno', 'song_id', 'source_type', 'source_screen_name', 'timestamp']]])\n\n## before data\nsong_dict = defaultdict(lambda: None)\ntype_dict = defaultdict(lambda: None)\nname_dict = defaultdict(lambda: None)\ntime_dict = defaultdict(lambda: None)\n\nbefore_data = np.zeros((len(concat), 4))\nconcat_values = concat[['song_id', 'source_type', 'source_screen_name', 'timestamp']].values\nconcat_msno = concat['msno'].values\n\n# Loop xuôi (Before)\nprint(\"Processing Before Data...\")\nfor i in range(len(concat)):\n    msno = concat_msno[i]\n    \n    if(song_dict[msno] == None):\n        before_data[i] = concat_values[i]\n        before_data[i, 3] = np.nan\n    else:\n        before_data[i, 0] = song_dict[msno]\n        before_data[i, 1] = type_dict[msno]\n        before_data[i, 2] = name_dict[msno]\n        before_data[i, 3] = time_dict[msno]\n\n    song_dict[msno] = concat_values[i, 0]\n    type_dict[msno] = concat_values[i, 1]\n    name_dict[msno] = concat_values[i, 2]\n    time_dict[msno] = concat_values[i, 3]\n\nprint('data before done.')\n\n## after data\nsong_dict = defaultdict(lambda: None)\ntype_dict = defaultdict(lambda: None)\nname_dict = defaultdict(lambda: None)\ntime_dict = defaultdict(lambda: None)\n\nafter_data = np.zeros((len(concat), 4))\n\n# Loop ngược (After)\nprint(\"Processing After Data...\")\nfor i in range(len(concat))[::-1]:\n    msno = concat_msno[i]\n    \n    if(song_dict[msno] == None):\n        after_data[i] = concat_values[i]\n        after_data[i, 3] = np.nan\n    else:\n        after_data[i, 0] = song_dict[msno]\n        after_data[i, 1] = type_dict[msno]\n        after_data[i, 2] = name_dict[msno]\n        after_data[i, 3] = time_dict[msno]\n\n    song_dict[msno] = concat_values[i, 0]\n    type_dict[msno] = concat_values[i, 1]\n    name_dict[msno] = concat_values[i, 2]\n    time_dict[msno] = concat_values[i, 3]\n\nprint('data after done.')\n\n## to_csv\nidx = 0\nfor i in ['song_id', 'source_type', 'source_screen_name', 'timestamp']:\n    tr['before_'+i] = before_data[:len(tr), idx]\n    tr['after_'+i] = after_data[:len(tr), idx]\n    \n    te['before_'+i] = before_data[len(tr):, idx]\n    te['after_'+i] = after_data[len(tr):, idx]\n    \n    idx += 1\n\nfor i in ['song_id', 'source_type', 'source_screen_name']:\n    tr['before_'+i] = tr['before_'+i].fillna(-1).astype(int)\n    te['before_'+i] = te['before_'+i].fillna(-1).astype(int)\n    tr['after_'+i] = tr['after_'+i].fillna(-1).astype(int)\n    te['after_'+i] = te['after_'+i].fillna(-1).astype(int)\n\n# === SỬA LỖI Ở ĐÂY ===\n\n# 1. Tính toán khoảng cách thời gian và xử lý số âm/NaN TRƯỚC khi log\n# fillna(0) tạm thời cho phép trừ để tránh lỗi, sau đó clip(lower=0) để đảm bảo không âm\ntr['before_timestamp'] = (tr['timestamp'] - tr['before_timestamp']).fillna(0).clip(lower=0)\nte['before_timestamp'] = (te['timestamp'] - te['before_timestamp']).fillna(0).clip(lower=0)\n\ntr['before_timestamp'] = np.log1p(tr['before_timestamp'])\nte['before_timestamp'] = np.log1p(te['before_timestamp'])\n\ntr['after_timestamp'] = (tr['after_timestamp'] - tr['timestamp']).fillna(0).clip(lower=0)\nte['after_timestamp'] = (te['after_timestamp'] - te['timestamp']).fillna(0).clip(lower=0)\n\ntr['after_timestamp'] = np.log1p(tr['after_timestamp'])\nte['after_timestamp'] = np.log1p(te['after_timestamp'])\n\n# 2. Thay thế fillna(inplace=True) bằng phép gán\n# Vì ở bước trên ta đã fillna(0) để tính toán rồi, nên thực tế bước này có thể dư thừa\n# nhưng giữ lại logic mean của tác giả cho chắc chắn (nếu có logic khác)\nmean_tr_before = np.nanmean(tr['before_timestamp'])\nmean_te_before = np.nanmean(te['before_timestamp'])\nmean_tr_after = np.nanmean(tr['after_timestamp'])\nmean_te_after = np.nanmean(te['after_timestamp'])\n\ntr['before_timestamp'] = tr['before_timestamp'].fillna(mean_tr_before)\nte['before_timestamp'] = te['before_timestamp'].fillna(mean_te_before)\ntr['after_timestamp'] = tr['after_timestamp'].fillna(mean_tr_after)\nte['after_timestamp'] = te['after_timestamp'].fillna(mean_te_after)\n\n# =====================\n\ntr.to_csv(f'{TEMP_DIR}/train_id_cnt_svd_stamp_before_after.csv', index=False)\nte.to_csv(f'{TEMP_DIR}/test_id_cnt_svd_stamp_before_after.csv', index=False)\n\nprint(\"Hoàn thành before_after_process.py (Đã fix sạch lỗi)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T01:15:34.057199Z","iopub.status.idle":"2026-01-07T01:15:34.057613Z","shell.execute_reply.started":"2026-01-07T01:15:34.057324Z","shell.execute_reply":"2026-01-07T01:15:34.057334Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"rm /kaggle/working/temporal_data/songs_id_cnt_isrc_svd.csv","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T01:15:34.058657Z","iopub.status.idle":"2026-01-07T01:15:34.059055Z","shell.execute_reply.started":"2026-01-07T01:15:34.058865Z","shell.execute_reply":"2026-01-07T01:15:34.058882Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"gc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T01:15:34.060945Z","iopub.status.idle":"2026-01-07T01:15:34.061278Z","shell.execute_reply.started":"2026-01-07T01:15:34.061135Z","shell.execute_reply":"2026-01-07T01:15:34.061152Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import sys\nimport gc\n\n# Lấy danh sách các biến trong môi trường hiện tại\n# Giữ lại các module hệ thống, xóa các biến dataframe to\nfor name in list(globals().keys()):\n    if not name.startswith('_') and name not in ['tf', 'np', 'pd', 'os', 'sys', 'gc', 'In', 'Out', 'get_ipython', 'exit', 'quit']:\n        del globals()[name]\n\n# Ép hệ thống dọn rác\ngc.collect()\nprint(\"Đã dọn dẹp RAM thủ công.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T01:15:34.063451Z","iopub.status.idle":"2026-01-07T01:15:34.063764Z","shell.execute_reply.started":"2026-01-07T01:15:34.063620Z","shell.execute_reply":"2026-01-07T01:15:34.063634Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport gc\nimport os\n\n# Cấu hình đường dẫn\nWORK_DIR = '/kaggle/working'\nTEMP_DIR = os.path.join(WORK_DIR, 'temporal_data')\n\nprint(\"--- Đang chạy: data_for_training.py (Chế độ tuần tự - Giữ nguyên chất lượng dữ liệu) ---\")\n\n# ==============================================================================\n# PHẦN 1: XỬ LÝ TRAIN\n# ==============================================================================\nprint(\"\\n1. Đang xử lý Train Data...\")\n# Chỉ load file train\ntrain = pd.read_csv(f'{TEMP_DIR}/train_id_cnt_svd_stamp_before_after.csv')\nprint(f\"   Train shape: {train.shape}\")\n\n# Lưu ngay ra file đích (Giữ nguyên float_format gốc)\ntrain.to_csv(f'{WORK_DIR}/train.csv', index=False, float_format='%.6f')\nprint(\"   -> Đã lưu train.csv\")\n\n# Xóa ngay khỏi RAM\ndel train\ngc.collect() # Dọn dẹp RAM ngay lập tức\n\n# ==============================================================================\n# PHẦN 2: XỬ LÝ TEST\n# ==============================================================================\nprint(\"\\n2. Đang xử lý Test Data...\")\ntest = pd.read_csv(f'{TEMP_DIR}/test_id_cnt_svd_stamp_before_after.csv')\nprint(f\"   Test shape: {test.shape}\")\n\ntest.to_csv(f'{WORK_DIR}/test.csv', index=False, float_format='%.6f')\nprint(\"   -> Đã lưu test.csv\")\n\ndel test\ngc.collect()\n\n# ==============================================================================\n# PHẦN 3: XỬ LÝ MEMBERS\n# ==============================================================================\nprint(\"\\n3. Đang xử lý Member Data...\")\nmember = pd.read_csv(f'{TEMP_DIR}/members_id_cnt_svd_stamp.csv')\n\n# --- Tạo Members cho GBDT ---\nprint(\"   Creating members_gbdt.csv...\")\nmember.to_csv(f'{WORK_DIR}/members_gbdt.csv', index=False)\n\n# --- Tạo Members cho NN (Neural Network) ---\nprint(\"   Creating members_nn.csv...\")\nmember['bd_missing'] = np.isnan(member['bd'].values) * 1\n\n# FillNA theo logic gốc\ncolumns = ['bd']\nfor col in columns:\n    member[col] = member[col].fillna(np.nanmean(member[col]))\n\n# Xử lý timestamp std\nmin_std = member['msno_timestamp_std'].min()\nif pd.isna(min_std): min_std = 0\nmember['msno_timestamp_std'] = member['msno_timestamp_std'].fillna(min_std)\n\nmember.to_csv(f'{WORK_DIR}/members_nn.csv', index=False)\nprint(\"   -> Đã lưu xong member files\")\n\ndel member\ngc.collect()\n\n# ==============================================================================\n# PHẦN 4: XỬ LÝ SONGS\n# ==============================================================================\nprint(\"\\n4. Đang xử lý Song Data...\")\nsong = pd.read_csv(f'{TEMP_DIR}/songs_id_cnt_isrc_svd_stamp.csv')\n\n# --- Tạo Songs cho GBDT ---\nprint(\"   Creating songs_gbdt.csv...\")\n# Backup dataframe gốc để dùng cho phần NN sau này (hoặc copy cột cần thiết)\n# Tuy nhiên để tiết kiệm RAM tối đa, ta xử lý trực tiếp, nhưng cần lưu ý logic biến đổi\n# Code gốc tạo songs_gbdt trước, biến đổi fillna(0) và astype(int).\n# Điều này sẽ làm thay đổi dữ liệu gốc. Ta cần cẩn thận.\n# Chiến thuật: Tạo bản sao cột cần thiết hoặc xử lý song song.\n# Do RAM 30GB khá lớn cho 1 file song, ta có thể copy song ra biến tạm.\n\nsong_gbdt = song.copy()\ncolumns = ['composer', 'lyricist', 'language', 'first_genre_id', 'second_genre_id', 'third_genre_id']\nfor col in columns:\n    song_gbdt[col] = song_gbdt[col].fillna(0).astype(int)\n\n# Xử lý artist_name\nmax_artist = song_gbdt['artist_name'].max()\nif pd.isna(max_artist): max_artist = 0\nsong_gbdt['artist_name'] = song_gbdt['artist_name'].fillna(max_artist+1).astype(int)\n\nsong_gbdt['isrc_missing'] = song_gbdt['isrc_missing'].astype(int)\nsong_gbdt.to_csv(f'{WORK_DIR}/songs_gbdt.csv', index=False)\n\ndel song_gbdt\ngc.collect()\n\n# --- Tạo Songs cho NN ---\nprint(\"   Creating songs_nn.csv...\")\n# Quay lại với biến 'song' (dữ liệu chưa bị fill 0 ở bước trên)\nsong['song_id_missing'] = np.isnan(song['song_length'].values) * 1\n\ncolumns = ['song_length', 'genre_id_cnt', 'artist_song_cnt', 'composer_song_cnt', \\\n       'lyricist_song_cnt', 'genre_song_cnt', 'song_rec_cnt', \\\n       'artist_rec_cnt', 'composer_rec_cnt', 'lyricist_rec_cnt', \\\n       'genre_rec_cnt', 'yy', 'cc_song_cnt', \\\n       'xxx_song_cnt', 'yy_song_cnt', 'cc_rec_cnt', 'xxx_rec_cnt', \\\n       'yy_rec_cnt', 'song_timestamp_std', 'artist_cnt', 'lyricist_cnt', \\\n       'composer_cnt', 'is_featured'] + ['artist_component_%d'%i for i in range(16)]\n\nfor col in columns:\n    if col in song.columns:\n        mean_val = np.nanmean(song[col])\n        if pd.isna(mean_val): mean_val = 0\n        song[col] = song[col].fillna(mean_val)\n\nsong.to_csv(f'{WORK_DIR}/songs_nn.csv', index=False)\nprint(\"   -> Đã lưu xong song files\")\n\ndel song\ngc.collect()\n\nprint(\"\\n--- HOÀN TẤT TOÀN BỘ QUÁ TRÌNH ---\")\nprint(\"RAM đã được giải phóng. Bạn có thể xóa folder temporal_data ngay bây giờ.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T01:15:34.065402Z","iopub.status.idle":"2026-01-07T01:15:34.065839Z","shell.execute_reply.started":"2026-01-07T01:15:34.065649Z","shell.execute_reply":"2026-01-07T01:15:34.065662Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport gc\nimport os\nimport time\nfrom sklearn.preprocessing import LabelEncoder\nfrom scipy.sparse import coo_matrix\nfrom scipy.sparse.linalg import svds\nimport pyarrow as pa\nimport pyarrow.parquet as pq\n\n# --- CẤU HÌNH ---\nWORK_DIR = '/kaggle/working'\nTEMP_DIR = '/kaggle/working/temporal_data'\n# Đảm bảo dùng file gốc trong working như hình bạn gửi\nINPUT_DIR = '/kaggle/working' \n\nprint(\"==================================================\")\nprint(\"CHIẾN DỊCH GIẢI CỨU: TÁI TẠO TEST_FINAL.PARQUET\")\nprint(\"==================================================\")\n\n# 1. LOAD RAW DATA (Chỉ load cột cần thiết để tiết kiệm RAM)\nprint(\"[1/5] Loading Raw Data...\")\ntrain = pd.read_csv(f'{INPUT_DIR}/train.csv', dtype={'target': np.int8})\ntest = pd.read_csv(f'{INPUT_DIR}/test.csv')\nmembers = pd.read_csv(f'{INPUT_DIR}/members.csv')\nsongs = pd.read_csv(f'{INPUT_DIR}/songs.csv')\n\n# Encode ID (Bắt buộc phải làm lại để khớp ID với file Train đã lưu)\nprint(\"[2/5] Encoding IDs...\")\n# User\nmsno_enc = LabelEncoder()\nall_msno = pd.concat([train['msno'], test['msno'], members['msno']]).unique().astype(str)\nmsno_enc.fit(all_msno)\ntrain['msno'] = msno_enc.transform(train['msno'].astype(str))\ntest['msno'] = msno_enc.transform(test['msno'].astype(str))\nmembers['msno'] = msno_enc.transform(members['msno'].astype(str))\n\n# Song\nsong_enc = LabelEncoder()\nall_songs = pd.concat([train['song_id'], test['song_id'], songs['song_id']]).unique().astype(str)\nsong_enc.fit(all_songs)\ntrain['song_id'] = song_enc.transform(train['song_id'].astype(str))\ntest['song_id'] = song_enc.transform(test['song_id'].astype(str))\nsongs['song_id'] = song_enc.transform(songs['song_id'].astype(str))\n\n# 2. TÁI TẠO SEQUENCE (BEFORE/AFTER)\n# Cần Train để biết lịch sử nghe của Test\nprint(\"[3/5] Re-creating Sequence Features...\")\n# Gộp tạm để tính\nconcat = pd.concat([\n    train[['msno', 'song_id', 'source_system_tab', 'source_screen_name', 'source_type']], \n    test[['msno', 'song_id', 'source_system_tab', 'source_screen_name', 'source_type']]\n], axis=0)\n\n# Sort (Giả định dữ liệu đã tương đối theo thứ tự, nếu có timestamp thì sort timestamp)\n# Logic: Lấy bài hát trước đó của mỗi user\nconcat['before_song_id'] = concat.groupby('msno')['song_id'].shift(1)\nUNKNOWN_ID = concat['song_id'].max() + 1\nconcat['before_song_id'] = concat['before_song_id'].fillna(UNKNOWN_ID).astype(int)\n\n# Tách lấy phần Test\ntest_seq = concat.iloc[len(train):].reset_index(drop=True)\ntest['before_song_id'] = test_seq['before_song_id'].values\n\n# Encode context của Test (để khớp với model)\nfor col in ['source_system_tab', 'source_screen_name', 'source_type']:\n    le = LabelEncoder()\n    full_col = pd.concat([train[col], test[col]]).astype(str)\n    le.fit(full_col)\n    test[col] = le.transform(test[col].astype(str))\n\ndel concat, test_seq\ngc.collect()\n\n# 3. TÁI TẠO SVD & COUNTS (Feature Members & Songs)\nprint(\"[4/5] Re-creating SVD & Counts...\")\n\n# --- A. Tạo lại Count ---\n# Song Count\nsong_cnt = pd.concat([train['song_id'], test['song_id']]).value_counts().to_dict()\nsongs['song_cnt'] = songs['song_id'].map(song_cnt).fillna(0)\nsongs['song_cnt'] = np.log1p(songs['song_cnt'])\n\n# Artist Count (Cần xử lý songs raw một chút)\nsongs['artist_name'] = songs['artist_name'].fillna('Unknown').astype(str)\nart_le = LabelEncoder()\nsongs['artist_name'] = art_le.fit_transform(songs['artist_name'])\nart_cnt = songs['artist_name'].value_counts().to_dict()\nsongs['artist_cnt'] = songs['artist_name'].map(art_cnt).fillna(0)\nsongs['artist_cnt'] = np.log1p(songs['artist_cnt'])\n\n# User Count\nuser_cnt = pd.concat([train['msno'], test['msno']]).value_counts().to_dict()\nmembers['user_cnt'] = members['msno'].map(user_cnt).fillna(0)\nmembers['user_cnt'] = np.log1p(members['user_cnt'])\n\n# --- B. Tạo lại SVD (User-Song) ---\n# SVD phải tính trên cả Train+Test để ra vector đúng\nprint(\"   -> Running SVD...\")\nn_users = members['msno'].max() + 1\nn_items = songs['song_id'].max() + 1\n# +2 cho unknown IDs\nn_items = max(n_items, UNKNOWN_ID + 2) \n\nrow = pd.concat([train['msno'], test['msno']]).values\ncol = pd.concat([train['song_id'], test['song_id']]).values\ndata = np.ones(len(row))\nR = coo_matrix((data, (row, col)), shape=(n_users, n_items))\n\n# Tính SVD (K=48 như cũ)\nu, s, vt = svds(R.astype(float), k=48)\n\n# Gán vector vào Members\nu_cols = [f'member_component_{i}' for i in range(48)]\nmem_svd = pd.DataFrame(u, columns=u_cols)\nmem_svd['msno'] = range(n_users)\nmembers = members.merge(mem_svd, on='msno', how='left')\n\n# Gán vector vào Songs\nv_cols = [f'song_component_{i}' for i in range(48)]\nsong_svd = pd.DataFrame(vt.T[:len(songs)], columns=v_cols) # Chỉ lấy phần khớp ID\nsong_svd['song_id'] = range(len(songs))\nsongs = songs.merge(song_svd, on='song_id', how='left')\n\n# Drop cột rác trong Songs/Members để nhẹ gánh\nmembers.drop(['city', 'gender', 'registered_via', 'registration_init_time', 'expiration_date'], axis=1, inplace=True, errors='ignore')\nsongs.drop(['genre_ids', 'composer', 'lyricist', 'language', 'isrc'], axis=1, inplace=True, errors='ignore')\n\n# 4. GHÉP VÀ LƯU FILE TEST_FINAL\nprint(\"[5/5] Merging & Saving Test...\")\n\n# Merge\ntest = test.merge(members, on='msno', how='left')\ntest = test.merge(songs, on='song_id', how='left')\n\n# Fill NaN\ntest.fillna(0, inplace=True)\n\n# Tối ưu bộ nhớ\nfor col in test.columns:\n    if test[col].dtype == 'float64': test[col] = test[col].astype('float32')\n    if test[col].dtype == 'int64': test[col] = test[col].astype('int32')\n\n# Lưu Parquet\nparquet_path = f'{TEMP_DIR}/test_final.parquet'\ntable = pa.Table.from_pandas(test)\npq.write_table(table, parquet_path, compression='snappy')\n\nprint(\"\\n>>> GIẢI CỨU THÀNH CÔNG!\")\nprint(f\"File Test đã được tạo lại tại: {parquet_path}\")\nprint(\"Bây giờ bạn đã có đủ cặp: train_final.parquet và test_final.parquet\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T01:15:34.066967Z","iopub.status.idle":"2026-01-07T01:15:34.067224Z","shell.execute_reply.started":"2026-01-07T01:15:34.067112Z","shell.execute_reply":"2026-01-07T01:15:34.067123Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport glob\nimport pandas as pd\nimport gc\n\n# --- CẤU HÌNH ---\nWORK_DIR = '/kaggle/working/temporal_data'\nif not os.path.exists(WORK_DIR):\n    WORK_DIR = '/kaggle/working'\n\nprint(\"--- [BƯỚC 1] DỌN DẸP & NÉN DỮ LIỆU ĐỂ EXPORT ---\")\n\n# 1. Xóa thẳng tay các file trung gian (Rác) để lấy chỗ trống nén file\nprint(\"1. Deleting intermediate junk files...\")\njunk_patterns = [\n    '*_id.csv', '*_cnt.csv', '*_svd.csv', '*_isrc.csv', \n    '*_stamp.csv', '*_experiments.csv', '*_before_after.csv',\n    'members*.csv', 'songs*.csv', 'train_add.csv', 'test_add.csv'\n]\n\ncleaned_size = 0\nfor pat in junk_patterns:\n    for f in glob.glob(f'{WORK_DIR}/{pat}'):\n        # CHẮC CHẮN KHÔNG XÓA FILE FINAL\n        if 'final' in f: \n            continue\n        try:\n            size = os.path.getsize(f)\n            cleaned_size += size\n            os.remove(f)\n        except: pass\n\nprint(f\"   -> Đã xóa rác, giải phóng: {cleaned_size / (1024**3):.2f} GB\")\n\n# 2. Chuyển đổi Final CSV -> Parquet (Nén dữ liệu)\ndef convert_to_parquet(name):\n    csv_path = f'{WORK_DIR}/{name}.csv'\n    parquet_path = f'{WORK_DIR}/{name}.parquet'\n    \n    if os.path.exists(csv_path):\n        print(f\"2. Converting {name}.csv to Parquet...\")\n        # Đọc CSV (Dùng chunk nếu RAM yếu, nhưng ở đây ưu tiên nhanh)\n        df = pd.read_csv(csv_path)\n        \n        # Downcast để giảm dung lượng file output\n        for col in df.columns:\n            if df[col].dtype == 'float64': df[col] = df[col].astype('float32')\n            if df[col].dtype == 'int64': df[col] = df[col].astype('int32')\n            \n        df.to_parquet(parquet_path, index=False)\n        print(f\"   -> Saved: {parquet_path}\")\n        \n        del df; gc.collect()\n        \n        # Xóa luôn CSV gốc để tiết kiệm ổ cứng tối đa\n        os.remove(csv_path)\n        print(f\"   -> Deleted original CSV: {csv_path}\")\n\nconvert_to_parquet('train_final')\nconvert_to_parquet('test_final')\n\nprint(\"\\n>>> XONG! Bây giờ trong Output chỉ còn 2 file .parquet nhẹ hều.\")\nprint(\"Hãy thực hiện BƯỚC 2 (Save Version) ngay.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T01:15:34.068254Z","iopub.status.idle":"2026-01-07T01:15:34.068529Z","shell.execute_reply.started":"2026-01-07T01:15:34.068383Z","shell.execute_reply":"2026-01-07T01:15:34.068394Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\n# 1. Load dữ liệu\nprint(\"--- LOADING DATA ---\")\ntry:\n    # Sửa đường dẫn nếu file của bạn nằm chỗ khác\n    train_df = pd.read_parquet('/kaggle/input/processed/train_final.parquet')\n    test_df = pd.read_parquet('/kaggle/input/processed/test_final.parquet')\n    print(\"Load thành công!\")\nexcept Exception as e:\n    print(f\"Lỗi load file: {e}\")\n    print(\"Bạn hãy kiểm tra lại đường dẫn file parquet nhé.\")\n\n# 2. Hàm soi dữ liệu chi tiết\ndef inspect_data(df, name=\"DATASET\"):\n    print(f\"\\n{'='*20} {name} INFO {'='*20}\")\n    print(f\"Shape: {df.shape} (Dòng, Cột)\")\n    \n    # Tạo bảng thống kê\n    info = pd.DataFrame({\n        'Type': df.dtypes,\n        'Null_Count': df.isnull().sum(),\n        'Null_Percent': (df.isnull().sum() / len(df)) * 100,\n        'Unique_Values': df.nunique(),\n        'Example_Value': [str(df[col].values[0])[:50] for col in df.columns] # Lấy giá trị đầu tiên để xem mẫu\n    })\n    \n    # Sắp xếp để dễ nhìn: Object lên đầu, Số xuống dưới\n    info = info.sort_values(by='Type')\n    print(info)\n    \n    # Kiểm tra xem có cột Target không\n    if 'target' in df.columns:\n        print(\"\\nTarget Distribution:\")\n        print(df['target'].value_counts(normalize=True))\n    else:\n        print(\"\\n(Không tìm thấy cột 'target' - Đây là tập Test hoặc Target đã bị tách)\")\n\n# 3. Thực thi soi\nif 'train_df' in locals():\n    inspect_data(train_df, \"TRAIN FINAL\")\n    # inspect_data(test_df, \"TEST FINAL\") # Bỏ comment nếu muốn xem cả test\n\nprint(\"\\n\" + \"=\"*50)\nprint(\"HÃY GỬI CHO TÔI KẾT QUẢ IN RA Ở TRÊN\")\nprint(\"Đặc biệt chú ý cột 'Example_Value' để biết nội dung là vector hay text thường.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T01:15:34.070742Z","iopub.status.idle":"2026-01-07T01:15:34.071008Z","shell.execute_reply.started":"2026-01-07T01:15:34.070878Z","shell.execute_reply":"2026-01-07T01:15:34.070889Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport catboost as cb\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nimport gc\n# --- BƯỚC VÁ LỖI THIẾU CỘT (QUAN TRỌNG) ---\ntrain_features = train_df.drop(columns=['target'])\nmissing_cols = list(set(train_features.columns) - set(test_df.columns))\n\nif len(missing_cols) > 0:\n    print(f\"Fixing {len(missing_cols)} missing columns in Test...\")\n    # Tạo bảng chứa các cột thiếu điền -1\n    missing_data = pd.DataFrame(-1, index=test_df.index, columns=missing_cols)\n    test_df = pd.concat([test_df, missing_data], axis=1)\n\n# Đồng bộ thứ tự cột\nX_test = test_df[train_features.columns].copy()\n\n# Xử lý Category (Ép kiểu Int)\ncat_features = [\n    'msno', 'song_id', 'source_system_tab', 'source_screen_name', \n    'source_type', 'city', 'gender', 'registered_via', 'language', \n    'artist_name', 'composer', 'lyricist', 'is_featured',\n    'first_genre_id', 'second_genre_id', 'third_genre_id'\n]\ncat_features += [c for c in train_features.columns if 'genre' in c or 'id' in c]\nvalid_cat_features = [c for c in cat_features if c in train_features.columns]\n\nprint(\"Converting Types...\")\nfor col in valid_cat_features:\n    train_df[col] = train_df[col].fillna(-1).astype(int)\n    X_test[col] = X_test[col].fillna(-1).astype(int)\n\n# 2. CHIA TRAIN/VAL (BẮT BUỘC ĐỂ CÓ BIỂU ĐỒ)\nprint(\"Splitting Train/Val for Plotting...\")\nX = train_df.drop(columns=['target'])\ny = train_df['target']\n\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Tạo Pool\ncat_indices = [X.columns.get_loc(c) for c in valid_cat_features]\ntrain_pool = cb.Pool(X_train, y_train, cat_features=cat_indices)\nval_pool = cb.Pool(X_val, y_val, cat_features=cat_indices)\ntest_pool = cb.Pool(X_test, cat_features=cat_indices)\n\n# Xóa rác\ndel train_df, test_df, X, y, X_train, X_val, y_train, y_val\ngc.collect()\n\n# 3. TRAIN MODEL (CÓ EVAL_SET)\nparams = {\n    'learning_rate': 0.2181,\n    'depth': 8,\n    'l2_leaf_reg': 7.84,\n    'random_strength': 0.612,\n    'bagging_temperature': 0.359,\n    'iterations': 3000, \n    'loss_function': 'Logloss',\n    'eval_metric': 'AUC',\n    'task_type': 'GPU',\n    'devices': '0',\n    'verbose': 100,\n    'early_stopping_rounds': 100 # Dừng sớm nếu không tốt hơn\n}\n\nprint(\"Start Training & Logging...\")\nmodel = cb.CatBoostClassifier(**params)\n# QUAN TRỌNG: Phải có eval_set mới vẽ được biểu đồ\nmodel.fit(train_pool, eval_set=val_pool)\n\n# 4. VẼ BIỂU ĐỒ CHO SLIDE\nprint(\"\\n--- DRAWING PLOTS ---\")\nhistory = model.get_evals_result()\nepochs = range(len(history['learn']['AUC']))\n\nfig, ax = plt.subplots(1, 2, figsize=(15, 6))\n\n# Biểu đồ AUC\nax[0].plot(epochs, history['learn']['AUC'], label='Train AUC', color='blue')\nax[0].plot(epochs, history['validation']['AUC'], label='Val AUC', color='orange')\nax[0].set_title(f\"Model AUC (Best Val: {max(history['validation']['AUC']):.4f})\", fontsize=14)\nax[0].set_xlabel(\"Iterations\")\nax[0].set_ylabel(\"AUC Score\")\nax[0].legend()\nax[0].grid(True, alpha=0.3)\n\n# Biểu đồ Loss\nax[1].plot(epochs, history['learn']['Logloss'], label='Train Loss', color='blue')\nax[1].plot(epochs, history['validation']['Logloss'], label='Val Loss', color='orange')\nax[1].set_title(\"Model LogLoss (Training Curve)\", fontsize=14)\nax[1].set_xlabel(\"Iterations\")\nax[1].set_ylabel(\"LogLoss\")\nax[1].legend()\nax[1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.savefig('catboost_training_slide.png', dpi=300) # Lưu ảnh nét căng để làm slide\nplt.show()\nprint(\"Đã lưu biểu đồ: catboost_training_slide.png\")\n\n# 5. SUBMIT LUÔN\nprint(\"Predicting for Submission...\")\npreds = model.predict_proba(test_pool)[:, 1]\n\ntry:\n    ids = pd.read_csv('/kaggle/input/kkbox-music-recommendation-challenge/test.csv', usecols=['id'])['id']\nexcept:\n    ids = range(len(preds))\n\nsub = pd.DataFrame({'id': ids, 'target': preds})\nfilename = 'submission_catboost_with_plot.csv'\nsub.to_csv(filename, index=False)\nprint(f\">>> DONE! Saved: {filename}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T01:15:34.072639Z","iopub.status.idle":"2026-01-07T01:15:34.073008Z","shell.execute_reply.started":"2026-01-07T01:15:34.072812Z","shell.execute_reply":"2026-01-07T01:15:34.072829Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\n\nprint(\"--- FINAL FIX: AUTO-ADJUST PLOT & SUBMIT ---\")\n\n# 1. LẤY DỮ LIỆU TỪ HISTORY\nhistory = model.get_evals_result()\n\n# Tìm đúng tên key (phòng trường hợp tên khác)\nval_auc_key = [k for k in history['validation'].keys() if 'AUC' in k][0]\nval_loss_key = [k for k in history['validation'].keys() if 'Logloss' in k][0]\ntrain_loss_key = [k for k in history['learn'].keys() if 'Logloss' in k][0]\n\n# Lấy dữ liệu ra\nval_auc_data = history['validation'][val_auc_key]      # Dài khoảng 601\nval_loss_data = history['validation'][val_loss_key]    # Dài khoảng 601\ntrain_loss_data = history['learn'][train_loss_key]     # Dài 3000\n\n# 2. TẠO TRỤC X TỰ ĐỘNG (Dựa trên độ dài thực tế của dữ liệu)\n# Trục X cho Train (từ 0 đến 2999)\nx_train = range(len(train_loss_data))\n\n# Trục X cho Val (Tự tính bước nhảy dựa trên tỷ lệ)\n# Ví dụ: nếu Train 3000 dòng, Val 601 dòng -> bước nhảy là 5\nstep = len(train_loss_data) / (len(val_auc_data) - 1) if len(val_auc_data) > 1 else 1\nx_val = [i * step for i in range(len(val_auc_data))]\n\nprint(f\"Shape check: Train X={len(x_train)}, Y={len(train_loss_data)}\")\nprint(f\"Shape check: Val X={len(x_val)}, Y={len(val_loss_data)}\")\n\n# 3. VẼ BIỂU ĐỒ\nfig, ax = plt.subplots(1, 2, figsize=(16, 6))\n\n# --- Biểu đồ 1: Validation AUC ---\nax[0].plot(x_val, val_auc_data, label=f'Val AUC (Best: {max(val_auc_data):.4f})', color='tab:orange', linewidth=2)\nax[0].set_title('Validation AUC Score')\nax[0].set_xlabel('Iterations')\nax[0].set_ylabel('AUC')\nax[0].legend()\nax[0].grid(True, alpha=0.3)\n\n# --- Biểu đồ 2: LogLoss (So sánh Train & Val) ---\nax[1].plot(x_train, train_loss_data, label='Train Loss', color='tab:blue', linestyle='--', alpha=0.6)\nax[1].plot(x_val, val_loss_data, label='Val Loss', color='tab:orange', linewidth=2)\nax[1].set_title('LogLoss Convergence')\nax[1].set_xlabel('Iterations')\nax[1].set_ylabel('LogLoss')\nax[1].legend()\nax[1].grid(True, alpha=0.3)\n\nplt.tight_layout()\n# Lưu ảnh\nimg_name = 'catboost_final_0895_corrected.png'\nplt.savefig(img_name, dpi=300)\nplt.show()\nprint(f\"Đã lưu biểu đồ thành công: {img_name}\")\n\n# 4. TẠO FILE SUBMISSION\nprint(\"\\n--- SUBMISSION GENERATION ---\")\nif 'test_pool' in locals():\n    print(\"Predicting...\")\n    preds = model.predict_proba(test_pool)[:, 1]\n    \n    # Lấy ID\n    try:\n        if 'ids' not in locals():\n            ids = pd.read_csv('/kaggle/input/kkbox-music-recommendation-challenge/test.csv', usecols=['id'])['id']\n    except:\n        ids = range(len(preds))\n\n    sub = pd.DataFrame({'id': ids, 'target': preds})\n    filename = 'submission_catboost_0895_FINAL.csv'\n    sub.to_csv(filename, index=False)\n    print(f\">>> XONG! File nộp bài của bạn: {filename}\")\nelse:\n    print(\"Cảnh báo: Không tìm thấy 'test_pool'. Bạn đã chạy bước tạo pool chưa?\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T01:15:34.074108Z","iopub.status.idle":"2026-01-07T01:15:34.074481Z","shell.execute_reply.started":"2026-01-07T01:15:34.074283Z","shell.execute_reply":"2026-01-07T01:15:34.074299Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 4. TẠO FILE SUBMISSION\nprint(\"\\n--- SUBMISSION GENERATION ---\")\nif 'test_pool' in locals():\n    print(\"Predicting...\")\n    preds = model.predict_proba(test_pool)[:, 1]\n    \n    # Lấy ID\n    try:\n        if 'ids' not in locals():\n            ids = pd.read_csv('/kaggle/input/kkbox-music-recommendation-challenge/test.csv', usecols=['id'])['id']\n    except:\n        ids = range(len(preds))\n\n    sub = pd.DataFrame({'id': ids, 'target': preds})\n    filename = 'submission_catboost_0895_FINAL.csv'\n    sub.to_csv(filename, index=False)\n    print(f\">>> XONG! File nộp bài của bạn: {filename}\")\nelse:\n    print(\"Cảnh báo: Không tìm thấy 'test_pool'. Bạn đã chạy bước tạo pool chưa?\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T01:15:34.075648Z","iopub.status.idle":"2026-01-07T01:15:34.075951Z","shell.execute_reply.started":"2026-01-07T01:15:34.075817Z","shell.execute_reply":"2026-01-07T01:15:34.075832Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport lightgbm as lgb\nfrom sklearn.decomposition import TruncatedSVD\nfrom scipy import sparse\nimport gc\nimport datetime\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\n\nprint(\"LIGHTGBM: LOGIC CHUẨN - TỐI ƯU HÓA\")\n\n# ==============================================================================\n# 1. LOAD DATA & MERGE (QUY HOẠCH TẬP TRUNG)\n# ==============================================================================\nINPUT_DIR = '/kaggle/working/'\n\nprint(\"1. Loading Raw Data...\")\n# Load Train/Test\ntrain = pd.read_csv(f'{INPUT_DIR}/train.csv', dtype={'target': np.int8})\ntest = pd.read_csv(f'{INPUT_DIR}/test.csv')\n\n# Load Members/Songs\nmembers = pd.read_csv(f'{INPUT_DIR}/members.csv')\nsongs = pd.read_csv(f'{INPUT_DIR}/songs.csv')\n\n# Ghép Train/Test để xử lý Feature đồng nhất (Tránh lệch pha, lệch encoding)\ntrain['is_train'] = 1\ntest['is_train'] = 0\ntest['target'] = np.nan\n\n# Nối lại\nfull_data = pd.concat([train, test], ignore_index=True)\nprint(f\"   -> Tổng dữ liệu: {len(full_data)} dòng. (Đang xử lý tập trung)\")\n\n# Dọn rác\ndel train, test\ngc.collect()\n\n# Merge thông tin phụ\nprint(\"2. Merging Members & Songs...\")\nfull_data = full_data.merge(members, on='msno', how='left')\nfull_data = full_data.merge(songs, on='song_id', how='left')\ndel members, songs\ngc.collect()\n\n# ==============================================================================\n# 2. FEATURE ENGINEERING: TIME & BASIC\n# ==============================================================================\nprint(\"3. Feature Engineering: Time & Basic...\")\n\n# Xử lý ngày tháng\nfor col in ['registration_init_time', 'expiration_date']:\n    full_data[col] = pd.to_datetime(full_data[col], format='%Y%m%d', errors='coerce')\n\n# Time Left (Thời hạn gói cước - Feature cực mạnh)\nfull_data['time_left'] = (full_data['expiration_date'] - full_data['registration_init_time']).dt.days.fillna(0).astype(int)\nfull_data['reg_year'] = full_data['registration_init_time'].dt.year.fillna(2017).astype(int)\nfull_data['reg_month'] = full_data['registration_init_time'].dt.month.fillna(1).astype(int)\n\n# Chuyển đổi timestamp thành số để LightGBM học (dùng Unix timestamp)\nfull_data['timestamp'] = full_data['registration_init_time'].astype('int64') // 10**9\nfull_data['timestamp'] = full_data['timestamp'].replace(-9223372037, 0) # Fix lỗi NaT conversion\n\n# Xử lý Category (Label Encoding cho LightGBM)\n# LightGBM xử lý category rất tốt nếu ta chuyển nó về dạng số nguyên (0, 1, 2...)\ncat_cols = ['source_system_tab', 'source_screen_name', 'source_type', \n            'city', 'gender', 'registered_via', 'language', 'artist_name', 'composer', 'lyricist', \n            'msno', 'song_id', 'genre_ids'] # Mã hóa cả ID để dùng cho SVD sau này\n\nprint(\"   -> Encoding Categoricals...\")\nfor col in cat_cols:\n    full_data[col] = full_data[col].astype(str).astype('category')\n    # Lưu lại code để dùng (ví dụ cho SVD)\n    full_data[col] = full_data[col].cat.codes\n\n# ==============================================================================\n# 3. FEATURE ENGINEERING: PROBABILITY (THÓI QUEN NGƯỜI DÙNG)\n# ==============================================================================\nprint(\"4. Feature Engineering: Probability (User Habits)...\")\n# Logic: Tính xác suất User A nghe nhạc ở ngữ cảnh B. P(Context | User)\n\ndef create_prob_feature(df, group_col, target_col, name):\n    # Đếm tổng lượt tương tác của user\n    group_counts = df.groupby(group_col)[target_col].transform('count')\n    # Đếm lượt tương tác trong ngữ cảnh cụ thể\n    # (Để tiết kiệm RAM, ta dùng frequency encoding kết hợp)\n    context_counts = df.groupby([group_col, target_col])[target_col].transform('count')\n    \n    df[name] = context_counts / group_counts\n    return df\n\n# User hay nghe ở Source Type nào? (Top 1 Feature)\nfull_data = create_prob_feature(full_data, 'msno', 'source_type', 'msno_source_type_prob')\n\n# User hay nghe ở Screen Name nào?\nfull_data = create_prob_feature(full_data, 'msno', 'source_screen_name', 'msno_source_screen_name_prob')\n\n# User hay nghe nhạc của Artist nào? (Có thể nặng RAM, nếu crash thì bỏ dòng này)\n# full_data = create_prob_feature(full_data, 'msno', 'artist_name', 'msno_artist_name_prob')\n\n# Đếm tổng số lần user xuất hiện (Rec Count)\nfull_data['msno_rec_cnt'] = full_data.groupby('msno')['msno'].transform('count')\n\n# ==============================================================================\n# 4. FEATURE ENGINEERING: MATRIX FACTORIZATION (SVD - VŨ KHÍ BÍ MẬT)\n# ==============================================================================\nprint(\"5. Feature Engineering: SVD / Embeddings (The Secret Weapon)...\")\n\n# Tạo ma trận thưa User x Song\n# Row: msno, Col: song_id, Value: 1 (Interaction)\n# Vì ta đã mã hóa msno và song_id thành số nguyên (0..N) ở bước Encoding nên dùng trực tiếp được\nn_users = full_data['msno'].max() + 1\nn_songs = full_data['song_id'].max() + 1\n\n# Chỉ dùng tập Train để học SVD (tránh data leakage từ tương lai/test set nếu muốn khắt khe)\n# Nhưng để tốt nhất cho Kaggle, ta thường dùng cả Full Data để học features ẩn\nrows = full_data['msno'].values\ncols = full_data['song_id'].values\ndata = np.ones(len(full_data))\n\nsparse_matrix = sparse.csr_matrix((data, (rows, cols)), shape=(n_users, n_songs))\n\n# Thực hiện SVD (Giảm chiều xuống 20-30 features)\nn_components = 20 # Số lượng feature ẩn\nprint(f\"   -> Running TruncatedSVD ({n_components} components)...\")\nsvd = TruncatedSVD(n_components=n_components, random_state=42)\n# Ta cần embedding của User và Song. \n# Tuy nhiên TruncatedSVD cho ra User Vectors. Để có Song Vectors ta cần fit trên transpose hoặc dùng kỹ thuật khác.\n# Cách đơn giản & hiệu quả: Dùng SVD trên User-Song matrix -> Lấy User Embeddings (Left singular vectors)\nuser_vectors = svd.fit_transform(sparse_matrix)\nsong_vectors = svd.components_.T # Right singular vectors (Song embeddings)\n\n# Tính Dot Product: Sự tương hợp giữa User và Song\n# Dot = User_Vec * Song_Vec\nprint(\"   -> Calculating Dot Product (Interaction Score)...\")\n# Vì không thể nhân ma trận lớn, ta tính dot product cho từng hàng tương ứng\n# Lấy vector tương ứng cho từng dòng dữ liệu\nu_vecs = user_vectors[full_data['msno'].values]\ns_vecs = song_vectors[full_data['song_id'].values]\n\n# Tính dot product từng cặp\nfull_data['song_embeddings_dot'] = (u_vecs * s_vecs).sum(axis=1)\n\n# Thêm các User Components vào bảng (member_component_0...n)\n# Đây chính là các tính năng \"Ẩn\" mà LightGBM rất thích\ncomponent_cols = [f'member_component_{i}' for i in range(5)] # Lấy 5 cái quan trọng nhất để tiết kiệm RAM\ndf_user_comps = pd.DataFrame(user_vectors[:, :5], columns=component_cols)\ndf_user_comps['msno'] = range(n_users) # Index tương ứng với mã hóa\n\nfull_data = full_data.merge(df_user_comps, on='msno', how='left')\n\n# Dọn dẹp RAM khẩn cấp\ndel sparse_matrix, user_vectors, song_vectors, u_vecs, s_vecs, df_user_comps, svd\ngc.collect()\n\n# ==============================================================================\n# 5. PREPARE & TRAIN LIGHTGBM (CÓ LOG & BIỂU ĐỒ)\n# ==============================================================================\nprint(\"6. Preparing to Train...\")\n# Loại bỏ các cột không dùng\ncols_to_drop = ['registration_init_time', 'expiration_date', 'is_train', 'target', 'id']\nfeatures = [c for c in full_data.columns if c not in cols_to_drop]\n\nprint(f\"   -> Training Features ({len(features)}): {features}\")\n\n# Tách dữ liệu\ntrain_full = full_data[full_data['is_train'] == 1]\ntest_df = full_data[full_data['is_train'] == 0]\ny_full = full_data[full_data['is_train'] == 1]['target']\nsubmission_ids = test_df['id'].values\n\n# Xóa bảng to để nhẹ RAM\ndel full_data\ngc.collect()\n\n# --- BƯỚC MỚI: CHIA TRAIN/VAL ĐỂ VẼ BIỂU ĐỒ ---\n# Chia 90% Train - 10% Validation (Để theo dõi model học)\nprint(\"   -> Splitting Train/Val for Monitoring...\")\nX_train, X_val, y_train, y_val = train_test_split(\n    train_full[features], y_full, test_size=0.1, random_state=42\n)\n\n# Tạo Dataset cho LGBM\nprint(\"7. Training LightGBM...\")\ndtrain = lgb.Dataset(X_train, label=y_train)\ndval = lgb.Dataset(X_val, label=y_val, reference=dtrain)\n\n# # Tham số LightGBM\n# params = {\n#     'objective': 'binary',\n#     'metric': 'auc', # Metric để vẽ biểu đồ\n#     'boosting': 'gbdt',\n#     'learning_rate': 0.1,\n#     'verbose': -1, # Tắt log hệ thống, chỉ dùng log_evaluation\n#     'num_leaves': 128,\n#     'bagging_fraction': 0.65,\n#     'feature_fraction': 1,\n#     'max_depth': 10,\n#     'seed': 42,\n#     # 'device': 'gpu' # Bật nếu dùng GPU\n# }\n\n# # Biến lưu lịch sử để vẽ hình\n# evals_result = {}\n\n# # Train\n# model = lgb.train(\n#     params,\n#     dtrain,\n#     num_boost_round=2400,\n#     valid_sets=[dtrain, dval],      # Đưa cả train và val vào để theo dõi\n#     valid_names=['train', 'valid'], # Đặt tên cho biểu đồ\n#     evals_result=evals_result,      # Lưu kết quả vào đây\n#     callbacks=[\n#         lgb.log_evaluation(period=100), # In log mỗi 100 vòng\n#         lgb.early_stopping(stopping_rounds=100) # Dừng sớm nếu không cải thiện\n#     ]\n# )\n\n# # --- BƯỚC MỚI: VẼ BIỂU ĐỒ ---\n# print(\"\\n--- PLOTTING TRAINING CURVE ---\")\n# lgb.plot_metric(evals_result, metric='auc')\n# plt.title('LightGBM Training AUC')\n# plt.grid(True)\n# plt.show()\n\n# # Feature Importance\n# print(\"\\nTop 10 Feature Importance:\")\n# importance = pd.DataFrame({\n#     'feature': model.feature_name(),\n#     'gain': model.feature_importance(importance_type='gain')\n# }).sort_values('gain', ascending=False)\n# print(importance.head(10))\n\n# # ==============================================================================\n# # 6. PREDICT & SUBMIT\n# # ==============================================================================\n# print(\"8. Predicting & Submitting...\")\n# # Dự đoán trên tập Test\n# preds = model.predict(test_df[features], num_iteration=model.best_iteration)\n\n# sub = pd.DataFrame({'id': submission_ids, 'target': preds})\n# filename = 'submission_LGBM_LOGIC_OPTIMIZED.csv'\n# sub.to_csv(filename, index=False)\n\n# print(f\">>> HOÀN TẤT! File: {filename}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T01:18:06.010046Z","iopub.execute_input":"2026-01-07T01:18:06.010346Z","iopub.status.idle":"2026-01-07T01:19:25.981616Z","shell.execute_reply.started":"2026-01-07T01:18:06.010324Z","shell.execute_reply":"2026-01-07T01:19:25.980642Z"}},"outputs":[{"name":"stdout","text":"LIGHTGBM: LOGIC CHUẨN - TỐI ƯU HÓA\n1. Loading Raw Data...\n   -> Tổng dữ liệu: 9934208 dòng. (Đang xử lý tập trung)\n2. Merging Members & Songs...\n3. Feature Engineering: Time & Basic...\n   -> Encoding Categoricals...\n4. Feature Engineering: Probability (User Habits)...\n5. Feature Engineering: SVD / Embeddings (The Secret Weapon)...\n   -> Running TruncatedSVD (20 components)...\n   -> Calculating Dot Product (Interaction Score)...\n6. Preparing to Train...\n   -> Training Features (28): ['msno', 'song_id', 'source_system_tab', 'source_screen_name', 'source_type', 'city', 'bd', 'gender', 'registered_via', 'song_length', 'genre_ids', 'artist_name', 'composer', 'lyricist', 'language', 'time_left', 'reg_year', 'reg_month', 'timestamp', 'msno_source_type_prob', 'msno_source_screen_name_prob', 'msno_rec_cnt', 'song_embeddings_dot', 'member_component_0', 'member_component_1', 'member_component_2', 'member_component_3', 'member_component_4']\n   -> Splitting Train/Val for Monitoring...\n7. Training LightGBM...\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import lightgbm as lgb\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport gc\n\nprint(\"--- RE-TRAINING LIGHTGBM (FIXED CALLBACKS) ---\")\n\n# 1. Kiểm tra xem dữ liệu còn trong RAM không\nif 'dtrain' not in locals() or 'dval' not in locals():\n    print(\"⚠️ Cảnh báo: Biến dtrain/dval bị mất. Đang tạo lại...\")\n    # Tái tạo lại nếu lỡ mất\n    dtrain = lgb.Dataset(X_train, label=y_train)\n    dval = lgb.Dataset(X_val, label=y_val, reference=dtrain)\nelse:\n    print(\"✅ Đã tìm thấy dữ liệu trong RAM. Bắt đầu train ngay...\")\n\n# 2. Cấu hình tham số\nparams = {\n    'objective': 'binary',\n    'metric': 'auc',\n    'boosting': 'gbdt',\n    'learning_rate': 0.1,\n    'verbose': 1,          # Tắt log hệ thống thừa\n    'num_leaves': 128,\n    'bagging_fraction': 0.65,\n    'feature_fraction': 1,\n    'max_depth': 10,\n    'seed': 42,\n    # 'device': 'gpu'      # Uncomment nếu có GPU\n}\n\n# 3. Chuẩn bị biến lưu kết quả\nevals_result = {} \n\n# 4. Train với Callbacks chuẩn (SỬA LỖI Ở ĐÂY)\nprint(\"Training started...\")\nmodel = lgb.train(\n    params,\n    dtrain,\n    num_boost_round=6000,\n    valid_sets=[dtrain, dval],\n    valid_names=['train', 'valid'],\n    callbacks=[\n        lgb.log_evaluation(period=100),          # In log mỗi 100 vòng\n        lgb.early_stopping(stopping_rounds=100), # Dừng sớm nếu không tốt hơn\n        lgb.record_evaluation(evals_result)      # <--- FIX: Ghi kết quả qua callback\n    ]\n)\n\n# 5. VẼ BIỂU ĐỒ TRAINING CURVE\nprint(\"\\n--- PLOTTING TRAINING CURVE ---\")\ntry:\n    lgb.plot_metric(evals_result, metric='auc')\n    plt.title('LightGBM Training AUC (Learning Curve)')\n    plt.grid(True, alpha=0.3)\n    plt.savefig('lgb_training_curve.png', dpi=300) # Lưu ảnh lại\n    plt.show()\n    print(\"Đã lưu biểu đồ: lgb_training_curve.png\")\nexcept Exception as e:\n    print(f\"Lỗi vẽ biểu đồ (không ảnh hưởng kết quả): {e}\")\n\n# 6. FEATURE IMPORTANCE\nprint(\"\\nTop 15 Feature Importance:\")\nimportance = pd.DataFrame({\n    'feature': model.feature_name(),\n    'gain': model.feature_importance(importance_type='gain')\n}).sort_values('gain', ascending=False)\nprint(importance.head(15))\n\n# 7. DỰ ĐOÁN & NỘP BÀI\nprint(\"\\n--- PREDICTING & SUBMITTING ---\")\nif 'test_df' in locals() and 'features' in locals():\n    # Dự đoán\n    preds = model.predict(test_df[features], num_iteration=model.best_iteration)\n    \n    # Load ID chuẩn để tạo file\n    if 'submission_ids' not in locals():\n        submission_ids = pd.read_csv('/kaggle/input/kkbox-music-recommendation-challenge/test.csv', usecols=['id'])['id'].values\n        \n    sub = pd.DataFrame({'id': submission_ids, 'target': preds})\n    \n    # Fix lỗi định dạng ID float -> int\n    sub['id'] = sub['id'].astype(int)\n    \n    filename = 'submission_LGBM_FINAL_LOGIC.csv'\n    sub.to_csv(filename, index=False)\n    \n    print(f\">>> HOÀN TẤT! File: {filename}\")\nelse:\n    print(\"Lỗi: Không tìm thấy test_df để dự đoán. Bạn cần chạy lại bước load data.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T01:19:45.646823Z","iopub.execute_input":"2026-01-07T01:19:45.647193Z","iopub.status.idle":"2026-01-07T03:23:11.048512Z","shell.execute_reply.started":"2026-01-07T01:19:45.647165Z","shell.execute_reply":"2026-01-07T03:23:11.046567Z"}},"outputs":[{"name":"stdout","text":"--- RE-TRAINING LIGHTGBM (FIXED CALLBACKS) ---\n✅ Đã tìm thấy dữ liệu trong RAM. Bắt đầu train ngay...\nTraining started...\n[LightGBM] [Info] Number of positive: 3342464, number of negative: 3297212\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.229185 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 4641\n[LightGBM] [Info] Number of data points in the train set: 6639676, number of used features: 28\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503408 -> initscore=0.013631\n[LightGBM] [Info] Start training from score 0.013631\nTraining until validation scores don't improve for 100 rounds\n[100]\ttrain's auc: 0.760478\tvalid's auc: 0.759063\n[200]\ttrain's auc: 0.775521\tvalid's auc: 0.772998\n[300]\ttrain's auc: 0.785758\tvalid's auc: 0.782235\n[400]\ttrain's auc: 0.793203\tvalid's auc: 0.788703\n[500]\ttrain's auc: 0.799141\tvalid's auc: 0.793701\n[600]\ttrain's auc: 0.804244\tvalid's auc: 0.797944\n[700]\ttrain's auc: 0.808313\tvalid's auc: 0.801116\n[800]\ttrain's auc: 0.812275\tvalid's auc: 0.804298\n[900]\ttrain's auc: 0.815829\tvalid's auc: 0.807066\n[1000]\ttrain's auc: 0.818985\tvalid's auc: 0.809418\n[1100]\ttrain's auc: 0.821987\tvalid's auc: 0.811623\n[1200]\ttrain's auc: 0.824756\tvalid's auc: 0.81358\n[1300]\ttrain's auc: 0.827224\tvalid's auc: 0.815191\n[1400]\ttrain's auc: 0.829462\tvalid's auc: 0.81661\n[1500]\ttrain's auc: 0.831571\tvalid's auc: 0.817862\n[1600]\ttrain's auc: 0.833732\tvalid's auc: 0.819198\n[1700]\ttrain's auc: 0.83568\tvalid's auc: 0.820384\n[1800]\ttrain's auc: 0.837468\tvalid's auc: 0.821348\n[1900]\ttrain's auc: 0.839361\tvalid's auc: 0.822499\n[2000]\ttrain's auc: 0.840997\tvalid's auc: 0.823312\n[2100]\ttrain's auc: 0.842706\tvalid's auc: 0.824288\n[2200]\ttrain's auc: 0.844366\tvalid's auc: 0.825177\n[2300]\ttrain's auc: 0.845932\tvalid's auc: 0.825982\n[2400]\ttrain's auc: 0.847474\tvalid's auc: 0.826799\n[2500]\ttrain's auc: 0.848915\tvalid's auc: 0.827455\n[2600]\ttrain's auc: 0.850364\tvalid's auc: 0.828145\n[2700]\ttrain's auc: 0.85169\tvalid's auc: 0.828716\n[2800]\ttrain's auc: 0.853019\tvalid's auc: 0.829295\n[2900]\ttrain's auc: 0.854282\tvalid's auc: 0.829786\n[3000]\ttrain's auc: 0.855486\tvalid's auc: 0.83029\n[3100]\ttrain's auc: 0.856826\tvalid's auc: 0.830965\n[3200]\ttrain's auc: 0.858033\tvalid's auc: 0.831449\n[3300]\ttrain's auc: 0.859185\tvalid's auc: 0.831929\n[3400]\ttrain's auc: 0.860316\tvalid's auc: 0.832328\n[3500]\ttrain's auc: 0.861459\tvalid's auc: 0.832751\n[3600]\ttrain's auc: 0.862531\tvalid's auc: 0.833127\n[3700]\ttrain's auc: 0.863635\tvalid's auc: 0.833531\n[3800]\ttrain's auc: 0.864719\tvalid's auc: 0.833986\n[3900]\ttrain's auc: 0.865781\tvalid's auc: 0.834293\n[4000]\ttrain's auc: 0.866807\tvalid's auc: 0.834638\n[4100]\ttrain's auc: 0.867859\tvalid's auc: 0.834999\n[4200]\ttrain's auc: 0.868893\tvalid's auc: 0.835382\n[4300]\ttrain's auc: 0.869818\tvalid's auc: 0.835654\n[4400]\ttrain's auc: 0.870789\tvalid's auc: 0.835913\n[4500]\ttrain's auc: 0.871727\tvalid's auc: 0.836174\n[4600]\ttrain's auc: 0.872675\tvalid's auc: 0.836527\n[4700]\ttrain's auc: 0.873573\tvalid's auc: 0.836798\n[4800]\ttrain's auc: 0.874463\tvalid's auc: 0.836977\n[4900]\ttrain's auc: 0.875374\tvalid's auc: 0.837234\n[5000]\ttrain's auc: 0.876245\tvalid's auc: 0.837463\n[5100]\ttrain's auc: 0.877096\tvalid's auc: 0.837701\n[5200]\ttrain's auc: 0.877925\tvalid's auc: 0.837907\n[5300]\ttrain's auc: 0.878771\tvalid's auc: 0.838153\n[5400]\ttrain's auc: 0.879609\tvalid's auc: 0.838365\n[5500]\ttrain's auc: 0.880398\tvalid's auc: 0.838512\n[5600]\ttrain's auc: 0.881203\tvalid's auc: 0.838663\n[5700]\ttrain's auc: 0.881999\tvalid's auc: 0.838873\n[5800]\ttrain's auc: 0.882798\tvalid's auc: 0.839039\n[5900]\ttrain's auc: 0.883552\tvalid's auc: 0.839213\n[6000]\ttrain's auc: 0.884308\tvalid's auc: 0.839388\nDid not meet early stopping. Best iteration is:\n[5999]\ttrain's auc: 0.884302\tvalid's auc: 0.839388\n\n--- PLOTTING TRAINING CURVE ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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\n"},"metadata":{}},{"name":"stdout","text":"Đã lưu biểu đồ: lgb_training_curve.png\n\nTop 15 Feature Importance:\n                         feature          gain\n22           song_embeddings_dot  3.109741e+06\n4                    source_type  2.826485e+06\n19         msno_source_type_prob  9.750040e+05\n20  msno_source_screen_name_prob  8.250431e+05\n21                  msno_rec_cnt  8.038931e+05\n25            member_component_2  7.624338e+05\n24            member_component_1  7.485606e+05\n26            member_component_3  7.439607e+05\n0                           msno  7.278720e+05\n27            member_component_4  7.086072e+05\n23            member_component_0  6.730657e+05\n15                     time_left  5.458434e+05\n11                   artist_name  5.088701e+05\n18                     timestamp  4.167383e+05\n3             source_screen_name  3.676576e+05\n\n--- PREDICTING & SUBMITTING ---\n>>> HOÀN TẤT! File: submission_LGBM_FINAL_LOGIC.csv\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import pandas as pd\nimport lightgbm as lgb\nimport gc\n\nprint(\"--- CHIẾN THUẬT: BACK-TESTING (DỰ ĐOÁN TẠI CÁC ĐIỂM DỪNG KHÁC NHAU) ---\")\n\n# Load lại Test IDs (để đảm bảo an toàn)\nif 'submission_ids' not in locals():\n    print(\"Loading submission IDs...\")\n    submission_ids = pd.read_csv('/kaggle/input/kkbox-music-recommendation-challenge/test.csv', usecols=['id'])['id'].values\n\n# Load lại features cần thiết (nếu mất)\nif 'test_df' in locals():\n    X_test_predict = test_df[features]\nelse:\n    print(\"⚠️ Cảnh báo: Biến test_df không tìm thấy. Bạn cần load lại dữ liệu test.\")\n\n# TẠO 3 FILE SUBMISSION Ở CÁC MỐC KHÁC NHAU\n# Mốc 1: 1500 vòng (An toàn, giống lúc 1000 rounds)\n# Mốc 2: 2500 vòng (Cân bằng, lúc Valid AUC ~0.827)\n# Mốc 3: 4000 vòng (Hơi gắt, thử vận may)\n\ncheckpoints = [1500, 2500, 4000]\n\nfor rounds in checkpoints:\n    print(f\"\\n>>> Đang dự đoán tại mốc {rounds} rounds...\")\n    \n    # Dùng num_iteration để giới hạn số cây\n    preds = model.predict(X_test_predict, num_iteration=rounds)\n    \n    sub = pd.DataFrame({'id': submission_ids, 'target': preds})\n    sub['id'] = sub['id'].astype(int) # Chắc chắn là int\n    \n    filename = f'submission_LGBM_rollback_{rounds}.csv'\n    sub.to_csv(filename, index=False)\n    print(f\"   -> Đã tạo file: {filename}\")\n\nprint(\"\\n>>> KHUYẾN NGHỊ:\")\nprint(\"1. Hãy nộp file 'submission_LGBM_rollback_2500.csv' trước.\")\nprint(\"2. Nếu vẫn thấp, nộp file 'submission_LGBM_rollback_1500.csv'.\")\nprint(\"3. File 6000 rounds (bạn đã nộp) đang bị Overfit.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T03:29:56.785506Z","iopub.execute_input":"2026-01-07T03:29:56.785898Z","iopub.status.idle":"2026-01-07T03:39:31.564501Z","shell.execute_reply.started":"2026-01-07T03:29:56.785811Z","shell.execute_reply":"2026-01-07T03:39:31.563266Z"}},"outputs":[{"name":"stdout","text":"--- CHIẾN THUẬT: BACK-TESTING (DỰ ĐOÁN TẠI CÁC ĐIỂM DỪNG KHÁC NHAU) ---\n\n>>> Đang dự đoán tại mốc 1500 rounds...\n   -> Đã tạo file: submission_LGBM_rollback_1500.csv\n\n>>> Đang dự đoán tại mốc 2500 rounds...\n   -> Đã tạo file: submission_LGBM_rollback_2500.csv\n\n>>> Đang dự đoán tại mốc 4000 rounds...\n   -> Đã tạo file: submission_LGBM_rollback_4000.csv\n\n>>> KHUYẾN NGHỊ:\n1. Hãy nộp file 'submission_LGBM_rollback_2500.csv' trước.\n2. Nếu vẫn thấp, nộp file 'submission_LGBM_rollback_1500.csv'.\n3. File 6000 rounds (bạn đã nộp) đang bị Overfit.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import pandas as pd\nimport lightgbm as lgb\nimport gc\n\nprint(\"--- PREDICT AT ROUND 3000 (TESTING BOUNDARY) ---\")\n\n# 1. KIỂM TRA MODEL\nif 'model' in locals():\n    print(\"✅ Đang dùng Model có sẵn trong RAM.\")\n    predictor = model\nelse:\n    # Thử load lại nếu lỡ mất\n    try:\n        print(\"⚠️ Đang load lại model từ file checkpoint...\")\n        predictor = lgb.Booster(model_file='lgb_checkpoint_2400.txt') # Hoặc tên file bạn đã lưu\n        # Lưu ý: Nếu file checkpoint chỉ có 2400 vòng thì không thể predict 3000 được.\n        # Nhưng nếu bạn đã chạy bước train tiếp (extended) thì model trong RAM đã có đủ số vòng.\n    except:\n        print(\"❌ LỖI: Không tìm thấy model. Bạn cần đảm bảo model đã được train ít nhất 3000 vòng.\")\n        # Dừng lại để tránh lỗi\n        predictor = None\n\nif predictor:\n    # Kiểm tra xem model đã train đủ 3000 vòng chưa\n    current_rounds = predictor.current_iteration()\n    if current_rounds < 3000:\n        print(f\"⚠️ CẢNH BÁO: Model hiện tại chỉ mới train đến vòng {current_rounds}. Không thể lấy vòng 3000.\")\n        print(f\"-> Sẽ lấy vòng cuối cùng hiện có: {current_rounds}\")\n        predict_round = current_rounds\n    else:\n        predict_round = 2000\n\n    # 2. DỰ ĐOÁN\n    print(f\"Predicting at iteration {predict_round}...\")\n    \n    # Load feature cần thiết\n    if 'test_df' in locals() and 'features' in locals():\n        preds = predictor.predict(test_df[features], num_iteration=predict_round)\n        \n        # 3. TẠO FILE NỘP\n        if 'submission_ids' not in locals():\n            submission_ids = pd.read_csv('/kaggle/input/kkbox-music-recommendation-challenge/test.csv', usecols=['id'])['id'].values\n\n        sub = pd.DataFrame({'id': submission_ids, 'target': preds})\n        sub['id'] = sub['id'].astype(int) # Chắc chắn là int\n        \n        filename = f'submission_LGBM_rollback_{predict_round}.csv'\n        sub.to_csv(filename, index=False)\n        \n        print(f\">>> XONG! File nộp bài: {filename}\")\n        print(\"Hãy thử nộp xem vận may thế nào nhé!\")\n    else:\n        print(\"❌ LỖI: Thiếu biến test_df hoặc features. Bạn cần load lại dữ liệu Test.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T04:06:50.838549Z","iopub.execute_input":"2026-01-07T04:06:50.838837Z","iopub.status.idle":"2026-01-07T04:09:10.653029Z","shell.execute_reply.started":"2026-01-07T04:06:50.838819Z","shell.execute_reply":"2026-01-07T04:09:10.652213Z"}},"outputs":[{"name":"stdout","text":"--- PREDICT AT ROUND 3000 (TESTING BOUNDARY) ---\n✅ Đang dùng Model có sẵn trong RAM.\nPredicting at iteration 2000...\n>>> XONG! File nộp bài: submission_LGBM_rollback_2000.csv\nHãy thử nộp xem vận may thế nào nhé!\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"import lightgbm as lgb\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nprint(\"--- FINAL STAGE: SAVING & FINE-TUNING ---\")\n\n# 1. LƯU MODEL HIỆN TẠI (QUAN TRỌNG)\n# Đây là mốc 5600 vòng - kết quả tốt nhất từ trước đến giờ\nmodel.save_model('lgb_model_best_6800.txt')\nprint(\"✅ Đã lưu model mốc 5600 vòng (Backup an toàn).\")\n\n# 2. TẠO FILE SUBMISSION (PHIÊN BẢN 5600)\n# Nộp ngay file này để \"chốt hạ\" điểm số 0.838+\nprint(\">>> Đang tạo file submission (Mốc 5600)...\")\nif 'test_df' in locals() and 'features' in locals():\n    preds = model.predict(test_df[features], num_iteration=model.best_iteration)\n    \n    if 'submission_ids' not in locals():\n        submission_ids = pd.read_csv('/kaggle/input/kkbox-music-recommendation-challenge/test.csv', usecols=['id'])['id'].values\n        \n    sub = pd.DataFrame({'id': submission_ids, 'target': preds})\n    sub['id'] = sub['id'].astype(int)\n    \n    filename_5600 = 'submission_LGBM_BEST_5600.csv'\n    sub.to_csv(filename_5600, index=False)\n    print(f\"✅ ĐÃ XUẤT FILE: {filename_5600}\")\n    print(\"👉 Hãy nộp file này lên Leaderboard ngay!\")\n\n# 3. CHIẾN THUẬT: GIẢM LEARNING RATE ĐỂ VẮT KIỆT HIỆU SUẤT (OPTIONAL)\n# Giảm LR từ 0.1 xuống 0.01 để model học chậm lại và kỹ hơn\nprint(\"\\n--- OPTIONAL: SQUEEZING THE LEMON (LR 0.01) ---\")\n\n# Cập nhật tham số\nparams['learning_rate'] = 0.01\nparams['num_leaves'] = 128 # Giữ nguyên\n\nprint(\"Training thêm 500 vòng với Learning Rate 0.01...\")\n\ntry:\n    # Train tiếp dựa trên model 5600\n    model = lgb.train(\n        params,\n        dtrain,\n        num_boost_round=500, # Train ít thôi nhưng chất\n        init_model=model,    # Tiếp tục từ model hiện tại\n        valid_sets=[dtrain, dval],\n        valid_names=['train', 'valid'],\n        callbacks=[\n            lgb.log_evaluation(period=50),\n            lgb.early_stopping(stopping_rounds=50),\n            # lgb.record_evaluation(evals_result) # Có thể bỏ qua nếu không cần vẽ tiếp\n        ]\n    )\n    \n    # Xuất file phiên bản \"Vắt kiệt\"\n    print(\">>> Đang tạo file submission (Final Squeeze)...\")\n    preds_final = model.predict(test_df[features], num_iteration=model.best_iteration)\n    sub['target'] = preds_final\n    filename_final = 'submission_LGBM_FINAL_SQUEEZE.csv'\n    sub.to_csv(filename_final, index=False)\n    print(f\"✅ ĐÃ XUẤT FILE CUỐI CÙNG: {filename_final}\")\n    \nexcept Exception as e:\n    print(f\"Không thể train tiếp (có thể do RAM hoặc data đã bị xóa): {e}\")\n    print(\"Không sao cả, bạn đã có file submission_LGBM_BEST_5600.csv rất xịn rồi!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T01:15:34.080264Z","iopub.status.idle":"2026-01-07T01:15:34.080618Z","shell.execute_reply.started":"2026-01-07T01:15:34.080453Z","shell.execute_reply":"2026-01-07T01:15:34.080473Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import lightgbm as lgb\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nprint(\"--- TIẾP TỤC TRAIN (FINE-TUNING) ---\")\n\n# 1. KIỂM TRA ĐIỀU KIỆN\nif 'model' not in locals() or 'dtrain' not in locals():\n    print(\"❌ LỖI: Không tìm thấy model hoặc data trong RAM. Bạn cần chạy lại bước train trước đó.\")\nelse:\n    print(\"✅ Đã tìm thấy Model cũ. Chuẩn bị train tiếp...\")\n    \n    # Lưu model hiện tại đề phòng rủi ro\n    model.save_model('lgb_checkpoint_2400.txt')\n    print(\"   -> Đã backup model hiện tại: lgb_checkpoint_2400.txt\")\n\n    # 2. THIẾT LẬP TRAIN TIẾP\n    # Giữ nguyên params, hoặc có thể giảm learning_rate đi một chút để học kỹ hơn (tùy chọn)\n    # Ở đây ta giữ nguyên để giữ đà tăng tốc\n    new_rounds = 1500  # Train thêm 1000 vòng nữa\n    \n    # Tạo biến lưu lịch sử mới\n    evals_result_new = {} \n\n    print(f\"   -> Training thêm {new_rounds} rounds nữa...\")\n    \n    # 3. TRAIN TIẾP (QUAN TRỌNG: init_model=model)\n    model = lgb.train(\n        params,\n        dtrain,\n        num_boost_round=new_rounds,\n        init_model=model,            # <--- MẤU CHỐT LÀ ĐÂY: Dùng model cũ làm nền tảng\n        valid_sets=[dtrain, dval],\n        valid_names=['train', 'valid'],\n        callbacks=[\n            lgb.log_evaluation(period=100),\n            lgb.early_stopping(stopping_rounds=100),\n            lgb.record_evaluation(evals_result_new)\n        ]\n    )\n    \n    # 4. CẬP NHẬT LỊCH SỬ ĐỂ VẼ HÌNH\n    # Nối kết quả mới vào kết quả cũ (nếu có biến evals_result từ trước)\n    if 'evals_result' in locals():\n        print(\"   -> Cập nhật biểu đồ...\")\n        for metric in ['auc']:\n            # Nối train\n            old_train = evals_result.get('train', {}).get(metric, [])\n            new_train = evals_result_new.get('train', {}).get(metric, [])\n            evals_result['train'][metric] = old_train + new_train\n            \n            # Nối valid\n            old_valid = evals_result.get('valid', {}).get(metric, [])\n            new_valid = evals_result_new.get('valid', {}).get(metric, [])\n            evals_result['valid'][metric] = old_valid + new_valid\n            \n        # Vẽ biểu đồ nối dài\n        try:\n            lgb.plot_metric(evals_result, metric='auc')\n            plt.title('LightGBM Extended Training AUC')\n            plt.axvline(x=len(old_train), color='r', linestyle='--', label='Resume Point') # Vạch đỏ đánh dấu điểm train tiếp\n            plt.legend()\n            plt.grid(True, alpha=0.3)\n            plt.show()\n        except:\n            pass\n    else:\n        # Nếu không có lịch sử cũ thì vẽ cái mới\n        lgb.plot_metric(evals_result_new, metric='auc')\n        plt.show()\n\n    # 5. DỰ ĐOÁN & NỘP BÀI (PHIÊN BẢN NÂNG CẤP)\n    print(\"\\n--- PREDICTING WITH IMPROVED MODEL ---\")\n    preds = model.predict(test_df[features], num_iteration=model.best_iteration)\n    \n    if 'submission_ids' not in locals():\n        submission_ids = pd.read_csv('/kaggle/input/kkbox-music-recommendation-challenge/test.csv', usecols=['id'])['id'].values\n\n    sub = pd.DataFrame({'id': submission_ids, 'target': preds})\n    sub['id'] = sub['id'].astype(int) # Đảm bảo int\n    \n    filename = 'submission_LGBM_EXTENDED.csv'\n    sub.to_csv(filename, index=False)\n    \n    print(f\">>> XONG! File mới: {filename}\")\n    print(\"Bạn hãy so sánh điểm Validation cuối cùng với lần trước xem tăng được bao nhiêu nhé!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T01:15:34.082639Z","iopub.status.idle":"2026-01-07T01:15:34.082900Z","shell.execute_reply.started":"2026-01-07T01:15:34.082778Z","shell.execute_reply":"2026-01-07T01:15:34.082789Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport lightgbm as lgb\nfrom sklearn.decomposition import TruncatedSVD\nfrom scipy import sparse\nimport gc\n\nprint(\">>> KHỞI ĐỘNG: CHIẾN DỊCH FEATURE ENGINEERING TOP 1\")\n\n# ==============================================================================\n# 1. LOAD & PREPROCESS (TỐI ƯU BỘ NHỚ)\n# ==============================================================================\nINPUT_DIR = '/kaggle/working/'\n\nprint(\"1. Loading Data...\")\n# Load các cột cần thiết\ntrain = pd.read_csv(f'{INPUT_DIR}/train.csv', dtype={'target': np.int8})\ntest = pd.read_csv(f'{INPUT_DIR}/test.csv')\nmembers = pd.read_csv(f'{INPUT_DIR}/members.csv')\nsongs = pd.read_csv(f'{INPUT_DIR}/songs.csv')\n\n# Nối Train/Test\ntrain['is_train'] = 1\ntest['is_train'] = 0\ntest['target'] = np.nan\nfull_data = pd.concat([train, test], ignore_index=True)\n\ndel train, test\ngc.collect()\n\nprint(\"2. Merging Info...\")\nfull_data = full_data.merge(members, on='msno', how='left')\nfull_data = full_data.merge(songs, on='song_id', how='left')\ndel members, songs\ngc.collect()\n\n# ==============================================================================\n# 2. XỬ LÝ THỜI GIAN (CRITICAL STEP)\n# ==============================================================================\nprint(\"3. Time Processing (Tạo Timestamp chuẩn)...\")\n# Chuyển đổi ngày tháng\nfor col in ['registration_init_time', 'expiration_date']:\n    full_data[col] = pd.to_datetime(full_data[col], format='%Y%m%d', errors='coerce')\n\n# Tạo Timestamp (Giây)\nfull_data['timestamp'] = full_data['registration_init_time'].astype(np.int64) // 10**9\nfull_data['timestamp'] = full_data['timestamp'].replace(-9223372037, 0) # Fix lỗi NaT\n\n# SẮP XẾP DỮ LIỆU THEO THỜI GIAN (BẮT BUỘC ĐỂ TÍNH till_now_cnt)\nprint(\"   -> Sorting by Time for Cumulative Features...\")\nfull_data = full_data.sort_values(['msno', 'timestamp'])\n\n# ==============================================================================\n# 3. TẠO CÁC \"SIÊU TÍNH NĂNG\" TỪ FILE IMPORTANCE\n# ==============================================================================\nprint(\"4. Creating TOP 1 Features...\")\n\n# --- A. MSNO_TILL_NOW_CNT (TOP 3 FEATURE) ---\n# Đếm tích lũy: Bài này là bài thứ mấy user nghe?\nprint(\"   -> Generating 'msno_till_now_cnt'...\")\nfull_data['msno_till_now_cnt'] = full_data.groupby('msno').cumcount()\n\n# --- B. TIME STATISTICS (TOP 5 FEATURES) ---\n# Thống kê hành vi thời gian của User\nprint(\"   -> Generating Time Stats (Mean, Std, Min, Max)...\")\ngroup_time = full_data.groupby('msno')['timestamp']\nfull_data['msno_timestamp_mean'] = group_time.transform('mean')\nfull_data['msno_timestamp_std']  = group_time.transform('std').fillna(0)\n# Upper/Lower time trong file importance khả năng cao là max/min time\nfull_data['msno_upper_time']     = group_time.transform('max')\nfull_data['msno_lower_time']     = group_time.transform('min')\n\n# --- C. PROBABILITY FEATURES (MỞ RỘNG) ---\nprint(\"   -> Generating Probabilities (Artist, Language, System)...\")\ndef create_prob(df, col):\n    # P(Feature | User)\n    counts = df.groupby(['msno', col])[col].transform('count')\n    total = df.groupby('msno')[col].transform('count')\n    df[f'msno_{col}_prob'] = counts / total\n    return df\n\n# Tạo cho các cột quan trọng như trong file\nprobs_cols = ['source_type', 'source_screen_name', 'source_system_tab', 'artist_name', 'language']\nfor col in probs_cols:\n    full_data = create_prob(full_data, col)\n\n# --- D. REC COUNT ---\nfull_data['msno_rec_cnt'] = full_data.groupby('msno')['msno'].transform('count')\n\n# --- E. TIME LEFT & BASIC ---\nfull_data['time_left'] = (full_data['expiration_date'] - full_data['registration_init_time']).dt.days.fillna(0)\n\n# ==============================================================================\n# 4. SVD NÂNG CAO (50 COMPONENTS)\n# ==============================================================================\nprint(\"5. Advanced SVD (50 Components)...\")\n\n# Mã hóa ID sang số để tạo ma trận\nfull_data['msno_int'] = full_data['msno'].astype('category').cat.codes\nfull_data['song_int'] = full_data['song_id'].astype('category').cat.codes\n\n# Tạo ma trận thưa\nrows = full_data['msno_int'].values\ncols = full_data['song_int'].values\ndata = np.ones(len(full_data))\nn_users = full_data['msno_int'].max() + 1\nn_songs = full_data['song_int'].max() + 1\nsparse_matrix = sparse.csr_matrix((data, (rows, cols)), shape=(n_users, n_songs))\n\n# SVD với 50 components (như file importance có component_45)\nn_components = 50 \nsvd = TruncatedSVD(n_components=n_components, random_state=42)\nuser_vecs = svd.fit_transform(sparse_matrix)\nsong_vecs = svd.components_.T\n\n# 1. Tính Dot Product (Tương tác User-Song)\nprint(\"   -> Calculating SVD Dot Product...\")\nu_vecs_selected = user_vecs[full_data['msno_int'].values]\ns_vecs_selected = song_vecs[full_data['song_int'].values]\nfull_data['song_embeddings_dot'] = (u_vecs_selected * s_vecs_selected).sum(axis=1)\n\n# 2. Thêm các User Components (Feature ẩn)\nprint(\"   -> Adding SVD Components to DataFrame...\")\n# Chỉ lấy các component quan trọng nhất để tránh tràn RAM (ví dụ top 20)\n# Nếu RAM bạn > 16GB thì có thể lấy hết 50\nn_keep = 20 \ncols_svd = [f'member_component_{i}' for i in range(n_keep)]\ndf_svd = pd.DataFrame(user_vecs[:, :n_keep], columns=cols_svd)\ndf_svd['msno_int'] = range(n_users)\n\nfull_data = full_data.merge(df_svd, on='msno_int', how='left')\n\n# Dọn dẹp\ndel sparse_matrix, user_vecs, song_vecs, u_vecs_selected, s_vecs_selected, df_svd\ngc.collect()\n\n# ==============================================================================\n# 5. PREPARE FOR LIGHTGBM\n# ==============================================================================\nprint(\"6. Formatting Data for LightGBM...\")\n\n# Xử lý Category (Encoding)\ncat_cols = ['source_system_tab', 'source_screen_name', 'source_type', 'city', 'gender', \n            'registered_via', 'language', 'artist_name', 'composer', 'lyricist', \n            'msno', 'song_id', 'genre_ids']\n\nfor col in cat_cols:\n    full_data[col] = full_data[col].astype('category').cat.codes\n\n# Chọn features\ncols_to_drop = ['registration_init_time', 'expiration_date', 'target', 'id', 'msno_int', 'song_int']\nfeatures = [c for c in full_data.columns if c not in cols_to_drop]\nprint(f\"   -> Final Feature Count: {len(features)}\")\nprint(f\"   -> List: {features}\")\n\n# Tách Train/Test\n# Vì lúc đầu ta sort theo thời gian, giờ phải cẩn thận khi tách\n# Dựa vào cột 'is_train'\ntrain_df = full_data[full_data['is_train'] == 1]\ntest_df = full_data[full_data['is_train'] == 0]\ny_train = full_data[full_data['is_train'] == 1]['target']\nsubmission_ids = pd.read_csv(f'{INPUT_DIR}/test.csv', usecols=['id'])['id'].values # Load lại ID chuẩn từ file gốc\n\ndel full_data\ngc.collect()\n\n# ==============================================================================\n# 6. TRAIN & SUBMIT\n# ==============================================================================\nprint(\"7. Training LightGBM (The Final Run)...\")\n\ndtrain = lgb.Dataset(train_df[features], label=y_train)\n\nparams = {\n    'objective': 'binary',\n    'metric': 'auc',\n    'boosting': 'gbdt',\n    'learning_rate': 0.05,  # Giảm LR để học kỹ hơn (chuẩn Top 1)\n    'num_leaves': 200,      # Tăng độ phức tạp vì nhiều feature xịn\n    'max_depth': 12,\n    'bagging_fraction': 0.8,\n    'feature_fraction': 0.8,\n    'verbose': -1,\n    'seed': 42\n}\n\n# Train 3000 vòng (với features này cần train lâu hơn)\nmodel = lgb.train(\n    params, \n    dtrain, \n    num_boost_round=1000,\n    callbacks=[lgb.log_evaluation(period=100)]\n)\n\nprint(\"8. Predicting...\")\npreds = model.predict(test_df[features])\n\n# Tạo file nộp\nsub = pd.DataFrame({'id': submission_ids, 'target': preds})\nfilename = 'submission_TOP_1_FEATURES.csv'\nsub.to_csv(filename, index=False)\n\nprint(f\">>> HOÀN TẤT! File: {filename}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T04:13:49.652645Z","iopub.execute_input":"2026-01-07T04:13:49.653008Z","iopub.status.idle":"2026-01-07T04:26:31.364984Z","shell.execute_reply.started":"2026-01-07T04:13:49.652984Z","shell.execute_reply":"2026-01-07T04:26:31.363726Z"}},"outputs":[{"name":"stdout","text":">>> KHỞI ĐỘNG: CHIẾN DỊCH FEATURE ENGINEERING TOP 1\n1. Loading Data...\n2. Merging Info...\n3. Time Processing (Tạo Timestamp chuẩn)...\n   -> Sorting by Time for Cumulative Features...\n4. Creating TOP 1 Features...\n   -> Generating 'msno_till_now_cnt'...\n   -> Generating Time Stats (Mean, Std, Min, Max)...\n   -> Generating Probabilities (Artist, Language, System)...\n5. Advanced SVD (50 Components)...\n   -> Calculating SVD Dot Product...\n   -> Adding SVD Components to DataFrame...\n6. Formatting Data for LightGBM...\n   -> Final Feature Count: 50\n   -> List: ['msno', 'song_id', 'source_system_tab', 'source_screen_name', 'source_type', 'is_train', 'city', 'bd', 'gender', 'registered_via', 'song_length', 'genre_ids', 'artist_name', 'composer', 'lyricist', 'language', 'timestamp', 'msno_till_now_cnt', 'msno_timestamp_mean', 'msno_timestamp_std', 'msno_upper_time', 'msno_lower_time', 'msno_source_type_prob', 'msno_source_screen_name_prob', 'msno_source_system_tab_prob', 'msno_artist_name_prob', 'msno_language_prob', 'msno_rec_cnt', 'time_left', 'song_embeddings_dot', 'member_component_0', 'member_component_1', 'member_component_2', 'member_component_3', 'member_component_4', 'member_component_5', 'member_component_6', 'member_component_7', 'member_component_8', 'member_component_9', 'member_component_10', 'member_component_11', 'member_component_12', 'member_component_13', 'member_component_14', 'member_component_15', 'member_component_16', 'member_component_17', 'member_component_18', 'member_component_19']\n7. Training LightGBM (The Final Run)...\n8. Predicting...\n>>> HOÀN TẤT! File: submission_TOP_1_FEATURES.csv\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"import pandas as pd\nimport gc\n\nprint(\"--- FIX LỖI 0.5: CĂN CHỈNH LẠI ID (Re-Alignment) ---\")\n\n# 1. KIỂM TRA DỮ LIỆU\nif 'test_df' not in locals() or 'model' not in locals():\n    raise ValueError(\"Cần có biến test_df và model trong RAM. Nếu mất bạn phải chạy lại bước Feature Engineering.\")\n\n# 2. DỰ ĐOÁN LẠI (TRÊN DATA ĐÃ SORT)\nprint(\"Predicting on sorted test data...\")\npreds = model.predict(test_df[features])\n\n# 3. TẠO DATAFRAME TẠM (Chứa ID đúng của từng dự đoán)\n# test_df đang được sort theo msno/time, nên ta lấy ID từ chính nó để ghép với pred\ntemp_sub = pd.DataFrame({\n    'id': test_df['id'].values,\n    'target': preds\n})\n\n# 4. LOAD SAMPLE SUBMISSION ĐỂ LẤY THỨ TỰ CHUẨN KAGGLE\nprint(\"Loading sample submission for correct order...\")\nsample_sub = pd.read_csv('/kaggle/working/sample_submission.csv')\n# Xóa cột target cũ của sample\ndel sample_sub['target']\n\n# 5. MERGE ĐỂ TRẢ VỀ ĐÚNG THỨ TỰ GỐC\nprint(\"Merging to restore original order...\")\n# Merge theo ID: Dòng nào ID nào sẽ nhận đúng target đó\nfinal_sub = sample_sub.merge(temp_sub, on='id', how='left')\n\n# 6. FILLNA & FORMAT\n# Phòng trường hợp sót (hiếm khi xảy ra nếu logic đúng)\nfinal_sub['target'] = final_sub['target'].fillna(0.5) \nfinal_sub['id'] = final_sub['id'].astype(int)\n\n# 7. LƯU FILE\nfilename = 'submission_TOP1_FIXED_ORDER.csv'\nfinal_sub.to_csv(filename, index=False)\n\nprint(f\">>> XONG! File chuẩn: {filename}\")\nprint(\"Nộp file này điểm sẽ về đúng thực lực (dự kiến > 0.68).\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T04:31:37.862817Z","iopub.execute_input":"2026-01-07T04:31:37.863651Z","iopub.status.idle":"2026-01-07T04:33:20.574677Z","shell.execute_reply.started":"2026-01-07T04:31:37.863603Z","shell.execute_reply":"2026-01-07T04:33:20.573447Z"}},"outputs":[{"name":"stdout","text":"--- FIX LỖI 0.5: CĂN CHỈNH LẠI ID (Re-Alignment) ---\nPredicting on sorted test data...\nLoading sample submission for correct order...\nMerging to restore original order...\n>>> XONG! File chuẩn: submission_TOP1_FIXED_ORDER.csv\nNộp file này điểm sẽ về đúng thực lực (dự kiến > 0.68).\n","output_type":"stream"}],"execution_count":12}]}